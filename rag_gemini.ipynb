{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"Generative AI with LangChain.pdf\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 0}, page_content=''),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 1}, page_content='Generative AI with LangChain\\nBuild large language model (LLM) apps with Python, \\nChatGPT, and other LLMs\\nBen Auffarth\\nBIRMINGHAM—MUMBAI'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 2}, page_content='Generative AI with LangChain\\nCopyright © 2023 Packt Publishing\\nAll rights reserved. No part of this book may be reproduced, stored in a retrieval system, or transmitted in \\nany form or by any means, without the prior written permission of the publisher, except in the case of brief \\nquotations embedded in critical articles or reviews.\\nEvery effort has been made in the preparation of this book to ensure the accuracy of the information \\npresented. However, the information contained in this book is sold without warranty, either express or \\nimplied. Neither the author, nor Packt Publishing or its dealers and distributors, will be held liable for any \\ndamages caused or alleged to have been caused directly or indirectly by this book.\\nPackt Publishing has endeavored to provide trademark information about all of the companies and products \\nmentioned in this book by the appropriate use of capitals. However, Packt Publishing cannot guarantee \\nthe accuracy of this information.\\nSenior Publishing Product Manager: Tushar Gupta\\nAcquisition Editor – Peer Reviews: Tejas Mhasvekar\\nProject Editor: Namrata Katare\\nContent Development Editors: Tanya D’cruz and Elliot Dallow\\nCopy Editor: Safis Editing\\nTechnical Editor: Kushal Sharma\\nProofreader: Safis Editing\\nIndexer: Manju Arasan\\nPresentation Designer: Ajay Patule\\nDeveloper Relations Marketing Executive: Monika Sangwan\\nFirst published: December 2023\\nProduction reference: 1141223\\nPublished by Packt Publishing Ltd.\\nGrosvenor House\\n11 St Paul’s Square\\nBirmingham\\nB3 1RB, UK.\\nISBN 978-1-83508-346-8\\nwww.packt.com'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 3}, page_content='To Diane and Nico\\n– Ben Auffarth'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 4}, page_content='Contributors\\nAbout the author\\nBen Auffarth is a seasoned data science leader with a background and Ph.D. in computational \\nneuroscience. Ben has analyzed terabytes of data, simulated brain activity on supercomputers \\nwith up to 64k cores, designed and conducted wet lab experiments, built production systems \\nprocessing underwriting applications, and trained neural networks on millions of documents. \\nHe’s the author of the books Machine Learning for Time Series and Artificial Intelligence with Python \\nCookbook. He now works in insurance at Hastings Direct.\\nCreating this book has been a long and sometimes arduous journey, but also an exciting one. It has \\nbeen enriched immeasurably by the contributions of several key individuals to whom I owe great \\nthanks. Foremost, I extend my heartfelt gratitude to Leo, whose insightful feedback significantly \\nrefined this book. I am equally delighted with my astute editors — Tanya, Elliot, and Kushal. Their \\nefforts went above and beyond expectations. Tanya, in particular, was instrumental in guiding me \\nthrough the writing process, continually challenging me to clarify my thoughts and significantly \\nshaping the final product.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 5}, page_content='About the reviewers\\nLeonid Ganeline is a machine learning engineer with extensive experience in natural language \\nprocessing. He has worked in several start-ups, creating models and production systems. He is \\nan active contributor to LangChain and several other open-source projects. His interest lies in \\nmodel evaluation, especially in LLM evaluation\\nI would like to express my gratitude to my parents, for teaching me how to think rationally, and to \\nmy wife, for supporting me in this endeavor.\\nRuchi Bhatia is a computer engineer with a Master’s degree in information systems management \\nfrom Carnegie Mellon University. Currently, she is leveraging her skills as a product marketing \\nmanager in the rapidly evolving field of data science and AI at HP. She takes pride in being the \\nyoungest triple Kaggle Grandmaster across the Notebooks, Datasets, and Discussion categories. \\nHer previous role as the Leader of Data Science at OpenMined allowed her to steer a team of data \\nscientists to create innovative and impactful solutions.\\nI want to take a moment to express my heartfelt thanks to my parents. Their unwavering support and \\nencouragement throughout my journey have been invaluable. Without their belief in my abilities \\nand their constant guidance, I wouldn’t have achieved the milestones I have today. Thank you, Mom \\nand Dad, for always being there for me.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 6}, page_content=\"Join our community on Discord\\nJoin our community's Discord space for discussions with the authors and other readers:\\nhttps://packt.link/lang\"),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 7}, page_content='Table of Contents\\nPreface   xv\\nChapter 1: What Is Generative AI?   1\\nIntroducing generative AI  ��������������������������������������������������������������������������������������������������� 2\\nWhat are generative models? • 4\\nWhy now? • 8\\nUnderstanding LLMs  ���������������������������������������������������������������������������������������������������������  11\\nWhat is a GPT? • 13\\nOther LLMs • 16\\nMajor players • 18\\nHow do GPT models work? • 20\\nPre-training • 23\\nTokenization • 24\\nScaling • 25\\nConditioning • 26\\nHow to try out these models • 27\\nWhat are text-to-image models?  ��������������������������������������������������������������������������������������� 27\\nWhat can AI do in other domains? �������������������������������������������������������������������������������������  33\\nSummary  �������������������������������������������������������������������������������������������������������������������������� 34\\nQuestions  �������������������������������������������������������������������������������������������������������������������������� 35'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 8}, page_content='Table of Contentsviii\\nChapter 2: LangChain for LLM Apps   37\\nGoing beyond stochastic parrots  ��������������������������������������������������������������������������������������� 38\\nWhat are the limitations of LLMs? • 38\\nHow can we mitigate LLM limitations? • 42\\nWhat is an LLM app? • 43\\nWhat is LangChain?  ���������������������������������������������������������������������������������������������������������� 46\\nExploring key components of LangChain �������������������������������������������������������������������������� 50\\nWhat are chains? • 50\\nWhat are agents? • 52\\nWhat is memory? • 54\\nWhat are tools? • 55\\nHow does LangChain work?  ���������������������������������������������������������������������������������������������� 57\\nComparing LangChain with other frameworks  ���������������������������������������������������������������� 60\\nSummary  ��������������������������������������������������������������������������������������������������������������������������  61\\nQuestions  �������������������������������������������������������������������������������������������������������������������������� 62\\nChapter 3: Getting Started with LangChain   65\\nHow to set up the dependencies for this book  ������������������������������������������������������������������� 65\\npip • 67\\nPoetry • 68\\nConda • 68\\nDocker • 68\\nExploring API model integrations  ������������������������������������������������������������������������������������� 69\\nFake LLM • 72\\nOpenAI • 73\\nHugging Face • 75\\nGoogle Cloud Platform • 77\\nJina AI • 80\\nReplicate • 82\\nOthers • 84'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 9}, page_content='Table of Contents ix\\nAzure • 84\\nAnthropic • 85\\nExploring local models  ����������������������������������������������������������������������������������������������������� 85\\nHugging Face Transformers • 86\\nllama.cpp • 87\\nGPT4All • 88\\nBuilding an application for customer service  �������������������������������������������������������������������� 89\\nSummary  �������������������������������������������������������������������������������������������������������������������������� 96\\nQuestions  �������������������������������������������������������������������������������������������������������������������������� 96\\nChapter 4: Building Capable Assistants   99\\nMitigating hallucinations through fact-checking  ����������������������������������������������������������� 100\\nSummarizing information  ����������������������������������������������������������������������������������������������  103\\nBasic prompting • 103\\nPrompt templates • 104\\nChain of density • 105\\nMap-Reduce pipelines • 107\\nMonitoring token usage • 109\\nExtracting information from documents  �������������������������������������������������������������������������  112\\nAnswering questions with tools  ��������������������������������������������������������������������������������������� 116\\nInformation retrieval with tools • 116\\nBuilding a visual interface • 118\\nExploring reasoning strategies  ����������������������������������������������������������������������������������������� 121\\nSummary  ������������������������������������������������������������������������������������������������������������������������� 129\\nQuestions  ������������������������������������������������������������������������������������������������������������������������ 130\\nChapter 5: Building a Chatbot like ChatGPT   131\\nWhat is a chatbot?  ������������������������������������������������������������������������������������������������������������ 132\\nUnderstanding retrieval and vectors  �������������������������������������������������������������������������������� 134\\nEmbeddings • 135\\nVector storage • 139'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 10}, page_content='Table of Contentsx\\nVector indexing • 140\\nVector libraries • 141\\nVector databases • 143\\nLoading and retrieving in LangChain  ������������������������������������������������������������������������������ 148\\nDocument loaders • 149\\nRetrievers in LangChain • 150\\nkNN retriever • 151\\nPubMed retriever • 152\\nCustom retrievers • 153\\nImplementing a chatbot  �������������������������������������������������������������������������������������������������� 153\\nDocument loader • 154\\nVector storage • 155\\nMemory • 160\\nConversation buffers • 161\\nRemembering conversation summaries • 164\\nStoring knowledge graphs • 164\\nCombining several memory mechanisms • 165\\nLong-term persistence • 166\\nModerating responses  ������������������������������������������������������������������������������������������������������ 167\\nSummary  ������������������������������������������������������������������������������������������������������������������������  170\\nQuestions  ������������������������������������������������������������������������������������������������������������������������� 171\\nChapter 6: Developing Software with Generative AI   173\\nSoftware development and AI  ������������������������������������������������������������������������������������������ 174\\nCode LLMs • 175\\nWriting code with LLMs  �������������������������������������������������������������������������������������������������� 179\\nStarCoder • 179\\nStarChat • 184\\nLlama 2 • 186\\nSmall local model • 187'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 11}, page_content='Table of Contents xi\\nAutomating software development  ���������������������������������������������������������������������������������  189\\nSummary  ������������������������������������������������������������������������������������������������������������������������  201\\nQuestions  ������������������������������������������������������������������������������������������������������������������������ 201\\nChapter 7: LLMs for Data Science   203\\nThe impact of generative models on data science  ����������������������������������������������������������� 204\\nAutomated data science  �������������������������������������������������������������������������������������������������� 207\\nData collection • 209\\nVisualization and EDA • 210\\nPreprocessing and feature extraction • 210\\nAutoML • 211\\nUsing agents to answer data science questions  ���������������������������������������������������������������� 213\\nData exploration with LLMs  �������������������������������������������������������������������������������������������� 217\\nSummary  ������������������������������������������������������������������������������������������������������������������������  222\\nQuestions  ������������������������������������������������������������������������������������������������������������������������ 223\\nChapter 8: Customizing LLMs and Their Output   225\\nConditioning LLMs  ��������������������������������������������������������������������������������������������������������� 226\\nMethods for conditioning • 227\\nReinforcement learning with human feedback • 228\\nLow-rank adaptation • 229\\nInference-time conditioning • 230\\nFine-tuning  ��������������������������������������������������������������������������������������������������������������������� 232\\nSetup for fine-tuning • 233\\nOpen-source models • 236\\nCommercial models • 241\\nPrompt engineering  �������������������������������������������������������������������������������������������������������� 242\\nPrompt techniques • 244\\nZero-shot prompting • 246\\nFew-shot learning • 246'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 12}, page_content='Table of Contentsxii\\nChain-of-thought prompting • 248\\nSelf-consistency • 249\\nTree-of-thought • 251\\nSummary  ������������������������������������������������������������������������������������������������������������������������  255\\nQuestions  ������������������������������������������������������������������������������������������������������������������������ 255\\nChapter 9: Generative AI in Production   257\\nHow to get LLM apps ready for production  ��������������������������������������������������������������������� 258\\nTerminology • 260\\nHow to evaluate LLM apps  ����������������������������������������������������������������������������������������������� 261\\nComparing two outputs • 264\\nComparing against criteria • 265\\nString and semantic comparisons • 267\\nRunning evaluations against datasets • 268\\nHow to deploy LLM apps  ������������������������������������������������������������������������������������������������� 273\\nFastAPI web server • 276\\nRay • 280\\nHow to observe LLM apps  ����������������������������������������������������������������������������������������������� 284\\nTracking responses • 287\\nObservability tools • 289\\nLangSmith • 291\\nPromptWatch • 294\\nSummary  ������������������������������������������������������������������������������������������������������������������������  295\\nQuestions  ������������������������������������������������������������������������������������������������������������������������ 296\\nChapter 10: The Future of Generative Models   299\\nThe current state of generative AI  ����������������������������������������������������������������������������������  300\\nChallenges • 302\\nTrends in model development • 304\\nBig Tech vs. small enterprises • 307\\nArtificial General Intelligence • 309'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 13}, page_content='Table of Contents xiii\\nEconomic consequences  �������������������������������������������������������������������������������������������������  310\\nCreative industries and advertising • 313\\nEducation • 315\\nLaw • 315\\nManufacturing • 316\\nMedicine • 316\\nMilitary • 316\\nSocietal implications  ������������������������������������������������������������������������������������������������������� 317\\nMisinformation and cybersecurity • 318\\nRegulations and implementation challenges • 319\\nThe road ahead  ���������������������������������������������������������������������������������������������������������������� 321\\nOther Books You May Enjoy   325\\nIndex   329'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 14}, page_content=''),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 15}, page_content='Preface\\nIn the dynamic and rapidly advancing field of AI, generative AI stands out as a disruptive force \\npoised to transform how we interact with technology. This book is an expedition into the intri-\\ncate world of large language models (LLMs) – the powerful engines driving this transformation \\n– designed to equip developers, researchers, and AI aficionados with the knowledge needed to \\nharness these tools.\\nVenture into the depths of deep learning, where unstructured data comes alive, and discover \\nhow LLMs like GPT-4 and others are carving a path for AI’s impact on businesses, societies, and \\nindividuals. With the tech industry and media abuzz with the capabilities and potential of these \\nmodels, it’s an opportune moment to explore how they function, thrive, and propel us toward \\nfuture horizons.\\nThis book serves as your compass, pointing you toward understanding the technical scaffolds that \\nuphold LLMs. We provide a prelude to their vast applications, the elegance of their underlying \\narchitecture, and the powerful implications of their existence. Written for a diverse audience, from \\nthose taking their first steps in AI to seasoned developers, the text melds theoretical concepts \\nwith practical, code-rich examples, preparing you to not only grasp LLMs intellectually but to \\nalso apply them inventively and responsibly.\\nAs we embark on this journey together, let us prime ourselves to shape and be shaped by the \\ngenerative AI narrative that’s unfolding at this very moment–a narrative where you, armed with \\nknowledge and foresight, stand at the forefront of this exhilarating technological evolution.\\nWho this book is for\\nThe book is intended for developers, researchers, and anyone else who is interested in learning \\nmore about LLMs. It is written in a clear and concise style, and it includes plenty of code examples \\nto help you learn by doing. '),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 16}, page_content='Prefacexvi\\nWhether you are a beginner or an experienced developer, this book will be a valuable resource \\nfor anyone who wants to get the most out of LLMs and to stay ahead of the curve about LLMs \\nand LangChain.\\nWhat this book covers\\nChapter 1, What Is Generative AI? , explains how generative AI has revolutionized the processing of \\ntext, images, and video, with deep learning at its core. This chapter introduces generative models \\nsuch as LLMs, detailing their technical underpinnings and transformative potential across various \\nsectors. This chapter covers the theory behind these models, highlighting neural networks and \\ntraining approaches, and the creation of human-like content. The chapter outlines the evolution \\nof AI, Transformer architecture, text-to-image models like Stable Diffusion, and touches on sound \\nand video applications.\\nChapter 2, LangChain for LLM Apps, uncovers the need to expand beyond the stochastic parrots \\nof LLMs–models that mimic language without true understanding–by harnessing LangChain’s \\nframework. Addressing limitations like outdated knowledge, action limitations, and hallucination \\nrisks, the chapter highlights how LangChain integrates external data and interventions for more \\ncoherent AI applications. The chapter critically engages with the concept of stochastic parrots, \\nrevealing the deficiencies in models that produce fluent but meaningless language, and explicates \\nhow prompting, chain-of-thought reasoning, and retrieval grounding augment LLMs to address \\nissues of contextuality, bias, and intransparency.\\nChapter 3, Getting Started with LangChain, provides foundational knowledge for you to set up \\nyour environment to run all examples in the book. It begins with installation guidance for Dock-\\ner, Conda, Pip, and Poetry. The chapter then details integrating models from various providers \\nlike OpenAI’s ChatGPT and Hugging Face, including obtaining necessary API keys. It also deals \\nwith running open-source models locally. The chapter culminates in constructing an LLM app \\nto assist customer service agents, exemplifying how LangChain can streamline operations and \\nenhance the accuracy of responses.\\nChapter 4, Building Capable Assistants, tackles turning LLMs into reliable assistants by weaving \\nin fact-checking to reduce misinformation, employing sophisticated prompting strategies for \\nsummarization, and integrating external tools for enhanced knowledge. It explores the Chain of \\nDensity for information extraction and discusses LangChain decorators and expression language \\nfor customizing behavior. The chapter introduces map-reduce in LangChain for handling long \\ndocuments and discusses token monitoring to manage API usage costs. '),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 17}, page_content='Preface xvii\\nIt looks at implementing a Streamlit application to create interactive LLM applications and using \\nfunction calling and tool usage to transcend basic text generation. Two distinct agent paradigms, \\nplan-and-solve and zero-shot, are implemented to demonstrate decision-making strategies. \\nChapter 5, Building a Chatbot like ChatGPT , delves into enhancing chatbot capabilities with re -\\ntrieval-augmented generation  (RAG), a method that provides LLMs with access to external \\nknowledge, improving their accuracy and domain-specific proficiency. This chapter discusses \\ndocument vectorization, efficient indexing, and the use of vector databases like Milvus and Pine-\\ncone for semantic search. We implement a chatbot, incorporating moderation chains to ensure \\nresponsible communication. The chatbot, available on GitHub, serves as a basis for exploring \\nadvanced topics like dialogue memory and context management.\\nChapter 6, Developing Software with Generative AI, examines the burgeoning role of LLMs in software \\ndevelopment, highlighting the potential for AI to automate coding tasks and serve as dynamic \\ncoding assistants. It explores the current state of AI-driven software development, experiments \\nwith models to generate code snippets, and introduces a design for an automated software de -\\nvelopment agent using LangChain. Critical reflections on the agent’s performance emphasize \\nthe importance of human oversight for error mitigation and high-level design, setting the stage \\nfor a future where AI and human developers work symbiotically.\\nChapter 7, LLMs for Data Science, explores the intersection of generative AI and data science, spot-\\nlighting LLMs’ potential to amplify productivity and drive scientific discovery. The chapter outlines \\nthe current scope of automation in data science through AutoML and extends this notion with \\nthe integration of LLMs for advanced tasks like augmenting datasets and generating executable \\ncode. It covers practical methods for LLMs to conduct exploratory data analysis, run SQL queries, \\nand visualize statistical data. Finally, the use of agents and tools demonstrates how LLMs can \\naddress complex data-centric questions.\\nChapter 8, Customizing LLMs and Their Output, delves into conditioning techniques like fine-tuning \\nand prompting, essential for tailoring LLM performance to complex reasoning and specialized \\ntasks. We unpack fine-tuning, where an LLM is further trained on task-specific data, and prompt \\nengineering, which strategically guides the LLM to generate desired outputs. Advanced prompting \\nstrategies such as few-shot learning and chain-of-thought are implemented, enhancing the rea-\\nsoning capabilities of LLMs. The chapter not only provides concrete examples of fine-tuning and \\nprompting but also discusses the future of LLM advancements and their applications in the field.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 18}, page_content='Prefacexviii\\nChapter 9, Generative AI in Production, addresses the complexities of deploying LLMs within re -\\nal-world applications, covering best practices for ensuring performance, meeting regulatory \\nrequirements, robustness at scale, and effective monitoring. It underscores the importance of \\nevaluation, observability, and systematic operation to make generative AI beneficial in customer \\nengagement and decision-making with financial consequences. It also outlines practical strat -\\negies for deployment and ongoing monitoring of LLM apps using tools like Fast API, Ray, and \\nnewcomers such as LangServe and LangSmith. These tools can provide automated evaluation \\nand metrics that support the responsible adoption of generative AI across sectors.\\nChapter 10, The Future of Generative Models, ventures into the potential advancements and so -\\ncio-technical challenges of generative AI. It examines the economic and societal impacts of these \\ntechnologies, debating job displacement, misinformation, and ethical concerns like human value \\nalignment. As various sectors brace for disruptive AI-induced changes, it reflects on the respon-\\nsibility of corporations, lawmakers, and technologists to forge effective governance frameworks. \\nThis final chapter emphasizes the importance of steering AI development toward augmenting \\nhuman potential while addressing risks such as deepfakes, bias, and the weaponization of AI. It \\nhighlights the urgency for transparency, ethical deployment, and equitable access to guide the \\ngenerative AI revolution positively.\\nTo get the most out of this book\\nTo benefit from the value this book offers, it is essential to have a foundational understanding of \\nPython. Additionally, possessing some basic knowledge of machine learning is recommended.\\nDownload the example code files\\nThe code bundle for the book is hosted on GitHub at https://github.com/benman1/generative_\\nai_with_langchain. We also have other code bundles from our rich catalog of books and videos \\navailable at https://github.com/PacktPublishing/. Check them out!\\nDownload the color images\\nWe also provide a PDF file that has color images of the screenshots/diagrams used in this book. \\nYou can download it here: https://packt.link/gbp/9781835083468.\\nConventions used\\nThere are a number of text conventions used throughout this book.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 19}, page_content='Preface xix\\nCodeInText: Indicates code words in text, database table names, folder names, filenames, file \\nextensions, pathnames, dummy URLs, user input, and Twitter handles. For example: “Mount the \\ndownloaded WebStorm-10*.dmg disk image file as another disk in your system.”\\nA block of code is set as follows:\\nfrom langchain.chains import LLMCheckerChain\\nfrom langchain.llms import OpenAI\\nllm = OpenAI(temperature=0.7)\\ntext = \"What type of mammal lays the biggest eggs?\"\\nWhen we wish to draw your attention to a particular part of a code block, the relevant lines or \\nitems are set in bold:\\nfrom pandasai.llm.openai import OpenAI\\nllm = OpenAI(api_token=\"YOUR_API_TOKEN\")\\npandas_ai = PandasAI(llm)\\nAny command-line input or output is written as follows:\\npip install -r requirements.txt\\nBold: Indicates a new term, an important word, or words that you see on the screen. For instance, \\nwords in menus or dialog boxes appear in the text like this. For example: “Select System info from \\nthe Administration panel.”\\nWarnings or important notes appear like this.\\nTips and tricks appear like this.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 20}, page_content='Prefacexx\\nGet in touch\\nFeedback from our readers is always welcome.\\nGeneral feedback: Email feedback@packtpub.com and mention the book’s title in the subject of \\nyour message. If you have questions about any aspect of this book, please email us at questions@\\npacktpub.com.\\nErrata: Although we have taken every care to ensure the accuracy of our content, mistakes do \\nhappen. If you have found a mistake in this book, we would be grateful if you reported this to us. \\nPlease visit http://www.packtpub.com/submit-errata, click Submit Errata, and fill in the form.\\nPiracy: If you come across any illegal copies of our works in any form on the internet, we would \\nbe grateful if you would provide us with the location address or website name. Please contact us \\nat copyright@packtpub.com with a link to the material.\\nIf you are interested in becoming an author: If there is a topic that you have expertise in and you \\nare interested in either writing or contributing to a book, please visit http://authors.packtpub.\\ncom.\\nShare your thoughts\\nOnce you’ve read Generative AI with LangChain , we’d love to hear your thoughts! Please click \\nhere to go straight to the Amazon review page for this book and share your feedback.\\nYour review is important to us and the tech community and will help us make sure we’re deliv -\\nering excellent quality content.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 21}, page_content='Download a free PDF copy of this book\\nThanks for purchasing this book!\\nDo you like to read on the go but are unable to carry your print books everywhere? Is your eBook \\npurchase not compatible with the device of your choice?\\nDon’t worry, now with every Packt book you get a DRM-free PDF version of that book at no cost.\\nRead anywhere, any place, on any device. Search, copy, and paste code from your favorite technical \\nbooks directly into your application.\\xa0\\nThe perks don’t stop there, you can get exclusive access to discounts, newsletters, and great free \\ncontent in your inbox daily\\nFollow these simple steps to get the benefits:\\n1. Scan the QR code or visit the link below\\nhttps://packt.link/free-ebook/9781835083468\\n2. Submit your proof of purchase\\n3. That’s it! We’ll send your free PDF and other benefits to your email directly'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 22}, page_content=''),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 23}, page_content='1\\nWhat Is Generative AI?\\nOver the last decade, deep learning has evolved massively to process and generate unstructured \\ndata like text, images, and video. These advanced AI models have gained popularity in various \\nindustries, and include large language models  (LLMs). There is currently a significant level of \\nfanfare in both the media and the industry surrounding AI, and there’s a fair case to be made \\nthat Artificial Intelligence (AI), with these advancements, is about to have a wide-ranging and \\nmajor impact on businesses, societies, and individuals alike. This is driven by numerous factors, \\nincluding advancements in technology, high-profile applications, and the potential for transfor-\\nmative impacts across multiple sectors.\\nIn this chapter, we’ll explore generative models and their application. We’ll provide an overview \\nof the technical concepts and training approaches that power these models’ ability to produce \\nnovel content. While we won’t be diving deep into generative models for sound or video, we aim \\nto convey a high-level understanding of how techniques like neural networks, large datasets, \\nand computational scale enable generative models to reach new capabilities in text and image \\ngeneration. The goal is to demystify the underlying magic that allows these models to generate \\nremarkably human-like content across various domains. With this foundation, readers will be \\nbetter prepared to consider both the opportunities and challenges posed by this rapidly advanc-\\ning technology.\\nWe’ll follow this structure:\\n• Introducing generative AI\\n• Understanding LLMs\\n• What are text-to-image models?\\n• What can AI do in other domains?\\nLet’s start from the beginning – by introducing the terminology!'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 24}, page_content='What Is Generative AI?2\\nIntroducing generative AI\\nIn the media, there is substantial coverage of AI-related breakthroughs and their potential impli-\\ncations. These range from advancements in Natural Language Processing (NLP) and computer \\nvision to the development of sophisticated language models like GPT-4. Particularly, generative \\nmodels have received a lot of attention due to their ability to generate text, images, and other \\ncreative content that is often indistinguishable from human-generated content. These same \\nmodels also provide wide functionality including semantic search, content manipulation, and \\nclassification. This allows cost savings with automation and allows humans to leverage their \\ncreativity to an unprecedented level.\\nBenchmarks capturing task performance in different domains have been major drivers of the de-\\nvelopment of these models. The following graph, inspired by a blog post titled GPT-4 Predictions \\nby Stephen McAleese on LessWrong, shows the improvements of LLMs in the Massive Multitask \\nLanguage Understanding (MMLU) benchmark, which was designed to quantify knowledge and \\nproblem-solving ability in elementary mathematics, US history, computer science, law, and more:\\nFigure 1.1: Average performance on the MMLU benchmark of LLMs\\nGenerative AI refers to algorithms that can generate novel content, as opposed to \\nanalyzing or acting on existing data like more traditional, predictive machine learn-\\ning or AI systems.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 25}, page_content='Chapter 1 3\\nYou can see significant improvements in recent years in this benchmark. Particularly, it highlights \\nthe progress of the models provided through a public user interface by OpenAI, especially the \\nimprovements between releases, from GTP-2 to GPT-3 and GPT-3.5 to GPT-4, although the results \\nshould be taken with a grain of salt, since they are self-reported and are obtained either by 5-shot \\nor zero-shot conditioning. Zero-shot means the models were prompted with the question, while \\nin 5-shot settings, models were additionally given 5 question-answer examples. These added \\nexamples could naively account for about 20% of performance according to Measuring Massive \\nMultitask Language Understanding (Hendrycks and colleagues, revised 2023).\\nThere are a few differences between these models and their training that can account for these \\nboosts in performance, such as scale, instruction-tuning, a tweak to the attention mechanisms, \\nand more and different training data. First and foremost, the massive scaling in parameters from \\n1.5 billion (GPT-2) to 175 billion (GPT-3) to more than a trillion (GPT-4) enables models to learn \\nmore complex patterns; however, another major change in early 2022 was the post-training \\nfine-tuning of models based on human instructions, which teaches the model how to perform a \\ntask by providing demonstrations and feedback.\\nAcross benchmarks, a few models have recently started to perform better than an average human \\nrater, but generally still haven’t reached the performance of a human expert. These achievements \\nof human engineering are impressive; however, it should be noted that the performance of these \\nmodels depends on the field; most models are still performing poorly on the GSM8K benchmark \\nof grade school math word problems.\\nGenerative Pre-trained Transformer (GPT) models, like OpenAI’s GPT-4, are prime examples of \\nAI advancements in the sphere of LLMs. ChatGPT has been widely adopted by the general pub -\\nlic, showing greatly improved chatbot capabilities enabled by being much bigger than previous \\nmodels. These AI-based chatbots can generate human-like responses as real-time feedback to \\ncustomers and can be applied to a wide range of use cases, from software development to writing \\npoetry and business communications.\\nAs AI models like OpenAI’s GPT continue to improve, they could become indispensable assets to \\nteams in need of diverse knowledge and skills. \\nPlease note that while most benchmark results come from 5-shot, a few, like the \\nGPT-2, PaLM, and PaLM-2 results, refer to zero-shot conditioning.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 26}, page_content='What Is Generative AI?4\\nFor example, GPT-4 could be considered a polymath that works tirelessly without demanding \\ncompensation (beyond subscription or API fees), providing competent assistance in subjects like \\nmathematics and statistics, macroeconomics, biology, and law (the model performs well on the \\nUniform Bar Exam). As these AI models become more proficient and easily accessible, they are \\nlikely to play a significant role in shaping the future of work and learning.\\nBy making knowledge more accessible and adaptable, these models have the potential to level \\nthe playing field and create new opportunities for people from all walks of life. These models have \\nshown potential in areas that require higher levels of reasoning and understanding, although \\nprogress varies depending on the complexity of the tasks involved.\\nAs for generative models with images, they have pushed the boundaries in their capabilities to \\nassist in creating visual content, and their performance in computer vision tasks such as object \\ndetection, segmentation, captioning, and much more.\\nLet’s clear up the terminology a bit and explain in more detail what is meant by generative model, \\nartificial intelligence, deep learning, and machine learning.\\nWhat are generative models?\\nIn popular media, the term artificial intelligence is used a lot when referring to these new models. \\nIn theoretical and applied research circles, it is often joked that AI is just a fancy word for ML, or \\nAI is ML in a suit, as illustrated in this image:\\nOpenAI is a US AI research company that aims to promote and develop friendly AI. It \\nwas established in 2015 with the support of several influential figures and companies, \\nwho pledged over $1 billion to the venture. The organization initially committed to \\nbeing non-profit, collaborating with other institutions and researchers by making \\nits patents and research open to the public. In 2018, Elon Musk resigned from the \\nboard citing a potential conflict of interest with his role at Tesla. In 2019, OpenAI \\ntransitioned to become a for-profit organization, and subsequently Microsoft made \\nsignificant investments in OpenAI, leading to the integration of OpenAI systems \\nwith Microsoft’s Azure-based supercomputing platform and the Bing search engine. \\nThe most significant achievements of the company include OpenAI Gym for training \\nreinforcement algorithms, and – more recently – the GPT-n models and the DALL-E \\ngenerative models, which generate images from text.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 27}, page_content='Chapter 1 5\\n  \\nFigure 1.2: ML in a suit. Generated by a model on replicate.com, Diffusers Stable Diffusion v2.1\\nIt’s worth distinguishing more clearly between the terms generative model, artificial intelligence, \\nmachine learning, deep learning, and language model:\\n• Artificial Intelligence (AI) is a broad field of computer science focused on creating intel-\\nligent agents that can reason, learn, and act autonomously.\\n• Machine Learning ( ML) is a subset of AI focused on developing algorithms that can \\nlearn from data.\\n• Deep Learning (DL) uses deep neural networks, which have many layers, as a mechanism \\nfor ML algorithms to learn complex patterns from data.\\n• Generative Models are a type of ML model that can generate new data based on patterns \\nlearned from input data.\\n• Language Models (LMs ) are statistical models used to predict words in a sequence of \\nnatural language. Some language models utilize deep learning and are trained on massive \\ndatasets, becoming large language models (LLMs).'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 28}, page_content='What Is Generative AI?6\\nThis class diagram illustrates how LLMs combine deep learning techniques like neural networks \\nwith sequence modeling objectives from language modeling, at a very large scale:\\nFigure 1.3: Class diagram of different models. LLMs represent the intersection of deep learning \\ntechniques with language modeling objectives.\\nGenerative models are a powerful type of AI that can generate new data that resembles the train-\\ning data. Generative AI models have come a long way, enabling the generation of new examples \\nfrom scratch using patterns in data. These models can handle different data modalities and are \\nemployed across various domains, including text, image, music, and video.\\nThe key distinction is that generative models synthesize new data rather than just making pre -\\ndictions or decisions. This enables applications like generating text, images, music, and video.\\nSome language models are generative, while some are not. Generative models facilitate the cre-\\nation of synthetic data to train AI models when real data is scarce or restricted. This type of data \\ngeneration reduces labeling costs and improves training efficiency. Microsoft Research took this \\napproach (Textbooks Are All You Need, June 2023) to training their phi-1 model, where they used \\nGPT-3.5 to create synthetic Python textbooks and exercises.\\nThere are many types of generative models, handling different data modalities across various \\ndomains. They are:\\n• Text-to-text: Models that generate text from input text, like conversational agents. Ex -\\namples: LLaMa 2, GPT-4, Claude, and PaLM 2.\\n• Text-to-image: Models that generate images from text captions. Examples: DALL-E 2, \\nStable Diffusion, and Imagen.\\n• Text-to-audio: Models that generate audio clips and music from text. Examples: Jukebox, \\nAudioLM, and MusicGen.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 29}, page_content='Chapter 1 7\\n• Text-to-video: Models that generate video content from text descriptions. Example: \\nPhenaki and Emu Video.\\n• Text-to-speech: Models that synthesize speech audio from input text. Examples: WaveNet \\nand Tacotron.\\n• Speech-to-text: Models that transcribe speech to text [also called Automatic Speech \\nRecognition (ASR)]. Examples: Whisper and SpeechGPT.\\n• Image-to-text: Models that generate image captions from images. Examples: CLIP and \\nDALL-E 3.\\n• Image-to-image: Applications for this type of model are data augmentation such as su-\\nper-resolution, style transfer, and inpainting.\\n• Text-to-code: Models that generate programming code from text. Examples: Stable Dif-\\nfusion and DALL-E 3.\\n• Video-to-audio: Models that analyze video and generate matching audio. Example: Soun-\\ndify.\\nThere are a lot more combinations of modalities to consider; these are just some that I have come \\nacross. Further, we could consider subcategories of text, such as text-to-math, which generates \\nmathematical expressions from text, where some models such as ChatGPT and Claude shine, or \\ntext-to-code, which are models that generate programming code from text, such as AlphaCode or \\nCodex. A few models are specialized in scientific text, such as Minerva or Galactica, or algorithm \\ndiscovery, such as AlphaTensor.\\nA few models work with several modalities for input or output. An example of a model that \\ndemonstrates generative capabilities in multimodal input is OpenAI’s GPT-4V model (GPT-4 with \\nvision), released in September 2023, which takes both text and images and comes with better \\nOptical Character Recognition (OCR) than previous versions to read text from images. Images \\ncan be translated into descriptive words, then existing text filters are applied. This mitigates the \\nrisk of generating unconstrained image captions.\\nAs the list shows, text is a common input modality that can be converted into various outputs \\nlike image, audio, and video. The outputs can also be converted back into text or within the same \\nmodality. LLMs have driven rapid progress for text-focused domains. These models enable a \\ndiverse range of capabilities via different modalities and domains. The LLM categories are the \\nmain focus of this book; however, we’ll also occasionally look at other models, text-to-image in \\nparticular. These models typically use a Transformer architecture trained on massive datasets \\nvia self-supervised learning.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 30}, page_content='What Is Generative AI?8\\nThe rapid progress shows the potential of generative AI across diverse domains. Within the in-\\ndustry, there is a growing sense of excitement around AI’s capabilities and its potential impact on \\nbusiness operations. But there are key challenges such as data availability, compute requirements, \\nbias in data, evaluation difficulties, potential misuse, and other societal impacts that need to \\nbe addressed going forward, which we’ll discuss in Chapter 10, The Future of Generative Models.\\nLet’s delve a bit more into this progress and pose the question why now ?\\nWhy now?\\nThe success of generative AI coming into the public spotlight in 2022 can be attributed to several \\ninterlinked drivers. The development and success of generative models have relied on improved \\nalgorithms, considerable advances in compute power and hardware design, the availability of \\nlarge, labeled datasets, and an active and collaborative research community helping to evolve a \\nset of tools and techniques.\\nDeveloping more sophisticated mathematical and computational methods has played a vital \\nrole in advancing generative models. The backpropagation algorithm introduced in the 1980s \\nby Geoffrey Hinton, David Rumelhart, and Ronald Williams is one such example. It provided a \\nway to effectively train multi-layer neural networks.\\nIn the 2000s, neural networks began to regain popularity as researchers developed more complex \\narchitectures. However, it was the advent of DL, a type of neural network with numerous layers, \\nthat marked a significant turning point in the performance and capabilities of these models. \\nInterestingly, although the concept of DL has existed for some time, the development and expan-\\nsion of generative models correlate with significant advances in hardware, particularly Graphics \\nProcessing Units (GPUs), which have been instrumental in propelling the field forward.\\nAs mentioned, the availability of cheaper and more powerful hardware has been a key factor in \\nthe development of deeper models. This is because DL models require a lot of computing power \\nto train and run. This concerns all aspects of processing power, memory, and disk space. This \\ngraph shows the cost of computer storage over time for different mediums such as disks, solid \\nstate, flash, and internal memory in terms of price in dollars per terabyte (adapted from Our \\nWorld in Data by Max Roser, Hannah Ritchie, and Edouard Mathieu; https://ourworldindata.\\norg/grapher/historical-cost-of-computer-memory-and-storage:'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 31}, page_content='Chapter 1 9\\nFigure 1.4: Cost of computer storage since the 1950s in dollars (unadjusted) per terabyte\\nWhile, in the past, training a DL model was prohibitively expensive, as the cost of hardware has \\ncome down, it has become possible to train bigger models on much larger datasets. The model \\nsize is one of the factors determining how well a model can approximate (as measured in per -\\nplexity) the training dataset.\\nThe importance of the number of parameters in an LLM : The more parameters \\na model has, the higher its capacity to capture relationships between words and \\nphrases as knowledge. As a simple example of these higher-order correlations, an \\nLLM could learn that the word “cat” is more likely to be followed by the word “dog” \\nif it is preceded by the word “chase,” even if there are other words in between. Gen-\\nerally, the lower a model’s perplexity, the better it will perform, for example, in terms \\nof answering questions.\\nParticularly, it seems that in models with between 2 and 7 billion parameters, new \\ncapabilities emerge such as the ability to generate different creative text in formats \\nlike poems, code, scripts, musical pieces, emails, and letters, and to answer even \\nopen-ended and challenging questions in an informative way.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 32}, page_content='What Is Generative AI?10\\nThis trend toward larger models started around 2009, when NVIDIA catalyzed what is often \\ncalled the Big Bang of DL. GPUs are particularly well suited for the matrix/vector computations \\nnecessary to train deep learning neural networks, therefore significantly increasing the speed and \\nefficiency of these systems by several orders of magnitude and reducing running times from weeks \\nto days. In particular, NVIDIA’s CUDA platform, which allows direct programming of GPUs, has \\nmade it easier than ever for researchers and developers to experiment with and deploy complex \\ngenerative models facilitating breakthroughs in vision, speech recognition, and – more recently \\n– LLMs. Many LLM papers describe the use of NVIDIA A100s for training.\\nIn the 2010s, several types of generative models started gaining traction. Autoencoders, a kind of \\nneural network that can learn to compress data from the input layer to a representation, and then \\nreconstruct the input, served as a basis for more advanced models like Variational Autoencoders \\n(VAEs), which were first proposed in 2013. VAEs, unlike traditional autoencoders, use variational \\ninference to learn the distribution of data, also called the latent space of input data. Around the \\nsame time, GANs were proposed by Ian Goodfellow and others in 2014.\\nOver the past decade, significant advancements have been made in the fundamental algorithms \\nused in DL, such as better optimization methods, more sophisticated model architectures, and \\nimproved regularization techniques. Transformer models, introduced in 2017, built upon this \\nprogress and enabled the creation of large-scale models like GPT-3. Transformers rely on atten-\\ntion mechanisms and resulted in a further leap in the performance of generative models. These \\nmodels, such as Google’s BERT and OpenAI’s GPT series, can generate highly coherent and con-\\ntextually relevant text.\\nThe development of transfer learning techniques, which allow a model pre-trained on one task \\nto be fine-tuned on another, similar task, has also been significant. These techniques have made \\nit more efficient and practical to train large generative models. Moreover, part of the rise of gen-\\nerative models can be attributed to the development of software libraries and tools (TensorFlow, \\nPyTorch, and Keras) specifically designed to work with these artificial neural networks, stream-\\nlining the process of building, training, and deploying them.\\nIn addition to the availability of cheaper and more powerful hardware, the availability of large \\ndatasets of labeled data has also been a key factor in the development of generative models. This \\nis because DL models, particularly generative ones, require vast amounts of text data for effective \\ntraining. The explosion of data available from the internet, particularly in the last decade, has \\ncreated a suitable environment for such models to thrive. As the internet has become more popular, \\nit has become easier to collect large datasets of text, images, and other data. '),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 33}, page_content='Chapter 1 11\\nThis has made it possible to train generative models on much larger datasets than would have \\nbeen possible in the past. To further drive the development of generative models, the research \\ncommunity has been developing benchmarks and other challenges, like the mentioned MMLU \\nand ImageNet for image classification, and has started to do the same for generative models.\\nIn summary, generative modeling is a fascinating and rapidly evolving field. It has the potential \\nto revolutionize the way we interact with computers and create original content. I am excited to \\nsee what the future holds for this field.\\nUnderstanding LLMs\\nText generation models, such as GPT-4 by OpenAI, can generate coherent and grammatically \\ncorrect text in different languages and formats. These models have practical applications in \\nfields like content creation and NLP, where the ultimate goal is to create algorithms capable of \\nunderstanding and generating natural language text.\\nLanguage modeling aims to predict the next word, character, or even sentence based on the pre-\\nvious ones in a sequence. In this sense, language modeling serves as a way of encoding the rules \\nand structures of a language in a way that can be understood by a machine. LLMs capture the \\nstructure of human language in terms of grammar, syntax, and semantics. These models form \\nthe backbone of larger NLP tasks, such as content creation, translation, summarization, machine \\ntranslation, and text-editing tasks such as spelling correction.\\nAt its core, language modeling, and more broadly NLP, relies heavily on the quality of representa-\\ntion learning. A generative language model encodes information about the text that it has been \\ntrained on and generates new text based on those learnings, thereby taking on the task of text \\ngeneration.\\nRepresentation learning is about a model learning its internal representations of \\nraw data to perform a machine learning task, rather than relying only on engineered \\nfeature extraction. For example, an image classification model based on representa-\\ntion learning might learn to represent images according to visual features like edges, \\nshapes, and textures. The model isn’t told explicitly what features to look for – it \\nlearns representations of the raw pixel data that help it make predictions.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 34}, page_content='What Is Generative AI?12\\nRecently, LLMs have found applications for tasks like essay generation, code development, trans-\\nlation, and understanding genetic sequences. More broadly, applications of language models \\ninvolve multiple areas, such as:\\n• Question answering: AI chatbots and virtual assistants can provide personalized and \\nefficient assistance, reducing response times in customer support and thereby enhanc -\\ning customer experience. These systems can be used in specific contexts like restaurant \\nreservations and ticket booking.\\n• Automatic summarization: Language models can create concise summaries of articles, \\nresearch papers, and other content, enabling users to consume and understand infor -\\nmation rapidly.\\n• Sentiment analysis: By analyzing opinions and emotions in texts, language models can \\nhelp businesses understand customer feedback and opinions more efficiently.\\n• Topic modeling: LLMs can discover abstract topics and themes across a corpus of docu-\\nments. It identifies word clusters and latent semantic structures.\\n• Semantic search: LLMs can focus on understanding meaning within individual documents. \\nIt uses NLP to interpret words and concepts for improved search relevance.\\n• Machine translation: Language models can translate texts from one language into an-\\nother, supporting businesses in their global expansion efforts. New generative models can \\nperform on par with commercial products (for example, Google Translate).\\nDespite the remarkable achievements, language models still face limitations when dealing with \\ncomplex mathematical or logical reasoning tasks. It remains uncertain whether continually in-\\ncreasing the scale of language models will inevitably lead to new reasoning capabilities. Further, \\nLLMs are known to return the most probable answers within the context, which can sometimes \\nyield fabricated information, termed hallucinations. This is a feature as well as a bug since it \\nhighlights their creative potential. We’ll talk about hallucinations in Chapter 5, Building a Chatbot \\nLike ChatGPT, but for now, let’s discuss the technical background of LLMs in some more detail.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 35}, page_content='Chapter 1 13\\nWhat is a GPT?\\nLLMs are deep neural networks adept at understanding and generating human language. The \\ncurrent generation of LLMs such as ChatGPT are deep neural network architectures that utilize \\nthe transformer model and undergo pre-training using unsupervised learning on extensive text \\ndata, enabling the model to learn language patterns and structures. Models have evolved rapidly, \\nenabling the creation of versatile foundational AI models suitable for a wide range of downstream \\ntasks and modalities, ultimately driving innovation across various applications and industries.\\nThe notable strength of the latest generation of LLMs as conversational interfaces (chatbots) lies \\nin their ability to generate coherent and contextually appropriate responses, even in open-ended \\nconversations. By generating the next word based on the preceding words repeatedly, the model \\nproduces fluent and coherent text often indistinguishable from text produced by humans. However, \\nChatGPT has been observed to “sometimes write plausible sounding but incorrect or nonsensical \\nanswers,” as expressed in a disclaimer by OpenAI. This is referred to as a hallucination and is just \\none of the concerns around LLMs.\\nA transformer is a DL architecture, first introduced in 2017 by researchers at Google and the \\nUniversity of Toronto (in an article called Attention Is All You Need; Vaswani and colleagues), that \\ncomprises self-attention and feed-forward neural networks, allowing it to effectively capture \\nthe word relationships in a sentence. The attention mechanism enables the model to focus on \\nvarious parts of the input sequence.\\nGenerative Pre-Trained Transformers (GPTs), on the other hand, were introduced by research-\\ners at OpenAI in 2018 together with the first of their eponymous GPT models, GPT-1 (Improving \\nLanguage Understanding by Generative Pre-Training; Radford and others). The pre-training process \\ninvolves predicting the next word in a text sequence, enhancing the model’s grasp of language \\nas measured in the quality of the output. Following pre-training, the model can be fine-tuned \\nfor specific language processing tasks like sentiment analysis, language translation, or chat. This \\ncombination of unsupervised and supervised learning enables GPT models to perform better \\nacross a range of NLP tasks and reduces the challenges associated with training LLMs.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 36}, page_content='What Is Generative AI?14\\nThe size of the training corpus for LLMs has been increasing drastically. GPT-1, introduced by \\nOpenAI in 2018, was trained on BookCorpus with 985 million words. BERT, released in the same \\nyear, was trained on a combined corpus of BookCorpus and English Wikipedia, totaling 3.3 billion \\nwords. Now, training corpora for LLMs reach up to trillions of tokens.\\nThis graph illustrates how LLMs have been growing:\\nFigure 1.5: LLMs from BERT to GPT-4 – size, training budget, and organizations. For the pro -\\nprietary models, parameter sizes are often estimates.\\nThe size of the data points indicates training cost in terms of petaFLOPs and petaFLOP/s-days. A \\npetaFLOP/s day is a unit of throughput that consists of performing 10 to the power of 15 opera-\\ntions per day. Training operations in the calculations are estimated as the approximate number \\nof addition and multiplication operations based on the GPU utilization efficiency.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 37}, page_content='Chapter 1 15\\nFor some models, especially proprietary and closed-source models, this information is not known \\n– in these cases, I’ve placed a cross. For example, for XLNet, the paper doesn’t give information \\nabout compute in flops; however, the training was done on 512 TPU v3 chips over 2.5 days.\\nThe development of GPT models has seen considerable progress, with OpenAI’s GPT-n series \\nleading the way in creating foundational AI models. GPT models can also work with modalities \\nbeyond text for input and output, as seen in GPT-4’s ability to process image input alongside \\ntext. Additionally, they serve as a foundation for text-to-image technologies like diffusion and \\nparallel decoding, enabling the development of Visual Foundation Models (VFMs) for systems \\nthat work with images.\\nTrained on 300 billion tokens, GPT-3 has 175 billion parameters, an unprecedented size for DL \\nmodels. GPT-4 is the most recent in the series, though its size and training details have not been \\npublished due to competitive and safety concerns. However, different estimates suggest it has \\nbetween 200 and 500 billion parameters. Sam Altman, the CEO of OpenAI, has stated that the \\ncost of training GPT-4 was more than $100 million.\\nChatGPT, a conversation model, was released by OpenAI in November 2022. Based on prior GPT \\nmodels (particularly GPT-3) and optimized for dialogue, it uses a combination of human-generat-\\ned roleplaying conversations and a dataset of human labeler demonstrations of the desired model \\nbehavior. The model exhibits excellent capabilities such as wide-ranging knowledge retention \\nand precise context tracking in multi-turn dialogues.\\nAnother substantial advancement came in March 2023 with GPT-4. GPT-4 provides superior \\nperformance on various evaluation tasks coupled with significantly better response avoidance \\nto malicious or provocative queries due to six months of iterative alignment during training.\\nOpenAI has been coy about the technical details; however, information has been circulating that, \\nwith about 1.8 trillion parameters, GPT-4 is more than 10x the size of GPT-3. Further, OpenAI \\nwas able to keep costs reasonable by utilizing a Mixture of Experts (MoE) model consisting of \\n16 experts within their model, each having about 111 billion parameters.\\nA foundation model (sometimes known as a base model) is a large model that was \\ntrained on an immense quantity of data at scale so that it can be adapted to a wide \\nrange of downstream tasks. In GPT models, this pre-training is done via self-super-\\nvised learning.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 38}, page_content='What Is Generative AI?16\\nApparently, GPT-4 was trained on about 13 trillion tokens. However, these are not unique tokens \\nsince they count repeated presentation of the data in each epoch. Training was conducted for 2 \\nepochs for text-based data and 4 for code-based data. For fine-tuning, the dataset consisted of \\nmillions of rows of instruction fine-tuning data. Another rumor, again to be taken with a grain of \\nsalt, is that OpenAI might be applying speculative decoding on GPT-4’s inference, with the idea \\nthat a smaller model (oracle model) could be predicting the large model’s responses, and these \\npredicted responses could help speed up decoding by feeding them into the larger model, thereby \\nskipping tokens. This is a risky strategy because – depending on the threshold of the confidence \\nof the oracle’s responses – the quality could deteriorate.\\nThere’s also a multi-modal version of GPT-4 that incorporates a separate vision encoder, trained \\non joined image and text data, giving the model the capability to read web pages and transcribe \\nwhat’s in images and video.\\nAs can be seen in Figure 1.5, there are quite a few models besides OpenAI’s, some of which are \\nsuitable as a substitute for the OpenAI closed-source models, which we will have a look at.\\nOther LLMs\\nOther notable foundational GPT models besides OpenAI’s include  Google DeepMind’s PaLM \\n2, the model behind Google’s chatbot Bard. Although GPT-4 leads most benchmarks in perfor -\\nmance, these and other models demonstrate a comparable performance in some tasks and have \\ncontributed to advancements in generative transformer-based language models.\\nPaLM 2, released in May 2023, was trained with the focus of improving multilingual and reasoning \\ncapabilities while being more compute efficient. Using evaluations at different compute scales, \\nthe authors (Anil and others; PaLM 2 Technical Report) estimated an optimal scaling of training \\ndata sizes and parameters. PaLM 2 is smaller and exhibits faster and more efficient inference, \\nallowing for broader deployment and faster response times for a more natural pace of interaction.\\nExtensive benchmarking across different model sizes has shown that PaLM 2 has significantly \\nimproved quality on downstream tasks, including multilingual common sense and mathematical \\nreasoning, coding, and natural language generation, compared to its predecessor PaLM.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 39}, page_content='Chapter 1 17\\nPaLM 2 was also tested on various professional language-proficiency exams. The exams used \\nwere for Chinese (HSK 7-9 Writing and HSK 7-9 Overall), Japanese (J-Test A-C Overall), Italian \\n(PLIDA C2 Writing and PLIDA C2 Overall), French (TCF Overall), and Spanish (DELE C2 Writing \\nand DELE C2 Overall). Across these exams, which were designed to test C2-level proficiency, \\nconsidered mastery or advanced professional level according to the CEFR (Common European \\nFramework of Reference for Languages), PaLM 2 achieved mostly high-passing grades.\\nThe releases of the LLaMa and LLaMa 2 series of models, with up to 70B parameters, by Meta AI \\nin February and July 2023, respectively, have been highly influential by enabling the community \\nto build on top of them, thereby kicking off a Cambrian explosion of open-source LLMs. LLaMa \\ntriggered the creation of models such as Vicuna, Koala, RedPajama, MPT, Alpaca, and Gorilla. \\nLLaMa 2, since its recent release, has already inspired several very competitive coding models, \\nsuch as WizardCoder.\\nOptimized for dialogue use cases, at their release, the LLMs outperformed other open-source \\nchat models on most benchmarks and seem on par with some closed-source models based on \\nhuman evaluations. The LLaMa 2 70B model performs on par or better than PaLM (540B) on \\nalmost all benchmarks, but there is still a large performance gap between LLaMa 2 70B and \\nGPT-4 and PaLM-2-L.\\nLLaMa 2 is an updated version of LLaMa 1 trained on a new mix of publicly available data. The \\npre-training corpus size has increased by 40% (2 trillion tokens of data), the context length of \\nthe model has doubled, and grouped-query attention has been adopted. \\nVariants of LLaMa 2 with different parameter sizes (7B, 13B, 34B, and 70B) have been released. \\nWhile LLaMa was released under a non-commercial license, the LLaMa 2  are open to the general \\npublic for research and commercial use.\\nLLaMa 2-Chat has undergone safety evaluation results compared to other open-source and closed-\\nsource models. Human raters judged the safety violations of model generations across approxi -\\nmately 2,000 adversarial prompts, including both single and multi-turn prompts.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 40}, page_content='What Is Generative AI?18\\nClaude and Claude 2 are AI assistants created by Anthropic. Evaluations suggest Claude 2, released \\nin July 2023, is one of the best GPT-4 competitors in the market. It improves on previous versions \\nin helpfulness, honesty, and lack of stereotype bias based on human feedback comparisons. It \\nalso performs well on standardized tests like GRE and MBE. Key model improvements include \\nan expanded context size of up to 200K tokens, far larger than most available models, and being \\ncommercial or open source. It also performs better on use cases like coding, summarization, and \\nlong document understanding.\\nThe model card Anthropic has created is fairly detailed, showing Claude 2 still has limitations in \\nareas like confabulation, bias, factual errors, and potential for misuse, problems it has in com-\\nmon with all LLMs. Anthropic is working to address these through techniques like data filtering, \\ndebiasing, and safety interventions.\\nThe development of LLMs has been limited to a few players due to high computational require -\\nments. In the next section, we’ll look into who these organizations are.\\nMajor players\\nTraining a large number of parameters on large-scale datasets requires significant compute power \\nand a skilled data science and data engineering team. Meta’s LLaMa 2 model, with a size of up \\nto 70 billion parameters, was trained on 1.4 trillion tokens, while PaLM 2, reportedly consisting \\nof 340 billion parameters – smaller than their previous LLMs – appears to have a larger scale of \\ntraining data in at least 100 languages. Modern LLMs can cost anywhere from 10 million to over \\n100 million US dollars in computing costs for training.\\nOnly a few companies, such as those shown in Figure 1.5, have been able to successfully train and \\ndeploy very large models. Major companies like Microsoft and Google have invested in start-ups \\nand collaborations to support the development of these models. Universities, such as KAUST, \\nCarnegie Mellon University, Nanyang Technological University, and Tel Aviv University, have also \\ncontributed to the development of these models. Some projects are developed through collab -\\norations between companies and universities, as seen in the cases of Stable Diffusion, Soundify, \\nand DreamFusion.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 41}, page_content='Chapter 1 19\\nThere are quite a few companies and organizations developing generative AI in gener-\\nal, as well as LLMs, and they are releasing them on different terms – here’s just a few:\\n• OpenAI have released GPT-2 as open source; however, subsequent mod -\\nels have been closed source but open for public usage on their website or \\nthrough an API.\\n• Google (including Google’s DeepMind division) have developed a number \\nof LLMs, starting from BERT and – more recently – Chinchilla, Gopher, PaLM, \\nand PaLM2. They previously released the code and weights (parameters) of a \\nfew of their models under open-source licensing, even though recently they \\nhave moved toward more secrecy in their development.\\n• Anthropic have released the Claude and Claude 2 models for public usage on \\ntheir website. The API is in private beta. The models themselves are closed \\nsource.\\n• Meta have released models like RoBERTa, BART, and LLaMa 2, including \\nparameters of the models (although often under a non-commercial license) \\nand the source code for setting up and training the models.\\n• Microsoft have developed models like Turing-NLG and Megatron-Turing \\nNLG but have focused on integrating OpenAI models into products over re-\\nleasing their own models. The training code and parameters for phi-1 have \\nbeen released for research use.\\n• Stability AI, the company behind Stable Diffusion, released the model \\nweights under a non-commercial license.\\n• The French AI startup Mistral has unveiled its free-to-use, open-license 7B \\nmodel, outperforming similar-sized models, generated from private datasets, \\nand developed with the intent to support the open generative AI community, \\nwhile also offering commercial products.\\n• EleutherAI is a grassroots collection of researchers developing open-access \\nmodels like GPT-Neo and GPT-J, fully open source and available to the public.\\n• Aleph Alpha, Alibaba, and Baidu are providing API access or integrating \\ntheir models into products rather than releasing parameters or training code.\\nThere are a few more notable institutions, such as the Technology Innovation In-\\nstitute (TII), an Abu Dhabi government-funded research institution, which open-\\nsourced Falcon LLM for research and commercial usage.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 42}, page_content='What Is Generative AI?20\\nThe complexity of estimating parameters in generative AI models suggests that smaller companies \\nor organizations without sufficient computation power and expertise may struggle to deploy \\nthese models successfully; although, recently, after the publication of the LLaMa models, we’ve \\nseen smaller companies making significant breakthroughs, for example, in terms of coding ability.\\nIn the next section, we’ll review the progress that DL and generative models have been making \\nover recent years that has led up to the current explosion of their apparent capabilities and the \\nattention these models have been getting.\\nLet’s get into the nitty-gritty details – how do these LLMs work under the hood? How do GPT \\nmodels work?\\nHow do GPT models work?\\nGenerative pre-training has been around for a while, employing methods such as Markov models \\nor other techniques. However, language models such as BERT and GPT were made possible by \\nthe transformer deep neural network architecture (Vaswani and others, Attention Is All You Need, \\n2017), which has been a game-changer for NLP. Designed to avoid recursion to allow parallel com-\\nputation, the Transformer architecture, in different variations, continues to push the boundaries \\nof what’s possible within the field of NLP and generative AI.\\nTransformers have pushed the envelope in NLP, especially in translation and language under -\\nstanding. Neural Machine Translation (NMT) is a mainstream approach to machine translation \\nthat uses DL to capture long-range dependencies in a sentence. Models based on transformers \\noutperformed previous approaches, such as using recurrent neural networks, particularly Long \\nShort-Term Memory (LSTM) networks.\\nThe transformer model architecture has an encoder-decoder structure, where the encoder maps \\nan input sequence to a sequence of hidden states, and the decoder maps the hidden states to an \\noutput sequence. The hidden state representations consider not only the inherent meaning of \\nthe words (their semantic value) but also their context in the sequence.\\nThe encoder is made up of identical layers, each with two sub-layers. The input embedding is \\npassed through an attention mechanism, and the second sub-layer is a fully connected feed-for-\\nward network. Each sub-layer is followed by a residual connection and layer normalization. The \\noutput of each sub-layer is the sum of the input and the output of the sub-layer, which is then \\nnormalized.\\nThe decoder uses this encoded information to generate the output sequence one item at a time, \\nusing the context of the previously generated items. It also has identical modules, with the same \\ntwo sub-layers as the encoder. '),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 43}, page_content='Chapter 1 21\\nIn addition, the decoder has a third sub-layer that performs Multi-Head Attention (MHA) over \\nthe output of the encoder stack. The decoder also uses residual connections and layer normaliza-\\ntion. The self-attention sub-layer in the decoder is modified to prevent positions from attending \\nto subsequent positions. This masking, combined with the fact that the output embeddings are \\noffset by one position, ensures that the predictions for position i can only depend on the known \\noutputs at positions less than i. These are indicated in the diagram here (source: Yuening Jia, \\nWikimedia Commons):\\nFigure 1.6: The Transformer architecture\\nThe architectural features that have contributed to the success of transformers are:\\n• Positional encoding: Since the transformer doesn’t process words sequentially but instead \\nprocesses all words simultaneously, it lacks any notion of the order of words. To remedy \\nthis, information about the position of words in the sequence is injected into the model \\nusing positional encodings. These encodings are added to the input embeddings repre -\\nsenting each word, thus allowing the model to consider the order of words in a sequence.\\n• Layer normalization: To stabilize the network’s learning, the transformer uses a tech -\\nnique called layer normalization. This technique normalizes the model’s inputs across \\nthe features dimension (instead of the batch dimension as in batch normalization), thus \\nimproving the overall speed and stability of learning.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 44}, page_content='What Is Generative AI?22\\n• Multi-head attention : Instead of applying attention once, the transformer applies it \\nmultiple times in parallel – improving the model’s ability to focus on different types of \\ninformation and thus capturing a richer combination of features.\\nA key reason for the success of transformers has been their ability to maintain performance across \\nlonger sequences better than other models, for example, recurrent neural networks.\\nThe basic idea behind attention mechanisms is to compute a weighted sum of the values (usu-\\nally referred to as values or content vectors) associated with each position in the input sequence, \\nbased on the similarity between the current position and all other positions. This weighted sum, \\nknown as the context vector, is then used as an input to the subsequent layers of the model, en-\\nabling the model to selectively attend to relevant parts of the input during the decoding process.\\nTo enhance the expressiveness of the attention mechanism, it is often extended to include mul-\\ntiple so-called heads, where each head has its own set of query, key, and value vectors, allowing \\nthe model to capture various aspects of the input representation. The individual context vectors \\nfrom each head are then concatenated or combined in some way to form the final output.\\nEarly attention mechanisms scaled quadratically with the length of the sequences (context size), \\nrendering them inapplicable to settings with long sequences. Different mechanisms have been \\ntried out to alleviate this. Many LLMs use some form of Multi-Query Attention (MQA), including \\nOpenAI’s GPT-series models, Falcon, SantaCoder, and StarCoder.\\nMQA is an extension of MHA, where attention computation is replicated multiple times. MQA \\nimproves the performance and efficiency of language models for various language tasks. By re -\\nmoving the heads dimension from certain computations and optimizing memory usage, MQA \\nallows for 11 times better throughput and 30% lower latency in inference tasks compared to \\nbaseline models without MQA.\\nLLaMa 2 and a few other models  used Grouped-Query Attention (GQA), which is a practice \\nused in autoregressive decoding to cache the key (K) and value (V) pairs for the previous tokens \\nin the sequence, speeding up attention computation. However, as the context window or batch \\nsizes increase, the memory costs associated with the KV cache size in MHA models also increase \\nsignificantly. To address this, the key and value projections can be shared across multiple heads \\nwithout much degradation of performance.\\nThere have been many other proposed approaches to obtain efficiency gains, such as sparse, low-\\nrank self-attention, and latent bottlenecks, to name just a few. Other work has tried to extend \\nsequences beyond the fixed input size; architectures such as transformer-XL reintroduce recur -\\nsion by storing hidden states of already encoded sentences to leverage them in the subsequent \\nencoding of the next sentences.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 45}, page_content='Chapter 1 23\\nThe combination of these architectural features allows GPT models to successfully tackle tasks \\nthat involve understanding and generating text in human language and other domains. The \\noverwhelming majority of LLMs are transformers, as are many other state-of-the-art models  \\nwe will encounter in the different sections of this chapter, including models for image, sound, \\nand 3D objects.\\nAs the name suggests, a particularity of GPTs lies in pre-training. Let’s see how these LLMs are \\ntrained!\\nPre-training\\nThe transformer is trained in two phases using a combination of unsupervised pre-training and \\ndiscriminative task-specific fine-tuning. The goal during pre-training is to learn a general-purpose \\nrepresentation that transfers to a wide range of tasks.\\nThe unsupervised pre-training can follow different objectives. In Masked Language Modeling  \\n(MLM), introduced in BERT: Pre-training of Deep Bidirectional Transformers for Language Under -\\nstanding by Devlin and others (2019), the input is masked out, and the model attempts to predict \\nthe missing tokens based on the context provided by the non-masked portion. For example, if \\nthe input sentence is “The cat [MASK] over the wall,” the model would ideally learn to predict \\n“jumped” for the mask.\\nIn this case, the training objective minimizes the differences between predictions and the masked \\ntokens according to a loss function. Parameters in the models are then iteratively updated ac -\\ncording to these comparisons.\\nNegative Log-Likelihood (NLL) and Perplexity (PPL) are important metrics used in training and \\nevaluating language models. NLL is a loss function used in ML algorithms, aimed at maximizing \\nthe probability of correct predictions. A lower NLL indicates that the network has successfully \\nlearned patterns from the training set, so it will accurately predict the labels of the training sam-\\nples. It’s important to mention that NLL is a value constrained within a positive interval.\\nPPL, on the other hand, is an exponentiation of NLL, providing a more intuitive way to understand \\nthe model’s performance. Smaller PPL values indicate a well-trained network that can predict \\naccurately while higher values indicate poor learning performance. Intuitively, we could say that \\na low perplexity means that the model is less surprised by the next word. Therefore, the goal in \\npre-training is to minimize perplexity, which means the model’s predictions align more with \\nthe actual outcomes.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 46}, page_content='What Is Generative AI?24\\nIn comparing different language models, perplexity is often used as a benchmark metric across \\nvarious tasks. It gives an idea about how well the language model is performing, where a lower \\nperplexity indicates the model is more certain of its predictions. Hence, a model with lower per-\\nplexity would be considered better performing in comparison to others with higher perplexity.\\nThe first step in training an LLM is tokenization . This process involves building a  vocabulary, \\nwhich maps tokens to unique numerical representations so that they can be processed by the \\nmodel, given that LLMs are mathematical functions that require numerical inputs and outputs.\\nTokenization\\nTokenizing a text means splitting it into tokens (words or subwords), which then are converted \\nto IDs through a look-up table mapping words in text to corresponding lists of integers.\\nBefore training the LLM, the tokenizer – more precisely, its dictionary – is typically fitted to the \\nentire training dataset and then frozen. It’s important to note that tokenizers do not produce \\narbitrary integers. Instead, they output integers within a specific range – from 0  to , where  \\nrepresents the vocabulary size of the tokenizer.\\nDefinitions:\\nToken: A token is an instance of a sequence of characters, typically forming a word, \\npunctuation mark, or number. Tokens serve as the base elements for constructing \\nsequences of text.\\nTokenization: This refers to the process of splitting text into tokens. A tokenizer \\nsplits on whitespace and punctuation to break text into individual tokens.\\nExamples:\\nConsider the following text:\\n“The quick brown fox jumps over the lazy dog!”\\nThis would get split into the following tokens:\\n[“The”, “quick”, “brown”, “fox”, “jumps”, “over”, “the”, “lazy”, “dog”, “!”]\\nEach word is an individual token, as is the punctuation mark.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 47}, page_content='Chapter 1 25\\nThere are a lot of tokenizers that work according to different principles, but common types of \\ntokenizers employed in models are Byte-Pair Encoding ( BPE), WordPiece, and SentencePiece. \\nFor example, LLaMa 2’s BPE tokenizer splits numbers into individual digits and uses bytes to \\ndecompose unknown UTF-8 characters. The total vocabulary size is 32K tokens.\\nIt is necessary to point out that LLMs can only generate outputs based on a sequence of tokens \\nthat does not exceed its context window. This context window refers to the length of the longest \\nsequence of tokens that an LLM can use. Typical context window sizes for LLMs can range from \\nabout 1,000 to 10,000 tokens.\\nNext, it is worth talking at least briefly about the scale of these architectures, and why these \\nmodels are as large as they are.\\nScaling\\nAs we’ve seen in Figure 1.5, language models have been becoming bigger over time. That corre -\\nsponds to a long-term trend in machine learning that models get bigger as computing resources \\nget cheaper, enabling higher performance. In a paper from 2020 by researchers from OpenAI, \\nKaplan and others (Scaling laws for neural language models, 2020) discussed scaling laws and the \\nchoice of parameters.\\nInterestingly, they compare lots of different architecture choices and, among other things, show \\nthat transformers outperform LSTMs as language models in terms of perplexity in no small part \\ndue to the improved use of long contexts. While recurrent networks plateau after less than 100 \\ntokens, transformers improve throughout the whole context. Therefore, transformers not only \\ncome with better training and inference speed but also give better performance when looking \\nat relevant contexts.\\nFurther, they found a power-law relationship between performance and each of the following \\nfactors: dataset size, model size (number of parameters), and the amount of computational re -\\nsources required for training. This implies that to improve performance by a certain factor, one of \\nthese elements must be scaled up by the power of that factor; however, for optimal performance, \\nall three factors must be scaled in tandem to avoid bottlenecks.\\nResearchers at DeepMind (An empirical analysis of compute-optimal large language model training; \\nHoffmann and others, 2022) analyzed the training compute and dataset size of LLMs and con-\\ncluded that LLMs are undertrained in terms of compute budget and dataset size as suggested \\nby scaling laws. '),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 48}, page_content='What Is Generative AI?26\\nThey predicted that large models would perform better if substantially smaller and trained for \\nmuch longer, and – in fact – validated their prediction by comparing a 70-billion-parameter \\nChinchilla model on a benchmark to their Gopher model, which consists of 280 billion parameters.\\nHowever, more recently, a team at Microsoft Research has challenged these conclusions and \\nsurprised everyone (Textbooks Are All You Need; Gunaseka and colleagues, June 2023), finding that \\na small network (350M parameters) trained on high-quality datasets can give very competitive \\nperformance. We’ll discuss this model again in Chapter 6, Developing Software with Generative AI, \\nand we’ll discuss the implications of scaling in Chapter 10, The Future of Generative Models.\\nIt will be instructive to observe whether model sizes for LLMs keep increasing at the same rate \\nas they have. This is an important question since it determines if the development of LLMs will \\nbe firmly in the hands of large organizations. It could be that there’s a saturation of performance \\nat a certain size, which only changes in the approach can overcome. However, we could see new \\nscaling laws linking performance with data quality.\\nAfter pre-training, a major step is how models are prepared for specific tasks either by fine-tuning \\nor prompting. Let’s see what this task conditioning is about!\\nConditioning\\nConditioning LLMs refers to adapting the model for specific tasks. It includes fine-tuning and \\nprompting:\\n• Fine-tuning involves modifying a pre-trained language model by training it on a specific \\ntask using supervised learning. For example, to make a model more amenable to chats \\nwith humans, the model is trained on examples of tasks formulated as natural language \\ninstructions (instruction tuning). For fine-tuning, pre-trained models are usually trained \\nagain using Reinforcement Learning from Human Feedback (RLHF) to be helpful and \\nharmless.\\n• Prompting techniques present problems in text form to generative models. There are a \\nlot of different prompting techniques, starting from simple questions to detailed instruc-\\ntions. Prompts can include examples of similar problems and their solutions. Zero-shot \\nprompting involves no examples, while few-shot prompting includes a small number of \\nexamples of relevant problem and solution pairs.\\nThese conditioning methods continue to evolve, becoming more effective and useful for a wide \\nrange of applications. Prompt engineering and conditioning methods will be explored further in \\nChapter 8, Customizing LLMs and Their Output.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 49}, page_content='Chapter 1 27\\nHow to try out these models\\nYou can access OpenAI’s model through their website or their API. If you want to try other LLMs \\non your laptop, open-source LLMs are a good place to get started. There is a whole zoo of stuff \\nout there!\\nYou can access these models through Hugging Face or other providers, as we’ll see starting in \\nChapter 3, Getting Started with LangChain. You can even download these open-source models, \\nfine-tune them, or fully train them. We’ll fine-tune a model in Chapter 8, Customizing LLMs and \\nTheir Output.\\nGenerative AI is extensively used in generating 3D images, avatars, videos, graphs, and illustra-\\ntions for virtual or augmented reality, video games graphic design, logo creation, image editing, \\nor enhancement. The most popular model category here is for text-conditioned image synthesis, \\nspecifically text-to-image generation. As mentioned, in this book, we’ll focus on LLMs, since \\nthey have the broadest practical application, but we’ll also have a look at image models, which \\nsometimes can be quite useful.\\nIn the next section, we’ll be reviewing state-of-the-art methods for text-conditioned image gen-\\neration. I’ll highlight the progress made in the field so far, but also discuss existing challenges \\nand potential future directions.\\nWhat are text-to-image models?\\nText-to-image models are a powerful type of generative AI that creates realistic images from \\ntextual descriptions. They have diverse use cases in creative industries and design for generating \\nadvertisements, product prototypes, fashion images, and visual effects. The main applications are:\\n• Text-conditioned image generation: Creating original images from text prompts like “a \\npainting of a cat in a field of flowers.” This is used for art, design, prototyping, and visual \\neffects.\\n• Image inpainting: Filling in missing or corrupted parts of an image based on the sur -\\nrounding context. This can restore damaged images (denoising, dehazing, and deblurring) \\nor edit out unwanted elements.\\n• Image-to-image translation : Converting input images to a different style or domain \\nspecified through text, like “make this photo look like a Monet painting.”\\n• Image recognition: Large foundation models can be used to recognize images, including \\nclassifying scenes, but also object detection, for example, detecting faces.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 50}, page_content='What Is Generative AI?28\\nModels like Midjourney, DALL-E 2, and Stable Diffusion provide creative and realistic images \\nderived from textual input or other images. These models work by training deep neural networks \\non large datasets of image-text pairs. The key technique used is diffusion models, which start with \\nrandom noise and gradually refine it into an image through repeated denoising steps.\\nPopular models like Stable Diffusion and DALL-E 2 use a text encoder to map input text into an \\nembedding space. This text embedding is fed into a series of conditional diffusion models, which \\ndenoise and refine a latent image in successive stages. The final model output is a high-resolution \\nimage aligned with the textual description.\\nTwo main classes of models are used: Generative Adversarial Networks ( GANs) and diffusion \\nmodels. GAN models like StyleGAN or GANPaint Studio can produce highly realistic images, but \\ntraining is unstable and computationally expensive. They consist of two networks that are pitted \\nagainst each other in a game-like setting – the generator, which generates new images from text \\nembeddings and noise, and the discriminator, which estimates the probability of the new data \\nbeing real. As these two networks compete, GANs get better at their task, generating realistic \\nimages and other types of data.\\nThe setup for training GANs is illustrated in this diagram (taken from A Survey on Text Genera-\\ntion Using Generative Adversarial Networks , G de Rosa and J P. Papa, 2022; https://arxiv.org/\\npdf/2212.11119.pdf):\\nFigure 1.7: GAN training\\nDiffusion models have become popular and promising for a wide range of generative tasks, in-\\ncluding text-to-image synthesis. These models offer advantages over previous approaches, such \\nas GANs, by reducing computation costs and sequential error accumulation. Diffusion models \\noperate through a process like diffusion in physics. They follow a forward diffusion process by \\nadding noise to an image until it becomes uncharacteristic and noisy. This process is analogous \\nto an ink drop falling into a glass of water and gradually diffusing.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 51}, page_content='Chapter 1 29\\nThe unique aspect of generative image models is the reverse diffusion process, where the model \\nattempts to recover the original image from a noisy, meaningless image. By iteratively applying \\nnoise removal transformations, the model generates images of increasing resolutions that align \\nwith the given text input. The final output is an image that has been modified based on the text \\ninput. An example of this is the Imagen text-to-image model (Photorealistic Text-to-Image Diffusion \\nModels with Deep Language Understanding  by Google Research, May 2022), which incorporates \\nfrozen text embeddings from LLMs, pre-trained on text-only corpora. A text encoder first maps \\nthe input text to a sequence of embeddings. A cascade of conditional diffusion models takes the \\ntext embeddings as input and generates images.\\nThe denoising process is demonstrated in this plot (source: user Benlisquare via Wikimedia Com-\\nmons):\\nFigure 1.8: European-style castle in Japan, created using the Stable Diffusion V1-5 AI diffusion \\nmodel\\nIn Figure 1.8, only some steps within the 40-step generation process are shown. You can see \\nthe image generation step by step, including the U-Net denoising process using the Denoising \\nDiffusion Implicit Model (DDIM) sampling method, which repeatedly removes Gaussian noise, \\nand then decodes the denoised output into pixel space.\\nWith diffusion models, you can see a wide variety of outcomes using only minimal changes to \\nthe initial setting of the model or – as in this case – numeric solvers and samplers. Although they \\nsometimes produce striking results, the instability and inconsistency are a significant challenge \\nto applying these models more broadly.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 52}, page_content='What Is Generative AI?30\\nStable Diffusion was developed by the CompVis group at LMU Munich (High-Resolution Image \\nSynthesis with Latent Diffusion Models by Blattmann and others, 2022). The Stable Diffusion model \\nsignificantly cuts training costs and sampling time compared to previous (pixel-based) diffusion \\nmodels. The model can be run on consumer hardware equipped with a modest GPU (for example, \\nthe GeForce 40 series). By creating high-fidelity images from text on consumer GPUs, the Stable \\nDiffusion model democratizes access. Further, the model’s source code and even the weights have \\nbeen released under the CreativeML OpenRAIL-M license, which doesn’t impose restrictions on \\nreuse, distribution, commercialization, and adaptation.\\nSignificantly, Stable Diffusion introduced operations in latent (lower-dimensional) space repre-\\nsentations, which capture the essential properties of an image, in order to improve computational \\nefficiency. A VAE provides latent space compression (called perceptual compression in the paper), \\nwhile a U-Net performs iterative denoising.\\nStable Diffusion generates images from text prompts through several clear steps:\\n1. It starts by producing a random tensor (random image) in the latent space, which serves \\nas the noise for our initial image.\\n2. A noise predictor (U-Net) takes in both the latent noisy image and the provided text prompt \\nand predicts the noise.\\n3. The model then subtracts the latent noise from the latent image.\\n4. Steps 2 and 3 are repeated for a set number of sampling steps, for instance, 40 times, as \\nshown in the plot.\\n5. Finally, the decoder component of the VAE transforms the latent image back into pixel \\nspace, providing the final output image.\\nA VAE is a model that encodes data into a learned, smaller representation (encoding). These rep-\\nresentations can then be used to generate new data similar to that used for training (decoding). \\nThis VAE is trained first.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 53}, page_content='Chapter 1 31\\nFor training the image generation model in the latent space itself ( latent diffusion model), a \\nloss function is used to evaluate the quality of the generated images. One commonly used loss \\nfunction is the Mean Squared Error ( MSE) loss, which quantifies the difference between the \\ngenerated image and the target image. The model is optimized to minimize this loss, encouraging \\nit to generate images that closely resemble the desired output.\\nThis training was performed on the LAION-5B dataset, consisting of billions of image-text pairs, \\nderived from Common Crawl data, comprising billions of image-text pairs from sources such as \\nPinterest, WordPress, Blogspot, Flickr, and DeviantArt.\\nA U-Net is a popular type of convolutional neural network (CNN) that has a sym-\\nmetric encoder-decoder structure. It is commonly used for image segmentation tasks, \\nbut in the context of Stable Diffusion, it can help to introduce and remove noise in \\nthe image. The U-Net takes a noisy image (seed) as input and processes it through a \\nseries of convolutional layers to extract features and learn semantic representations.\\nThese convolutional layers, typically organized in a contracting path, reduce the \\nspatial dimensions while increasing the number of channels. Once the contracting \\npath reaches the bottleneck of the U-Net, it then expands through a symmetric \\nexpanding path. In the expanding path, transposed convolutions (also known as \\nupsampling or deconvolutions) are applied to progressively upsample the spatial \\ndimensions while reducing the number of channels.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 54}, page_content='What Is Generative AI?32\\nThe following images illustrate text-to-image generation from a text prompt with diffusion \\n(source: Ramesh and others, Hierarchical Text-Conditional Image Generation with CLIP Latents, \\n2022; https://arxiv.org/abs/2204.06125):\\nFigure 1.9: Image generation from text prompts\\nOverall, image generation models such as Stable Diffusion and Midjourney process textual \\nprompts into generated images, leveraging the concept of forward and reverse diffusion processes \\nand operating in a lower-dimensional latent space for efficiency. But what about the conditioning \\nfor the model in the text-to-image use case?\\nThe conditioning process allows these models to be influenced by specific input textual prompts \\nor input types like depth maps or outlines for greater precision to create relevant images. These \\nembeddings are then processed by a text transformer and fed to the noise predictor, steering it \\nto produce an image that aligns with the text prompt.\\nIt’s out of the scope of this book to provide a comprehensive survey of generative AI models for \\nall modalities. However, let’s get a bit of an overview of what models can do for other domains.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 55}, page_content='Chapter 1 33\\nWhat can AI do in other domains?\\nGenerative AI models have demonstrated impressive capabilities across modalities including \\nsound, music, video, and 3D shapes. In the audio domain, models can synthesize natural speech, \\ngenerate original music compositions, and even mimic a speaker’s voice and the patterns of \\nrhythm and sound (prosody). Speech-to-text systems can convert spoken language into text \\n[Automatic Speech Recognition (ASR)]. For video, AI systems can create photorealistic footage \\nfrom text prompts and perform sophisticated editing like object removal. 3D models learned to \\nreconstruct scenes from images and generate intricate objects from textual descriptions.\\nThe following table summarizes some recent models in these domains:\\nModel Organization Year Domain Architecture Performance\\n3D-GQN DeepMind 2018 3D\\nDeep, iterative, latent \\nvariable density \\nmodels\\n3D scene \\ngeneration from \\n2D images\\nJukebox OpenAI 2020 Music VQ-VAE + transformer\\nHigh-fidelity \\nmusic generation \\nin different styles\\nWhisper OpenAI 2022\\nSound/\\nspeech Transformer\\nNear human-\\nlevel speech \\nrecognition\\nImagen Video Google 2022 Video\\nFrozen text \\ntransformers + video \\ndiffusion models\\nHigh-definition \\nvideo generation \\nfrom text\\nPhenaki\\nGoogle & \\nUCL 2022 Video\\nBidirectional masked \\ntransformer\\nRealistic video \\ngeneration from \\ntext\\nTecoGAN U. Munich 2022 Video\\nTemporal coherence \\nmodule\\nHigh-quality, \\nsmooth video \\ngeneration\\nDreamFusion Google 2022 3D NeRF + Diffusion\\nHigh-fidelity 3D \\nobject generation \\nfrom text'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 56}, page_content='What Is Generative AI?34\\nAudioLM Google 2023\\nSound/\\nspeech\\nTokenizer + \\ntransformer LM + \\ndetokenizer\\nHigh linguistic \\nquality speech \\ngeneration \\nmaintaining \\nspeaker’s identity\\nAudioGen Meta AI 2023\\nSound/\\nspeech\\nTransformer + text \\nguidance\\nHigh-quality \\nconditional and \\nunconditional \\naudio generation\\nUniversal \\nSpeech \\nModel (USM) Google 2023\\nSound/\\nspeech\\nEncoder-decoder \\ntransformer\\nState-of-the-art \\nmultilingual \\nspeech \\nrecognition\\nTable 1.1: Models for audio, video, and 3D domains\\nUnderlying many of these innovations are advances in deep generative architectures like GANs, \\ndiffusion models, and transformers. Leading AI labs at Google, OpenAI, Meta, and DeepMind are \\npushing the boundaries of what’s possible.\\nSummary\\nWith the rise of computing power, deep neural networks, transformers, generative adversarial \\nnetworks, and VAEs model the complexity of real-world data much more effectively than previ-\\nous generations of models, pushing the boundaries of what’s possible with AI algorithms. In this \\nchapter, we explored the recent history of DL and AI and generative models such as LLMs and GPTs, \\ntogether with the theoretical ideas underpinning them, especially the Transformer architecture. \\nWe also explained the basic concepts of models for image generation, such as the Stable Diffu-\\nsion model, and finally discussed applications beyond text and images, such as sound and video.\\nThe next chapter will explore the tooling of generative models, particularly LLMs, with the LangC-\\nhain framework, focusing on the fundamentals, the implementation, and the use of this particular \\ntool in exploiting and extending the capability of LLMs.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 57}, page_content='Chapter 1 35\\nQuestions\\nI think it’s a good habit to check that you’ve digested the material when reading a technical book. \\nFor this purpose, I’ve created a few questions relating to the content of this chapter. Let’s see if \\nyou can answer them:\\n1. What is a generative model?\\n2. Which applications exist for generative models?\\n3. What is an LLM and what does it do?\\n4. How can we get better performance from LLMs?\\n5. What are the conditions that make these models possible?\\n6. Which companies and organizations are the big players in developing LLMs?\\n7. What is a transformer and what does it consist of?\\n8. What does GPT stand for?\\n9. How does Stable Diffusion work?\\n10. What is a VAE?\\nIf you struggle to answer these questions, please refer to the corresponding sections in this chapter \\nto ensure that you’ve understood the material.\\nJoin our community on Discord\\nJoin our community’s Discord space for discussions with the authors and other readers:\\nhttps://packt.link/lang'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 58}, page_content=''),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 59}, page_content='2\\nLangChain for LLM Apps\\nLarge Language Models (LLMs) like GPT-4 have demonstrated immense capabilities in generating \\nhuman-like text. However, simply accessing LLMs via APIs has limitations. Instead, combining \\nthem with other data sources and tools can enable more powerful applications. In this chapter, \\nwe will introduce LangChain as a way to overcome LLM limitations and build innovative lan -\\nguage-based applications. We aim to demonstrate the potential of combining recent AI advance-\\nments with a robust framework like LangChain.\\nWe will start by outlining some challenges faced when using LLMs on their own, like the lack of \\nexternal knowledge, incorrect reasoning, and the inability to take action. LangChain provides \\nsolutions to these issues through different integrations and off-the-shelf components for specific \\ntasks. We will walk through examples of how developers can use LangChain’s capabilities to cre-\\nate customized natural language processing solutions, outlining the components and concepts \\ninvolved.\\nThe goal is to illustrate how LangChain enables building dynamic, data-aware applications that \\ngo beyond what is possible by simply accessing LLMs via API calls. Lastly, we will talk about \\nimportant concepts related to LangChain, such as chains, action plan generation, and memory, \\nwhich are important concepts to understand how LangChain works.\\nThe main sections of this chapter are:\\n• Going beyond stochastic parrots\\n• What is LangChain?\\n• Exploring key components of LangChain\\n• How does LangChain work?\\n• Comparing LangChain with other frameworks'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 60}, page_content='LangChain for LLM Apps38\\nGoing beyond stochastic parrots\\nLLMs have gained significant attention and popularity due to their ability to generate human-like \\ntext and understand natural language, which makes them useful in scenarios that revolve around \\ncontent generation, text classification, and summarization. However, their apparent fluency \\nobscures serious deficiencies that constrain real-world utility. The concept of stochastic parrots \\nhelps to elucidate this fundamental issue.\\nStochastic parrots refers to LLMs that can produce convincing language but lack any true com-\\nprehension of the meaning behind words. Coined by researchers Emily Bender, Timnit Gebru, \\nMargaret Mitchell, and Angelina McMillan-Major in their influential paper On the Dangers of \\nStochastic Parrots (2021), the term critiques models that mindlessly mimic linguistic patterns. \\nWithout being grounded in the real world, models can produce responses that are inaccurate, \\nirrelevant, unethical, or make little logical sense.\\nSimply scaling up compute and data does not impart reasoning capabilities or common sense. \\nLLMs struggle with challenges like the compositionality gap (Measuring and Narrowing the Com-\\npositionality Gap in Language Models by Ofir Press and colleagues; 2023). This means LLMs cannot \\nconnect inferences or adapt responses to new situations. Overcoming these obstacles requires \\naugmenting LLMs with techniques that add true comprehension. Raw model scale alone cannot \\ntransform stochastic parroting into beneficial systems. Innovations like prompting, chain-of-\\nthought reasoning, retrieval grounding, and others are needed to educate models.\\nLet’s look at this argument in a bit more detail. If you wish to skip these details, please move on \\nto the next section. We’ll continue here by looking at the limitations of LLMs, ways to overcome \\nthose limitations, and how LangChain facilitates applications that systematically mitigate the \\nshortcomings and extend the functionality of LLMs.\\nWhat are the limitations of LLMs?\\nAs has been established, LLMs offer impressive capabilities but suffer from limitations that hinder \\ntheir effectiveness in certain scenarios. Understanding these limitations is crucial when devel-\\noping applications. Some pain points associated with LLMs include:\\n• Outdated knowledge: LLMs rely solely on their training data. Without external integra-\\ntion, they cannot provide recent real-world information.\\n• Inability to take action: LLMs cannot perform interactive actions like searches, calcula-\\ntions, or lookups. This severely limits functionality.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 61}, page_content='Chapter 2 39\\n• Lack of context: LLMs struggle to incorporate relevant context like previous conversa -\\ntions and the supplementary details that are needed for coherent and useful responses.\\n• Hallucination risks: Insufficient knowledge on certain topics can lead to the generation \\nof incorrect or nonsensical content by LLMs if not properly grounded.\\n• Biases and discrimination: Depending on the data they were trained on, LLMs can exhibit \\nbiases that can be religious, ideological, or political in nature.\\n• Lack of transparency: The behavior of large, complex models can be opaque and difficult \\nto interpret, posing challenges to alignment with human values.\\n• Lack of context: LLMs may struggle to understand and incorporate context from previous \\nprompts or conversations. They may not remember previously mentioned details or may \\nfail to provide additional relevant information beyond the given prompt.\\nLet’s illustrate some of these limitations a bit more since they are very important. As mentioned, \\nLLMs face significant limitations in their lack of real-time knowledge and inability to take actions \\nthemselves, which restricts their effectiveness in many real-world contexts. For instance, LLMs \\nhave no inherent connection to external information sources. They are confined to the training \\ndata used to develop them, which inevitably becomes increasingly outdated over time. An LLM \\nwould have zero awareness of current events that occurred after its training data cut-off date. \\nAsking an LLM about breaking news or the latest societal developments would leave it unable \\nto construct responses without external grounding.\\nAdditionally, LLMs cannot interact dynamically with the world around them. They cannot check \\nthe weather, look up local data, or access documents. With no ability to perform web searches, \\ninterface with APIs, run calculations, or take any practical actions based on new prompts, LLMs \\noperate solely within the confines of pre-existing information. Even when discussing topics con-\\ntained in its training data, an LLM struggles to incorporate real-time context and specifics with-\\nout retrieving external knowledge. For example, an LLM could fluently discuss macroeconomic \\nprinciples used in financial analysis, but it would fail to actually conduct analysis by retrieving \\ncurrent performance data and computing relevant statistics. Without dynamic lookup abilities, its \\nfinancial discussion remains generic and theoretical. Similarly, an LLM may eloquently describe \\na past news event but then falter if asked for the latest developments on the same story today.\\nArchitecting solutions that combine LLMs with external data sources, analytical programs, and \\ntool integrations can help overcome these limitations. But in isolation, LLMs lack connection to \\nthe real-world context, which is often essential for useful applications. Their impressive natural \\nlanguage abilities need appropriate grounding and actions to produce substantive insights be -\\nyond eloquent but hollow text.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 62}, page_content='LangChain for LLM Apps40\\nLet’s look at a few examples of problems with LLMs. The cut-off day issue is illustrated here in \\nthe OpenAI ChatGPT interface asking about LangChain:\\nFigure 2.1: ChatGPT – a lack of up-to-date information\\nIn this case, the model was able to correctly catch the problem and give the correct feedback – \\nthis is not always the case, though. If you access the model through other endpoints or use other \\nmodels, it might just make up the information (hallucinate). Also, it might not have knowledge \\nabout certain entities, or it may refer to different entities entirely. Asking the same question in \\nthe OpenAI playground, I got this response:\\nFigure 2.2: OpenAI playground with GPT 3.5'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 63}, page_content='Chapter 2 41\\nIn this case, we can see that the model talks about a different LangChain, which is a decentralized \\nblockchain-based translation platform. This is a problem of relevance, which can be referred \\nto as a hallucination. It can be remedied by accessing external data, such as weather APIs, user \\npreferences, or relevant information from the web, and this is essential for creating personalized \\nand accurate language-driven applications.\\nLLMs struggle with certain tasks that involve logical reasoning or math problems. As an example, \\neven advanced LLMs perform poorly at high-school level math and cannot perform simple math \\noperations that they haven’t seen before. Again, we can illustrate this with a simple demonstration:\\nFigure 2.3: ChatGPT math solving\\nAs you can see, the model comes up with the correct response for the first question but fails with the \\nsecond. Just in case you were wondering what the true result is, if we use a calculator, we get this:\\nFigure 2.4: Multiplication with a calculator (BC)'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 64}, page_content='LangChain for LLM Apps42\\nThe LLM hasn’t stored the result of the calculation or hasn’t encountered it often enough in the \\ntraining data for it to be reliably remembered as encoded in its weights. Therefore, it fails to cor-\\nrectly come up with the solution. An LLM is not a suitable tool for the job in this case.\\nDeploying chatbots and other applications using LLMs requires thoughtful design and monitoring \\nto address risks like bias and inappropriate content. For instance, Microsoft’s Tay chatbot was \\ntaken offline shortly after launch in 2016 due to offensive tweets resulting from toxic interactions.\\nAs for reasoning, for example, an LLM may correctly identify a fruit’s density and water’s density \\nwhen asked about those topics independently, but it would struggle to synthesize those facts to \\ndetermine if the fruit will float (this being a multi-hop question). The model fails to bridge its \\ndisjointed knowledge.\\nLet’s see how we can address these challenges.\\nHow can we mitigate LLM limitations?\\nMitigating these limitations includes techniques like:\\n• Retrieval augmentation: This technique accesses knowledge bases to supplement an \\nLLM’s outdated training data, providing external context and reducing hallucination risk.\\n• Chaining: This technique integrates actions like searches and calculations.\\n• Prompt engineering: This involves the careful crafting of prompts by providing critical \\ncontext that guides appropriate responses.\\n• Monitoring, filtering, and reviews : This involves ongoing and effective oversight of \\nemerging issues regarding the application’s input and output to detect issues. Both man-\\nual reviews and automated filters then correct potential problems with the output. This \\nincludes the following:\\na. Filters, like block lists, sensitivity classifiers, and banned word filters, can auto -\\nmatically flag issues.\\nb. Constitutional principles monitor and filter unethical or inappropriate content.\\nc. Human reviews provide insight into model behavior and output.\\n• Memory: Retains conversation context by persisting conversation data and context across \\ninteractions.\\n• Fine-tuning: Training and tuning the LLM on more appropriate data for the application \\ndomain and principles. This adapts the model’s behavior for its specific purpose.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 65}, page_content='Chapter 2 43\\nTo re-emphasize what we previously mentioned, raw model scale alone cannot impart composi-\\ntional reasoning or other missing capabilities. Explicit techniques like elicit prompting and chain-\\nof-thought reasoning are needed to overcome the compositionality gap. Approaches like self-ask \\nprompting mitigate these flaws by encouraging models to methodically decompose problems.\\nIntegrating such tools into training pipelines provides the otherwise lacking faculties. Prompting \\nsupplies context, chaining enables inference steps, and retrieval incorporates facts. Together, \\nthese transform stochastic parrots into reasoning engines.\\nThoughtful prompt engineering and fine-tuning prepare models for real-world use. Ongoing \\nmonitoring then catches any emerging issues, both through automation and human review. \\nFilters act as a first line of defense. Adopting constitutional AI principles also encourages build-\\ning models capable of behaving ethically. This comprehensive approach combines preparation, \\nvigilance, and inherently beneficial design.\\nConnecting LLMs to external data further reduces hallucination risks and enhances responses \\nwith accurate, up-to-date information. However, securely integrating sources like databases adds \\ncomplexity. Frameworks like LangChain simplify this while providing structure and oversight \\nfor responsible LLM use. They allow composing prompted model queries and data sources to \\nsurmount standalone LLM deficits. With diligent augmentation, we can create AI systems pre -\\nviously not viable due to innate model limitations. This brings us to our next topic of discussion.\\nWhat is an LLM app?\\nCombining LLMs with other tools into applications using specialized tooling, LLM-powered \\napplications have the potential to transform our digital world. This is often done via a chain of \\none or multiple prompted calls to LLMs but can also make use of other external services (such \\nas APIs or data sources) to achieve tasks.\\nTraditional software applications typically follow a multi-layer architecture:\\nFigure 2.5: A traditional software application\\nThe client layer handles user interaction. The frontend layer handles presentation and business \\nlogic. The backend layer processes logic, APIs, computations, etc. Lastly, the database stores and \\nretrieves data.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 66}, page_content='LangChain for LLM Apps44\\nIn contrast, an LLM app is an application that utilizes an LLM to understand natural language \\nprompts and generate responsive text outputs. LLM apps typically have the following components:\\n• A client layer to collect user input as text queries or decisions.\\n• A prompt engineering layer to construct prompts that guide the LLM.\\n• An LLM backend to analyze prompts and produce relevant text responses.\\n• An output parsing layer to interpret LLM responses for the application interface.\\n• Optional integration with external services via function APIs, knowledge bases, and rea-\\nsoning algorithms to augment the LLM’s capabilities.\\nIn the simplest possible cases, the frontend, parsing, and knowledge base parts are sometimes \\nnot explicitly defined, leaving us with just the client, the prompt, and the LLM:\\nFigure 2.6: A simple LLM application\\nLLM apps can integrate external services via:\\n• Function APIs to access web tools and databases.\\n• Advanced reasoning algorithms for complex logic chains.\\n• Retrieval augmented generation via knowledge bases.\\nRetrieval augmented generation (RAG), which we will discuss in Chapter 5, Building a Chatbot like \\nChatGPT, enhances the LLM with external knowledge. These extensions expand the capabilities \\nof LLM apps beyond the LLM’s knowledge alone. For instance:\\n• Function calling allows parameterized API requests.\\n• SQL functions enable conversational database queries.\\n• Reasoning algorithms like chain-of-thought facilitate multi-step logic.\\nThis is illustrated here:\\nFigure 2.7: An advanced LLM application'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 67}, page_content='Chapter 2 45\\nAs can be seen in the preceding figure, the client layer collects user text queries and decisions. \\nPrompt engineering constructs guide the LLM, considering external knowledge or capability (or \\nearlier interactions) without changes to the model itself. The LLM backend dynamically under -\\nstands and responds to the prompts based on its training. Output parsing interprets the LLM \\ntext for the frontend. A knowledge base can enhance the LLM’s information, and optionally, like \\na database backend in a traditional app, information can be written to it.\\nLLM applications are important for several reasons:\\n• The LLM backend handles language in a nuanced, human-like way without hardcoded \\nrules.\\n• Responses can be personalized and contextualized based on past interactions.\\n• Advanced reasoning algorithms enable complex, multi-step inference chains.\\n• Dynamic responses based on the LLM or on up-to-date information retrieved in real time.\\nThe key capability LLM apps use is the ability to understand nuanced language in prompts and \\ngenerate coherent, human-like text responses. This facilitates more natural interactions and \\nworkflows compared to traditional code.\\nThe LLM provides human-like language capabilities without manual coding. Therefore, there is \\nno need to manually anticipate and code every language scenario in advance. The integration \\nof LLMs with external services, knowledge, and reasoning algorithms eases the development of \\ninnovative applications.\\nBut responsible data practices are critical – PII should be kept off public platforms and models \\nshould be fine-tuned in-house when needed. Both the frontend and the output parser could \\ninclude moderation and enforcing rules about behavior, privacy, and security. Future research \\nmust address concerns around potential misuse, biases, and limitations.\\nWe will see a lot of examples of LLM apps throughout this book; here are a few that we’ll encounter:\\n• Chatbots and virtual assistants:  These apps use LLMs like ChatGPT to have natural \\nconversations with users and assist with tasks like scheduling, customer service, and \\ninformation lookup.\\n• Intelligent search engines: LLM apps can parse search queries written in natural language \\nand generate relevant results.\\n• Automated content creation: Apps can leverage LLMs to generate content like articles, \\nemails, code, and more based on a text prompt.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 68}, page_content='LangChain for LLM Apps46\\n• Question answering: Users can ask an LLM app questions in plain language and receive \\ninformative answers that are quickly sourced from the model’s knowledge.\\n• Sentiment analysis: You can analyze customer feedback, reviews, and social posts using \\nan LLM app to summarize sentiment and extract key themes.\\n• Text summarization: You can automatically generate concise summaries of longer text \\ndocuments and articles using an LLM backend.\\n• Data analysis: You can use LLMs for automated data analysis and visualization to extract \\ninsights.\\n• Code generation: You can set up software pair-programming assistants that can help \\nsolve business problems.\\nThe true power of LLMs lies not in LLMs being used in isolation but in LLMs being combined \\nwith other sources of knowledge and computation. The LangChain framework aims to enable \\nprecisely this kind of integration, facilitating the development of context-aware, reasoning-based \\napplications. LangChain addresses pain points associated with LLMs and provides an intuitive \\nframework for creating customized NLP solutions.\\nWhat is LangChain?\\nCreated in 2022 by Harrison Chase, LangChain is an open-source Python framework for building \\nLLM-powered applications. It provides developers with modular, easy-to-use components for \\nconnecting language models with external data sources and services. The project has attracted \\nmillions in venture capital funding from the likes of Sequoia Capital and Benchmark, who sup -\\nplied funding to Apple, Cisco, Google, WeWork, Dropbox, and many other successful companies.\\nLangChain simplifies the development of sophisticated LLM applications by providing reusable \\ncomponents and pre-assembled chains. Its modular architecture abstracts access to LLMs and \\nexternal services into a unified interface. Developers can combine these building blocks to carry \\nout complex workflows.\\nBuilding impactful LLM apps involves challenges like prompt engineering, bias mitigation, pro-\\nductionizing, and integrating external data. LangChain reduces this learning curve through its \\nabstractions and composable structure.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 69}, page_content='Chapter 2 47\\nBeyond basic LLM API usage, LangChain facilitates advanced interactions like conversational \\ncontext and persistence through agents and memory. This allows for chatbots, gathering external \\ndata, and more.\\nIn particular, LangChain’s support for chains, agents, tools, and memory allows developers to \\nbuild applications that can interact with their environment in a more sophisticated way and store \\nand reuse information over time. Its modular design makes it easy to build complex applications \\nthat can be adapted to a variety of domains. Support for action plans and strategies improves \\nthe performance and robustness of applications. The support for memory and access to external \\ninformation reduces hallucinations, thus enhancing reliability.\\nThe key benefits LangChain offers developers are:\\n• Modular architecture for flexible and adaptable LLM integrations.\\n• Chaining together multiple services beyond just LLMs.\\n• Goal-driven agent interactions instead of isolated calls.\\n• Memory and persistence for statefulness across executions.\\n• Open-source access and community support.\\nAs mentioned, LangChain is open source and written in Python, although companion projects \\nexist that are implemented in JavaScript or – more precisely – TypeScript (LangChain.js), and the \\nfledgling Langchain.rb project for Ruby, which comes with a Ruby interpreter for code execution. \\nIn this book, we focus on the Python flavor of the framework.\\nWhile resources like documentation, courses, and communities help accelerate the learning pro-\\ncess, developing expertise in applying LLMs takes dedicated time and effort. For many developers, \\nthe learning curve can be a blocking factor to impactfully leveraging LLMs.\\nThere are active discussions on a Discord chat server, multiple blogs, and regular meetups taking \\nplace in various cities, including San Francisco and London. There’s even a chatbot, ChatLangC-\\nhain, that can answer questions about the LangChain documentation. It’s built using LangChain \\nand FastAPI and is available online through the documentation website!'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 70}, page_content='LangChain for LLM Apps48\\nLangChain comes with many extensions and a larger ecosystem that is developing around it. \\nAs mentioned, it has an immense number of integrations already, with many new ones being \\nadded every week. This screenshot highlights a few of the integrations (source: integrations.\\nlangchain.com):\\nFigure 2.8: LangChain integrations as of September 2023\\nAs for the broader ecosystem, LangSmith is a platform that complements LangChain by providing \\nrobust debugging, testing, and monitoring capabilities for LLM applications. For example, devel-\\nopers can quickly debug new chains by viewing detailed execution traces. Alternative prompts \\nand LLMs can be evaluated against datasets to ensure quality and consistency. Usage analytics \\nempower data-driven decisions around optimizations.\\nLlamaHub and LangChainHub provide open libraries of reusable elements to build sophisticated \\nLLM systems in a simplified manner. LlamaHub is a library of data loaders, readers, and tools \\ncreated by the LlamaIndex community. It provides utilities to easily connect LLMs to diverse \\nknowledge sources. The loaders ingest data for retrieval, while tools enable models to read/write \\nto external data services. LlamaHub simplifies the creation of customized data agents to unlock \\nLLM capabilities.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 71}, page_content='Chapter 2 49\\nLangChainHub is a central repository for sharing artifacts like prompts, chains, and agents used \\nin LangChain. Inspired by the Hugging Face Hub, it aims to be a one-stop resource for discover-\\ning high-quality building blocks to compose complex LLM apps. The initial launch focuses on a \\ncollection of reusable prompts. Future plans involve adding support for chains, agents, and other \\nkey LangChain components.\\nLangFlow and Flowise are UIs that allow chaining LangChain components in an executable \\nflowchart by dragging sidebar components onto the canvas and connecting them together to \\ncreate your pipeline. This is a quick way to experiment and prototype pipelines and is illustrated \\nin the following screenshot of Flowise (source: https://github.com/FlowiseAI/Flowise):\\nFigure 2.9: Flowise UI with an agent that uses an LLM, a calculator, and a search tool\\nYou can see an agent (discussed later in this chapter) that is connected to a search interface (Serp \\nAPI), an LLM, and a calculator. LangChain and LangFlow can be deployed locally, for example, \\nusing the Chainlit library, or on different platforms, including Google Cloud. The langchain-\\nserve library helps to deploy both LangChain and LangFlow on the Jina AI cloud as LLM-apps-\\nas-a-service with a single command.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 72}, page_content='LangChain for LLM Apps50\\nWhile still relatively new, LangChain unlocks more advanced LLM applications via its combina-\\ntion of components like memory, chaining, and agents. It aims to simplify what can otherwise \\nbe complex LLM application development. Hence, it is crucial at this point in the chapter that \\nwe shift focus to the workings of LangChain and its components.\\nExploring key components of LangChain\\nChains, agents, memory, and tools enable the creation of sophisticated LLM applications that \\ngo beyond basic API calls to a single LLM. In the following dedicated subsections on these key \\nconcepts, we’ll consider how they enable the development of capable systems by combining \\nlanguage models with external data and services.\\nWe won’t dive into implementation patterns in this chapter; however, we will discuss in more \\ndetail what some of these components are good for. By the end, you should have the level of un-\\nderstanding that’s required to architect systems with LangChain. Let’s start with chains!\\nWhat are chains?\\nChains are a critical concept in LangChain for composing modular components into reusable \\npipelines. For example, developers can put together multiple LLM calls and other components \\nin a sequence to create complex applications for things like chatbot-like social interactions, data \\nextraction, and data analysis. In the most generic terms, a chain is a sequence of calls to compo-\\nnents, which can include other chains. The most innocuous example of a chain is probably the \\nPromptTemplate, which passes a formatted response to a language model.\\nPrompt chaining is a technique that can be used to improve the performance of LangChain \\napplications, which involves chaining together multiple prompts to autocomplete a more com-\\nplex response. More complex chains integrate models with tools like LLMMath, for math-related \\nqueries, or SQLDatabaseChain, for querying databases. These are called utility chains, because \\nthey combine language models with specific tools.\\nChains can even enforce policies, like moderating toxic outputs or aligning with ethical princi-\\nples. LangChain implements chains to make sure the content of the output is not toxic, does not \\notherwise violate OpenAI’s moderation rules (OpenAIModerationChain), or that it conforms to \\nethical, legal, or custom principles (ConstitutionalChain).'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 73}, page_content='Chapter 2 51\\nAn LLMCheckerChain verifies statements to reduce inaccurate responses using a technique called \\nself-reflection. The LLMCheckerChain can prevent hallucinations and reduce inaccurate respons-\\nes by verifying the assumptions underlying the provided statements and questions. In a paper \\nby researchers at Carnegie Mellon, Allen Institute, University of Washington, NVIDIA, UC San \\nDiego, and Google Research in May 2023 (SELF-REFINE: Iterative Refinement with Self-Feedback), \\nthis strategy has been found to improve task performance by about 20% on average across a \\nbenchmark including dialogue responses, math reasoning, and code reasoning.\\nA few chains can make autonomous decisions. Like agents, router chains can decide which tool \\nto use based on their descriptions. A RouterChain can dynamically select which retrieval system, \\nsuch as prompts or indexes, to use.\\nChains deliver several key benefits:\\n• Modularity: Logic is divided into reusable components.\\n• Composability: Components can be sequenced flexibly.\\n• Readability: Each step in a pipeline is clear.\\n• Maintainability: Steps can be added, removed, and swapped.\\n• Reusability: Common pipelines become configurable chains.\\n• Tool integration: Easily incorporate LLMs, databases, APIs, etc.\\n• Productivity: Quickly build prototypes of configurable chains.\\nTogether, these benefits enable the encapsulation of complex workflows into easy-to-understand \\nand adaptable chained pipelines.\\nTypically, developing a LangChain chain involves breaking down a workflow into logical steps, \\nlike data loading, processing, model querying, and so on. Well-designed chains embrace sin -\\ngle-responsibility components being pipelined together. Steps should be stateless functions \\nto maximize reusability. Configurations should be made customizable. Robust error handling \\nwith exceptions and errors is critical for reliability. Monitoring and logging can be enabled with \\ndifferent mechanisms, including callbacks.\\nLet’s discuss agents next and how they make their decisions!'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 74}, page_content='LangChain for LLM Apps52\\nWhat are agents?\\nAgents are a key concept in LangChain for creating systems that interact dynamically with users \\nand environments over time. An agent is an autonomous software entity that is capable of taking \\nactions to accomplish goals and tasks.\\nChains and agents are similar concepts and it’s worth unpicking their differences. The core idea \\nin LangChain is the compositionality of LLMs and other components to work together. Both \\nchains and agents do that, but in different ways. Both extend LLMs, but agents do so by orches-\\ntrating chains while chains compose lower-level modules. While chains define reusable logic by \\nsequencing components, agents leverage chains to take goal-driven actions. Agents combine and \\norchestrate chains. The agent observes the environment, decides which chain to execute based \\non that observation, takes the chain’s specified action, and repeats.\\nAgents decide which actions to take using LLMs as reasoning engines. The LLM is prompted with \\navailable tools, user input, and previous steps. It then selects the next action or final response.\\nTools (discussed later in this chapter) are functions the agent calls to take real-world actions. \\nProviding the right tools and effectively describing them is critical for agents to accomplish goals.\\nThe agent executor runtime orchestrates the loop of querying the agent, executing tool actions, \\nand feeding observations back. This handles lower-level complexities like error handling, logging, \\nand parsing.\\nAgents provide several key benefits:\\n• Goal-oriented execution: Agents can plan chains of logic targeting specific goals.\\n• Dynamic responses: Observing environment changes lets agents react and adapt.\\n• Statefulness: Agents can maintain memory and context across interactions.\\n• Robustness: Errors can be handled by catching exceptions and trying alternate chains.\\n• Composition: Agent logic combines reusable component chains.\\nTogether, this enables agents to handle complex, multi-step workflows and continuously inter-\\nactive applications like chatbots.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 75}, page_content='Chapter 2 53\\nIn the section about the limitations of LLMs, we’ve seen that for calculations, a simple calculator \\noutperforms a model consisting of billions of parameters. In this case, an agent can decide to pass \\nthe calculation to a calculator or to a Python interpreter. We can see a simple app here, where an \\nagent is connected to both an OpenAI model and a Python function:\\nFigure 2.10: A simple LLM app with a Python function visualized in LangFlow\\nBased on the input, the agent can decide to run a Python function. Each agent also decides which \\ntool to use and when. We’ll look more at the mechanics of how this works in Chapter 4, Building \\nCapable Assistants.\\nA key limitation of agents and chains is their statelessness – each execution occurs in isolation \\nwithout retaining prior context. This is where the concept of memory becomes critical. Memory \\nin LangChain refers to persisting information across chain executions to enable statefulness.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 76}, page_content='LangChain for LLM Apps54\\nWhat is memory?\\nIn LangChain, memory refers to the persisting state between executions of a chain or agent. Robust \\nmemory approaches unlock key benefits for developers building conversational and interactive \\napplications. For example, storing chat history context in memory improves the coherence and \\nrelevance of LLM responses over time.\\nRather than treating each user input as an isolated prompt, chains can pass conversational mem-\\nory to models on each call to provide consistency. Agents can also persist facts, relationships, and \\ndeductions about the world in memory. This knowledge remains available even as real-world \\nconditions change, keeping the agent contextually informed. Memory of objectives and completed \\ntasks allows agents to track progress on multi-step goals across conversations. In addition, re -\\ntaining information in memory reduces the number of calls to LLMs for repetitive information. \\nThis lowers API usage and costs, while still providing the agent or chain with the needed context.\\nLangChain provides a standard interface for memory, integrations with storage options like \\ndatabases, and design patterns for effectively incorporating memory into chains and agents.\\nSeveral memory options exist – for example:\\n• ConversationBufferMemory stores all messages in model history. This increases latency \\nand costs.\\n• ConversationBufferWindowMemory retains only recent messages.\\n• ConversationKGMemory summarizes exchanges as a knowledge graph for integration \\ninto prompts.\\n• EntityMemory backed by a database persists agent state and facts.\\nMoreover, LangChain integrates many database options for durable storage:\\n• SQL options like Postgres and SQLite enable relational data modeling.\\n• NoSQL choices like MongoDB and Cassandra facilitate scalable unstructured data.\\n• Redis provides an in-memory database for high-performance caching.\\n• Managed cloud services like AWS DynamoDB remove infrastructure burdens.\\nBeyond databases, purpose-built memory servers like Remembrall and Motörhead offer optimized \\nconversational context. The right memory approach depends on factors like persistence needs, \\ndata relationships, scale, and resources, but robustly retaining state is key for conversational and \\ninteractive applications.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 77}, page_content='Chapter 2 55\\nLangChain’s memory integrations, from short-term caching to long-term databases, enable the \\nbuilding of stateful, context-aware agents. Architecting effective memory patterns unlocks the \\nnext generation of capable and reliable AI systems. LangChain comes with a long list of tools \\nthat we can use in applications. A short section will not be able to do this justice; however, I’ll \\nattempt to give a brief overview.\\nWhat are tools?\\nTools provide modular interfaces for agents to integrate external services like databases and \\nAPIs. Toolkits group tools that share resources. Tools can be combined with models to extend \\ntheir capability. LangChain offers tools like document loaders, indexes, and vector stores, which \\nfacilitate the retrieval and storage of data for augmenting data retrieval in LLMs.\\nThere are many tools available, and here are just a few examples:\\n• Machine translator: A language model can use a machine translator to better compre -\\nhend and process text in multiple languages. This tool enables non-translation-dedicated \\nlanguage models to understand and answer questions in different languages.\\n• Calculator: Language models can utilize a simple calculator tool to solve math problems. \\nThe calculator supports basic arithmetic operations, allowing the model to accurately \\nsolve mathematical queries in datasets specifically designed for math problem-solving.\\n• Maps: By connecting with the Bing Map API or similar services, language models can \\nretrieve location information, assist with route planning, provide driving distance cal -\\nculations, and offer details about nearby points of interest.\\n• Weather: Weather APIs provide language models with real-time weather information \\nfor cities worldwide. Models can answer queries about current weather conditions or \\nforecast the weather for specific locations within varying time periods.\\n• Stocks: Connecting with stock market APIs like Alpha Vantage allows language models \\nto query specific stock market information such as opening and closing prices, highest \\nand lowest prices, and more.\\n• Slides: Language models equipped with slide-making tools can create slides using \\nhigh-level semantics provided by APIs such as the python-pptx library or image retrieval \\nfrom the internet based on given topics. These tools facilitate tasks related to slide creation \\nthat are required in various professional fields.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 78}, page_content='LangChain for LLM Apps56\\n• Table processing: APIs built with pandas DataFrames enable language models to perform \\ndata analysis and visualization tasks on tables. By connecting to these tools, models can \\nprovide users with a more streamlined and natural experience for handling tabular data.\\n• Knowledge graphs: Language models can query knowledge graphs using APIs that mimic \\nhuman querying processes, such as finding candidate entities or relations, sending SPARQL \\nqueries, and retrieving results. These tools assist in answering questions based on factual \\nknowledge stored in knowledge graphs.\\n• Search engine: By utilizing search engine APIs like Bing Search, language models can \\ninteract with search engines to extract information and provide answers to real-time \\nqueries. These tools enhance the model’s ability to gather information from the web and \\ndeliver accurate responses.\\n• Wikipedia: Language models equipped with Wikipedia search tools can search for specific \\nentities on Wikipedia pages, look up keywords within a page, or disambiguate entities \\nwith similar names. These tools facilitate question-answering tasks using content re -\\ntrieved from Wikipedia.\\n• Online shopping: Connecting language models with online shopping tools allows them \\nto perform actions like searching for items, loading detailed information about products, \\nselecting item features, going through shopping pages, and making purchase decisions \\nbased on specific user instructions.\\nAdditional tools include AI Painting, which allows language models to generate images using AI \\nimage generation models; 3D Model Construction, enabling language models to create 3D models \\nusing a sophisticated 3D rendering engine; Chemical Properties, assisting in resolving scientific \\ninquiries about chemical properties using APIs like PubChem; and database tools that facilitate \\nnatural language access to database data for executing SQL queries and retrieving results.\\nThese various tools provide language models with additional functionalities and capabilities to \\nperform tasks beyond text processing. By connecting with these tools via APIs, language models \\ncan enhance their abilities in areas such as translation, math problem-solving, location-based \\nqueries, weather forecasting, stock market analysis, slide creation, table processing and analysis, \\nimage generation, text-to-speech conversion, and many more specialized tasks.\\nAll these tools can give us advanced AI functionality, and there’s virtually no limit to the tools \\navailable. We can easily build custom tools to extend the capability of LLMs, as we’ll see in the \\nnext chapter. The use of different tools expands the scope of applications for language models \\nand enables them to handle various real-world tasks more efficiently and effectively.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 79}, page_content='Chapter 2 57\\nAfter discussing chains, agents, memory, and tools, let’s put this all together to get a picture of \\nhow LangChain fits all of them together as moving parts.\\nHow does LangChain work?\\nThe LangChain framework simplifies building sophisticated LLM applications by providing mod-\\nular components that facilitate connecting language models with other data and services. The \\nframework organizes capabilities into modules spanning from basic LLM interaction to complex \\nreasoning and persistence.\\nThese components can be combined into pipelines also called chains that sequence the following \\nactions:\\n• Loading documents\\n• Embedding for retrieval\\n• Querying LLMs\\n• Parsing outputs\\n• Writing memory\\nChains match modules to application goals, while agents leverage chains for goal-directed in -\\nteractions with users. They repeatedly execute actions based on observations, plan optimal logic \\nchains, and persist memory across conversations.\\nThe modules, ranging from simple to advanced, are:\\n• LLMs and chat models : Provide interfaces to connect and query language models like \\nGPT-3. Support async, streaming, and batch requests.\\n• Document loaders: Ingest data from sources into documents with text and metadata. \\nEnable loading files, webpages, videos, etc.\\n• Document transformers: Manipulate documents via splitting, combining, filtering, trans-\\nlating, etc. Help adapt data for models.\\n• Text embeddings: Create vector representations of text for semantic search. Different \\nmethods for embedding documents vs. queries.\\n• Vector stores: Store embedded document vectors for efficient similarity search and re -\\ntrieval.\\n• Retrievers: General interface to return documents based on a query. Can leverage vector \\nstores.\\n• Tools: Interfaces that agents use to interact with external systems.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 80}, page_content='LangChain for LLM Apps58\\n• Agents: Goal-driven systems that use LLMs to plan actions based on environment ob -\\nservations.\\n• Toolkits: Initialize groups of tools that share resources like databases.\\n• Memory: Persist information across conversations and workflows by reading/writing \\nsession data.\\n• Callbacks: Hook into pipeline stages for logging, monitoring, streaming, and others. Call-\\nbacks enable monitoring chains.\\nTogether, the preceding capabilities facilitate the building of robust, efficient, and capable LLM \\napplications with LangChain. Each of them has its own complexity and importance, so it’s im -\\nportant to explain a bit more.\\nLangChain offers interfaces to connect with and query LLMs like GPT-3 and chat models. These \\ninterfaces support asynchronous requests, streaming responses, and batch queries. This provides \\na flexible API for integrating different language models.\\nAlthough LangChain doesn’t supply models itself, it supports integration through LLM wrappers \\nwith various language model providers, enabling the app to interact with chat models as well as \\ntext embedding model providers. Supported providers include OpenAI, HuggingFace, Azure, and \\nAnthropic. Providing a standardized interface means being able to effortlessly swap out models to \\nsave money and energy or get better performance. We’ll go into some of these options in Chapter \\n3, Getting Started with LangChain.\\nA core building block of LangChain is the prompt class, which allows users to interact with LLMs \\nby providing concise instructions or examples. Prompt engineering helps optimize prompts \\nfor optimal model performance. Templates give flexibility in terms of input and the available \\ncollection of prompts is battle-tested in a range of applications. We’ll discuss prompts starting \\nin Chapter 3, Getting Started with LangChain , and prompt engineering is the topic of Chapter 8, \\nCustomizing LLMs and Their Output.\\nDocument loaders allow ingesting data from various sources into documents containing text and \\nmetadata. This data can then be manipulated via document transformers – splitting, combining, \\nfiltering, translating, etc. These tools adapt external data for use in LLMs.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 81}, page_content='Chapter 2 59\\nData loaders include modules for storing data and utilities for interacting with external systems, \\nlike web searches or databases, and most importantly data retrieval. Examples are Microsoft Word \\ndocuments (.docx), HyperText Markup Language (HTML), and other common formats such as \\nPDF, text files, JSON, and CSV. Other tools will send emails to prospective customers, post funny \\npuns for your followers, or send Slack messages to your coworkers. We’ll look at these in Chapter \\n5, Building a Chatbot like ChatGPT.\\nText embedding models create vector representations of text that capture semantic meaning. This \\nenables semantic search by finding text with the most similar vector representations. Vector stores \\nbuild on this by indexing embedded document vectors for efficient similarity-based retrieval.\\nVector stores come in when working with large documents, where the document needs to be \\nchunked up in order to be passed to the LLM. These parts of the document would be stored as \\nembeddings, which means that they are vector representations of the information. All these \\ntools enhance the LLMs’ knowledge and improve their performance in applications like question \\nanswering and summarization.\\nThere are numerous integrations for vector storage. These include Alibaba Cloud OpenSearch, \\nAnalyticDB for PostgreSQL, Meta AI’s Annoy library for Approximate Nearest Neighbor (ANN) \\nsearch, Cassandra, Chroma, Elasticsearch, Facebook AI Similarity Search ( Faiss), MongoDB \\nAtlas Vector Search, PGVector as a vector similarity search for Postgres, Pinecone, scikit-learn \\n(SKLearnVectorStore for k-nearest neighbor search), and many more. We’ll explore these in \\nChapter 5, Building a Chatbot like ChatGPT.\\nThere are a few other frameworks besides LangChain; however, we’ll see that LangChain is one \\nof the most prominent and feature rich of them.\\nWhile the next chapters will dig into the details of some usage patterns and use cases \\nof LangChain components, the following resources provide invaluable information \\non LangChain’s components and how they can be assembled into pipelines.\\nFor full details on the dozens of available modules, refer to the comprehensive \\nLangChain API reference: https://api.python.langchain.com/. There are also \\nhundreds of code examples demonstrating real-world use cases: https://python.\\nlangchain.com/docs/use_cases/.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 82}, page_content='LangChain for LLM Apps60\\nComparing LangChain with other frameworks\\nLLM application frameworks have been developed to provide specialized tooling that can har-\\nness the power of LLMs effectively to solve complex problems. A few libraries have emerged that \\nmeet the requirements of effectively combining generative AI models with other tools to build \\nLLM applications.\\nThere are several open-source frameworks for building dynamic LLM applications. They all offer \\nvalue in developing cutting-edge LLM applications. This graph shows their popularity over time \\n(data source: GitHub star history, https://star-history.com/):\\nFigure 2.11: Comparison of popularity between different frameworks in Python\\nWe can see the number of stars on GitHub over time for each project. Haystack is the oldest of \\nthe compared frameworks, having started in early 2020 (as per the earliest GitHub commits). It \\nis also the least popular in terms of stars on GitHub. LangChain, LlamaIndex (previously called \\nGPTIndex), and SuperAGI were started in late 2022 or early 2023, and they have all fallen short \\nin popularity in a noticeably brief time compared to LangChain, which has been growing impres-\\nsively. AutoGen is a project recently released by Microsoft that has already garnered some interest. \\nIn this book, we’ll see a lot of the functionality of LangChain and explore its features, which are \\nthe reason its popularity is exploding right now.\\nLlamaIndex focuses on advanced retrieval rather than on the broader aspects of LLM apps. Similar-\\nly, Haystack focuses on creating large-scale search systems with components designed specifically \\nfor scalable information retrieval using retrievers, readers, and other data handlers combined \\nwith semantic indexing via pre-trained models.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 83}, page_content='Chapter 2 61\\nLangChain excels at chaining LLMs together using agents to delegate actions to models. Its use cas-\\nes emphasize prompt optimization and context-aware information retrieval/generation; however, \\nwith its Pythonic highly modular interface and its huge collection of tools, it is the number-one \\ntool to implement complex business logic.\\nSuperAGI has similar features to LangChain. It even comes with a marketplace, a repository for \\ntools and agents. However, it’s not as extensive and well supported as LangChain.\\nAutoGen simplifies the building, orchestrating, and optimizing of complex workflows powered \\nby LLMs. Its key innovation is enabling customizable conversational agents that automate coor-\\ndination between different LLMs, humans, and tools via automated chat. AutoGen streamlines \\nagent definition and interaction to automatically compose optimal LLM-based workflows.\\nI haven’t included AutoGPT (and similar tools like AutoLlama), a recursive application that breaks \\ndown tasks, because its reasoning capability, based on human and LLM feedback, is very limited \\ncompared to LangChain. As a consequence, it’s often caught in logic loops and regularly repeats \\nsteps. I’ve also omitted a few libraries that concentrate on prompt engineering, for example, \\nPromptify.\\nThere are other LLM app frameworks in languages such as Rust, JavaScript, Ruby, and Java. For \\nexample, Dust, written in Rust, focuses on the design of LLM apps and their deployment.\\nFrameworks like LangChain aim to lower barriers by providing guardrails, conventions, and \\npre-built modules, but foundational knowledge remains important for avoiding pitfalls and \\nmaximizing value from LLMs. Investing in education pays dividends when delivering capable, \\nresponsible applications.\\nSummary\\nLLMs produce convincing language but have significant limitations in terms of reasoning, knowl-\\nedge, and access to tools. The LangChain framework simplifies the building of sophisticated \\napplications powered by LLMs that can mitigate these shortcomings. It provides developers with \\nmodular, reusable building blocks like chains for composing pipelines and agents for goal-oriented \\ninteractions. These building blocks fit together as LLM apps that come with extended capabilities.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 84}, page_content='LangChain for LLM Apps62\\nAs we saw in this chapter, chains allow sequencing calls to LLMs, databases, APIs, and more to \\naccomplish multi-step workflows. Agents leverage chains to take actions based on observations \\nfor managing dynamic applications. Memory persists information across executions to maintain \\nstate. Together, these concepts enable developers to overcome the limitations of individual LLMs \\nby integrating external data, actions, and context. In other words, LangChain reduces complex \\norchestration into customizable building blocks.\\nIn the next chapters, we’ll build on these LangChain fundamentals to create capable, real-world \\napplications. We’ll implement conversational agents combining LLMs with knowledge bases and \\nadvanced reasoning algorithms. By leveraging LangChain’s capabilities, developers can unlock \\nthe full potential of LLMs to power the next generation of AI software. In the next chapter, we’ll \\nimplement our first apps with Langchain!\\nQuestions\\nPlease see if you can come up with answers to these questions. I’d recommend you go back to the \\ncorresponding sections of this chapter if you are unsure about any of them:\\n1. What are the limitations of LLMs?\\n2. What are stochastic parrots?\\n3. What are LLM applications?\\n4. What is LangChain and why should you use it?\\n5. What are LangChain’s key features?\\n6. What is a chain in LangChain?\\n7. What is an agent?\\n8. What is memory and why do we need it?\\n9. What kind of tools are available in LangChain?\\n10. How does LangChain work?'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 85}, page_content='Chapter 2 63\\nJoin our community on Discord\\nJoin our community’s Discord space for discussions with the authors and other readers:\\nhttps://packt.link/lang'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 86}, page_content=''),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 87}, page_content='3\\nGetting Started with LangChain\\nIn this book, we’ll write a lot of code and test many different integrations and tools. Therefore, in \\nthis chapter, we’ll give basic setup instructions for all the libraries needed with the most common \\ndependency management tools such as Docker, Conda, pip, and Poetry. This will ensure that you \\ncan run all the practical examples in this book.\\nNext, we’ll go through model integrations that we can use such as OpenAI’s ChatGPT, models on \\nHugging Face, Jina AI, and others. Further, we’ll introduce, set up, and work with a few providers \\nin turn. For each of them, we will show how to get an API key token.\\nIn the end, as a practical example, we’ll go through an example of a real-world application, an \\nLLM app that could help customer service agents, one of the main areas where LLMs could prove \\nto be game-changing. This will give us a bit more context around using LangChain, and we can \\nintroduce tips and tricks for using it effectively.\\nThe main sections are as follows:\\n• How to set up the dependencies for this book\\n• Model integrations\\n• Building an application for customer service\\nWe’ll start the chapter by setting up the environment for the book on our computer.\\nHow to set up the dependencies for this book\\nWe’ll assume at least a basic familiarity with Python, Jupyter, and environments in this book, but \\nlet’s quickly walk through this together. You can safely skip this section if you are confident about \\nyour setup or if you plan to install libraries separately for each chapter or application.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 88}, page_content='Getting Started with LangChain66\\nPlease make sure you have Python version 3.10 or higher installed. You can install it from python.\\norg or your platform’s package manager. If you use Docker, Conda, or Poetry, an appropriate Py-\\nthon version should be installed automatically as part of the instructions. You should also install \\nJupyter Notebook or JupyterLab to run the example notebooks interactively.\\nEnvironment management tools like Docker, Conda, Pip, and Poetry help create reproducible \\nPython environments for projects. They install dependencies and isolate projects. This table gives \\nan overview of these options for managing dependencies:\\nTool Pros Cons\\npip\\nDefault Python package manager\\nSimple commands to install packages\\nrequirements.txt for tracking dependencies\\nCan’t install non-Python system \\ndependencies\\nNo built-in virtual environment \\nmanagement (see venv or other \\ntools)\\nLimited dependency resolution\\nPoetry\\nIntuitive interface\\nRobust dependency resolution\\nBuilt-in virtual environment management\\nLock files and version control\\nLess common than Pip or Conda\\nLimited non-Python \\ndependency management\\nConda\\nManages Python and non-Python dependencies\\nHandles complex dependency trees\\nSupports multiple Python versions\\nBuilt-in virtual environment management\\nSlower than native package \\nmanagers\\nLarge disk usage\\nDocker\\nProvides fully isolated and reproducible \\nenvironments\\nEasily shared and distributed\\nGuaranteed consistency across systems\\nAdditional platform knowledge \\nrequired\\nLarger disk usage\\nSlower startup times\\nTable 3.1: Comparison of tools for managing dependencies\\nFor developers, Docker, which provides isolation via containers, is a good option. The downside \\nis that it uses a lot of disk space and is more complex than the other options. For data scientists, \\nI’d recommend Conda or Poetry. '),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 89}, page_content='Chapter 3 67\\nConda handles intricate dependencies efficiently, although it can be excruciatingly slow in large \\nenvironments. Poetry resolves dependencies well and managed environments; however, it doesn’t \\ncapture system dependencies.\\nAll tools allow sharing and replicating dependencies from configuration files. You can find a set \\nof instructions and the corresponding configuration files in the book’s repository at https://\\ngithub.com/benman1/generative_ai_with_langchain.\\nThis includes these files:\\n• requirements.txt for pip\\n• pyproject.toml for Poetry\\n• langchain_ai.yaml for Conda\\n• Dockerfile for Docker\\nDepending on whether system dependencies are managed, they can require additional tweaks \\nwith more setup, as in the case with pip and poetry. My preference is Conda because it strikes \\nthe right balance for me of complexity versus isolation.\\nAs mentioned, we won’t spend much time on installation but rather breeze through each of \\nthe different tools in turn. For all instructions, please make sure you have the book’s repository \\ndownloaded (using the GitHub user interface) or cloned on your computer, and you’ve changed \\ninto the project’s root directory.\\nIf you encounter issues during the installation process, consult the respective documentation or \\nraise an issue on the GitHub repository of this book. The different installations have been tested \\nat the time of the release of this book; however, things can change, and we will update the GitHub \\nREADME online to include workarounds for potential problems that could arise.\\nFor each tool, the key steps are installing the tool, using the configuration file from the repository, \\nand activating the environment. This sets up a reproducible environment to run all the examples \\nin the book (with very few exceptions, which will be noted).\\nLet’s go from the simplest to the most complex. We’ll start with pip!\\npip\\npip is the default Python package manager. To use Pip:\\n1. If it’s not already included in your Python distribution, install pip following the instruc-\\ntions here: https://pip.pypa.io/.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 90}, page_content='Getting Started with LangChain68\\n2. Use a virtual environment for isolation (for example, venv).\\n3. Install the dependencies from requirements.txt:\\npip install -r requirements.txt\\nPoetry\\nPoetry is relatively new, but is popular with Python developers and data scientists because of its \\nconvenience. It manages dependencies and virtual environments. To use Poetry:\\n1. Install poetry by following the instructions at https://python-poetry.org/.\\n2. Run poetry install to install the dependencies.\\nConda\\nConda manages Python environments and dependencies. To use Conda:\\n1. Install Miniconda or Anaconda following the instructions from this link: https://docs.\\ncontinuum.io/anaconda/install/.\\n2. Create the environment from langchain_ai.yml:\\nconda env create --file langchain_ai.yaml\\n3. Activate the environment:\\nconda activate langchain_ai\\nDocker\\nDocker provides isolated, reproducible environments using containers. To use Docker:\\n1. Install Docker Engine; follow the installation instructions here: https://docs.docker.\\ncom/get-docker/.\\n2. Build the Docker image from the Dockerfile in this repository:\\ndocker build -t langchain_ai \\n3. Run the Docker container interactively:\\ndocker run -it langchain_ai \\nLet’s move on and see some of the models that you can use with LangChain!\\nThere are many cloud providers of models, where you can use the model through an interface; \\nother sources allow you to download a model to your computer. '),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 91}, page_content='Chapter 3 69\\nWith the help of LangChain, we can interact with all of these – for example, through Application \\nProgramming Interface (APIs), or we can call models that we have downloaded on our computer. \\nLet’s start with models accessed through APIs with cloud providers.\\nExploring API model integrations\\nBefore properly starting with generative AI, we need to set up access to models such as LLMs or \\ntext-to-image models so we can integrate them into our applications. As discussed in Chapter 1, \\nWhat Is Generative AI?, there are various LLMs by tech giants, like GPT-4 by OpenAI, BERT and \\nPaLM-2 by Google, LLaMA by Meta, and many more.\\nFor LLMs, OpenAI, Hugging Face, Cohere, Anthropic, Azure, Google Cloud Platform’s Vertex AI \\n(PaLM-2), and Jina AI are among the many providers supported in LangChain; however, this \\nlist is growing all the time. You can check out the full list of supported integrations for LLMs at \\nhttps://integrations.langchain.com/llms.\\nHere’s a screenshot of this page as of the time of writing (October 2023), which includes both \\ncloud providers and interfaces for local models:\\nFigure 3.1: LLM integrations in LangChain'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 92}, page_content='Getting Started with LangChain70\\nLangChain implements three different interfaces – we can use chat models, LLMs, and embedding \\nmodels. Chat models and LLMs are similar in that they both process text input and produce text \\noutput. However, there are some differences in the types of input and output they handle. Chat \\nmodels are specifically designed to handle a list of chat messages as input and generate a chat \\nmessage as output. They are commonly used in chatbot applications where conversations are ex-\\nchanged. You can find chat models at https://python.langchain.com/docs/integrations/chat.\\nFinally, text embedding models are used to convert text inputs into numerical representations \\ncalled embeddings. We’ll focus on text generation in this chapter, and discuss embeddings, vector \\ndatabases, and neural search in Chapter 5, Building a Chatbot Like ChatGPT. Suffice it to say here \\nthat these embeddings are a way to capture and extract information from the input text. They \\nare widely used in natural language processing tasks like sentiment analysis, text classification, \\nand information retrieval. Embedding models are listed at https://python.langchain.com/\\ndocs/integrations/text_embedding.\\nAs for image models, the big developers include OpenAI (DALL-E), Midjourney, Inc. (Midjourney), \\nand Stability AI (Stable Diffusion). LangChain currently doesn’t have out-of-the-box handling \\nof models that are not for text; however, its documentation describe how to work with Replicate, \\nwhich also provides an interface to Stable Diffusion models.\\nFor each of these providers, to make calls against their API, you’ll first need to create an account \\nand obtain an API key. This is free of charge for all providers and, with some of them, you don’t \\neven have to give them your credit card details.\\nTo set an API key in an environment, in Python, we can execute the following lines:\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"<your token>\"\\nHere, OPENAI_API_KEY is the environment key that is appropriate for OpenAI. Setting the keys in \\nyour environment has the advantage of not needing to include them as parameters in your code \\nevery time you use a model or service integration.\\nYou can also expose these variables in your system environment from your terminal. In Linux and \\nmacOS, you can set a system environment variable from the terminal using the export command:\\nexport OPENAI_API_KEY=<your token>'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 93}, page_content='Chapter 3 71\\nTo permanently set the environment variable in Linux or macOS, you would need to add the \\npreceding line to the ~/.bashrc or ~/.bash_profile file, respectively, and then reload the shell \\nusing the command source ~/.bashrc or source ~/.bash_profile.\\nIn Windows, you can set a system environment variable from the command prompt using the \\nset command:\\nset OPENAI_API_KEY=<your token>\\nTo permanently set the environment variable in Windows, you can add the preceding line to a \\nbatch script.\\nMy personal choice is to create a config.py file, where all the keys are stored. I then import a \\nfunction from this module that will load all these keys into the environment. If you look for this \\nfile in the Github repository, you’ll notice that it is missing. This is on purpose (in fact, I’ve disabled \\nthe tracking of this file in Git) since I don’t want to share my keys with other people for security \\nreasons (and because I don’t want to pay for anyone else’s usage).\\nMy config.py looks like this:\\nimport os\\nOPENAI_API_KEY = \"... \"\\n# I\\'m omitting all other keys\\ndef set_environment():\\n    variable_dict = globals().items()\\n    for key, value in variable_dict:\\n        if \"API\" in key or \"ID\" in key:\\n            os.environ[key] = value\\nYou can set all your keys in the config.py file. This function, set_environment(), loads all the \\nkeys into the environment as mentioned. Anytime you want to run an application, you import \\nthe function and run it like so:\\nfrom config import set_environment\\nset_environment()'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 94}, page_content='Getting Started with LangChain72\\nNow, let’s go through a few prominent model providers in turn. We’ll give an example of usage \\nfor each of them. Let’s start with a fake LLM that we can use for testing purposes. This will help \\nto illustrate the general idea of calling language models in LangChain.\\nFake LLM\\nThe fake LLM allows you to simulate LLM responses during testing without needing actual API \\ncalls. This is useful for rapid prototyping and unit testing agents. Using the FakeLLM avoids hit-\\nting rate limits during testing. It also allows you to mock various responses to validate that your \\nagent handles them properly. Overall, it enables fast agent iteration without needing a real LLM.\\nFor example, you could initialize a FakeLLM that returns \"Hello\" as follows:\\nfrom langchain.llms import FakeLLM\\nfake_llm = FakeLLM(responses=[\"Hello\"])\\nYou can execute this example in either Python directly or in a notebook.\\nThe fake LLM is only for testing purposes. The LangChain documentation has an example of \\ntool use with LLMs. This is a bit more complex than the previous example but gives a hint of the \\ncapabilities we have at our fingertips:\\nfrom langchain.llms.fake import FakeListLLM\\nfrom langchain.agents import load_tools\\nfrom langchain.agents import initialize_agent\\nfrom langchain.agents import AgentType\\ntools = load_tools([\"python_repl\"])\\nresponses = [\"Action: Python_REPL\\\\nAction Input: print(2 + 2)\", \"Final \\nAnswer: 4\"]\\nllm = FakeListLLM(responses=responses)\\nagent = initialize_agent(\\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\\n)\\nagent.run(\"whats 2 + 2\")\\nWe set up an agent that makes decisions based on the React strategy that we explained in Chapter \\n2, LangChain for LLM Apps (ZERO_SHOT_REACT_DESCRIPTION). We run the agent with a text: the \\nquestion what\\'s 2 + 2.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 95}, page_content='Chapter 3 73\\nAs you can see, we connect a tool, a Python Read-Eval-Print Loop (REPL), that will be called de-\\npending on the output of the LLM. FakeListLLM will give two responses (\"Action: Python_REPL\\\\\\nnAction Input: print(2 + 2)\" and \"Final Answer: 4\") that won’t change based on the input.\\nWe can also observe how the fake LLM output leads to a call to the Python interpreter, which \\nreturns 4. Please note that the action must match the name attribute of the tool, PythonREPLTool, \\nwhich starts like this:\\nclass PythonREPLTool(BaseTool):\\n    \"\"\"A tool for running python code in a REPL.\"\"\"\\n    name = \"Python_REPL\"\\n    description = (\\n        \"A Python shell. Use this to execute python commands. \"\\n        \"Input should be a valid python command. \"\\n        \"If you want to see the output of a value, you should print it out \\n\"\\n        \"with `print(...)`.\"\\n    )\\nAs you can see in the preceding code block, the names and descriptions of the tools are passed \\nto the LLM, which then decides an action based on the information provided. The action can be \\nexecuting a tool or planning.\\nThe output of the Python interpreter is passed to the fake LLM, which ignores the observation \\nand returns 4. Obviously, if we change the second response to \"Final Answer: 5\", the output \\nof the agent wouldn’t correspond to the question.\\nIn the next sections, we’ll make our example more meaningful by using an actual LLM rather \\nthan a fake one. One of the first providers anyone will think of is OpenAI.\\nOpenAI\\nAs explained in Chapter 1, What Is Generative AI?, OpenAI is an American AI research laboratory \\nthat is the current market leader in generative AI models, especially LLMs. They offer a range of \\nmodels with various levels of power suitable for different tasks. We’ll see, in this chapter, how to \\ninteract with OpenAI models with the LangChain and the OpenAI Python client libraries. OpenAI \\nalso offers an Embedding class for text embedding models.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 96}, page_content='Getting Started with LangChain74\\nWe will use OpenAI for our applications but will also try LLMs from other organizations. When you \\nsend a prompt to an LLM API, it processes the prompt word by word, breaking down (tokenizing) \\nthe text into individual tokens. The number of tokens directly correlates with the amount of text.\\nWhen using commercial LLMs like GPT-3 and GPT-4 via APIs, each token has an associated cost \\nbased on factors like the LLM model and API pricing tiers. Token usage refers to how many to -\\nkens from the model’s quota have been consumed to generate a response. Strategies like using \\nsmaller models, summarizing outputs, and preprocessing inputs help reduce the tokens required \\nto get useful results. Being aware of token usage is key for optimizing productivity within budget \\nconstraints when leveraging commercial LLMs.\\nWe need to obtain an OpenAI API key first. To create an API key, follow these steps:\\n1. You need to create a login at https://platform.openai.com/.\\n2. Set up your billing information.\\n3. You can see the API keys under Personal | View API Keys.\\n4. Click on Create new secret key and give it a name.\\nHere’s how this should look on the OpenAI platform:\\nFigure 3.2: OpenAI API platform – Create new secret key\\nAfter clicking Create secret key, you should see the message API key generated. You need to copy \\nthe key to your clipboard and keep it. We can set the key as an environment variable (OPENAI_API_\\nKEY) or pass it as a parameter every time you construct a class for OpenAI calls.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 97}, page_content='Chapter 3 75\\nWe can use the OpenAI language model class to set up an LLM to interact with. Let’s create an \\nagent that calculates using this model – I am omitting the imports from the previous example:\\nfrom langchain.llms import OpenAI\\nllm = OpenAI(temperature=0., model=\"text-davinci-003\")\\nagent = initialize_agent(\\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\\n)\\nagent.run(\"whats 4 + 4\")\\nWe should be seeing this output:\\n> Entering new  chain...\\n I need to add two numbers\\nAction: Python_REPL\\nAction Input: print(4 + 4)\\nObservation: 8\\nThought: I now know the final answer\\nFinal Answer: 4 + 4 = 8\\n> Finished chain.\\n\\'4 + 4 = 8\\'\\nThe agent comes up with the right solution. It’s a simple problem, but I still find it fascinating to \\nbe able to put my question in natural language. During the course of this book, we’ll try to come \\nup with solutions to more complex problems. But for now, let’s move on to the next provider \\nand more examples!\\nHugging Face\\nHugging Face is a very prominent player in the NLP space and has considerable traction in open-\\nsource and hosting solutions. The company is an American company that develops tools for build-\\ning machine learning applications. Its employees develop and maintain the Transformers Python \\nlibrary, which is used for NLP tasks, includes implementations of state-of-the-art and popular \\nmodels like Mistral 7B, BERT, and GPT-2, and is compatible with PyTorch, TensorFlow, and JAX.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 98}, page_content='Getting Started with LangChain76\\nHugging Face also provides the Hugging Face Hub, a platform for hosting Git-based code re -\\npositories, machine learning models, datasets, and web applications, which provides over 120k \\nmodels, 20k datasets, and 50k demo apps (spaces) for machine learning. It is an online platform \\nwhere people can collaborate and facilitate machine learning development.\\nThese tools allow users to load and use models, embeddings, and datasets from Hugging Face. \\nThe HuggingFaceHub integration, for example, provides access to different models for tasks like \\ntext generation and text classification. The HuggingFaceEmbeddings integration allows users to \\nwork with sentence-transformer models.\\nHugging Face offer various other libraries within their ecosystem, including Datasets for dataset \\nprocessing, Evaluate for model evaluation, Simulate for simulation, and Gradio for machine \\nlearning demos.\\nIn addition to their products, Hugging Face has been involved in initiatives such as the BigScience \\nResearch Workshop, where they released an open LLM called BLOOM with 176 billion parame -\\nters. They have received significant funding, including a $40 million Series B round and a recent \\nSeries C funding round led by Coatue and Sequoia at a $2 billion valuation. Hugging Face has \\nalso formed partnerships with companies like Graphcore and Amazon Web Services to optimize \\ntheir offerings and make them available to a broader customer base.\\nTo use Hugging Face as a provider for your models, you can create an account and API keys at \\nhttps://huggingface.co/settings/profile. Additionally, you can make the token available \\nin your environment as HUGGINGFACEHUB_API_TOKEN.\\nLet’s see an example, where we use an open-source model developed by Google, the Flan-T5-XXL \\nmodel:\\nfrom langchain.llms import HuggingFaceHub\\nllm = HuggingFaceHub(\\n    model_kwargs={\"temperature\": 0.5, \"max_length\": 64},\\n    repo_id=\"google/flan-t5-xxl\"\\n)\\nprompt = \"In which country is Tokyo?\"\\ncompletion = llm(prompt)\\nprint(completion)\\nWe get the response \"japan\".'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 99}, page_content='Chapter 3 77\\nThe LLM takes a text input, a question in this case, and returns a completion. The model has a \\nlot of knowledge and can come up with answers to knowledge questions.\\nGoogle Cloud Platform\\nThere are many models and functions available through Google Cloud Platform (GCP) and Vertex \\nAI, GCP’s machine learning platform. GCP provides access to LLMs like LaMDA, T5, and PaLM. \\nGoogle has also updated the Google Cloud Natural Language ( NL) API with a new LLM-based \\nmodel for Content Classification. This updated version offers an expansive pre-trained classifi-\\ncation taxonomy to help with ad targeting and content-based filtering. The NL API’s improved v2 \\nclassification model is enhanced with over 1,000 labels and supports 11 languages with improved \\naccuracy (it is unclear, however, which model is used under the hood).\\nFor models with GCP, you need to have the gcloud command-line interface (CLI) installed. You \\ncan find the instructions here: https://cloud.google.com/sdk/docs/install.\\nYou can then authenticate and print a key token with this command from the terminal:\\ngcloud auth application-default login\\nYou also need to enable Vertex AI for your project. To enable Vertex AI, install the Google Vertex \\nAI SDK with the pip install google-cloud-aiplatform command. If you’ve followed the in-\\nstructions on GitHub as indicated in the previous section, you should already have this installed.\\nThen we have to set up the Google Cloud project ID. You have different options for this:\\n• Using gcloud config set project my-project \\n• Passing a constructor argument when initializing the LLM\\n• Using aiplatform.init()\\n• Setting a GCP environment variable\\nI found all these options work fine. You can find more details about these options in the Vertex \\ndocumentation. The GCP environment variable works well with the config.py file that I men-\\ntioned earlier. I found the gcloud command very convenient though, so I went with this. Please \\nmake sure you set the project ID before you move on.\\nIf you haven’t enabled it, you should get a helpful error message pointing you to the right website, \\nwhere you click Enable.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 100}, page_content='Getting Started with LangChain78\\nLet’s run a model!\\nfrom langchain.llms import VertexAI\\nfrom langchain import PromptTemplate, LLMChain\\ntemplate = \"\"\"Question: {question}\\nAnswer: Let\\'s think step by step.\"\"\"\\nprompt = PromptTemplate(template=template, input_variables=[\"question\"])\\nllm = VertexAI()\\nllm_chain = LLMChain(prompt=prompt, llm=llm, verbose=True)\\nquestion = \"What NFL team won the Super Bowl in the year Justin Beiber was \\nborn?\"\\nllm_chain.run(question)\\nWe should see this response:\\n[1m> Entering new chain...[0m\\nPrompt after formatting:\\n[[Question: What NFL team won the Super Bowl in the year Justin Beiber was \\nborn?\\nAnswer: Let\\'s think step by step.[0m\\n[1m> Finished chain.[0m\\nJustin Beiber was born on March 1, 1994. The Super Bowl in 1994 was won by \\nthe San Francisco 49ers.\\nI’ve set verbose to True to see the model’s reasoning process. It’s quite impressive that it produces \\nthe right response even given a misspelling of the name. The step-by-step prompt instruction is \\nkey to the correct answer.\\nVertex AI offers a range of models tailored for tasks like following instructions, conversation, and \\ncode generation/assistance:\\n• text-bison is fine-tuned to follow natural language instructions, with a max input of 8,192 \\ntokens and an output of 1,024.\\n• chat-bison is optimized for multi-turn conversation with a max input of 4,096 tokens, \\nan output of 1,024 tokens, and up to 2,500 turns.\\n• code-bison generates code from natural language descriptions, with a max input of 4,096 \\ntokens and an output of 2,048 tokens.\\n• codechat-bison is a chatbot that is fine-tuned to help with code-related questions. It has \\nan input limit of 4,096 tokens and an output limit of 2,048 tokens.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 101}, page_content='Chapter 3 79\\n• code-gecko suggests code completions. It has a max input length of 2,048 tokens and \\nan output of 64 tokens.\\nThese models also have different input/output limits and training data and are often updated. \\nFor more detailed and up-to-date information about models, including when models have been \\nupdated, you can check out the documentation at https://cloud.google.com/vertex-ai/docs/\\ngenerative-ai/learn/overview.\\nWe can also generate code. Let’s see if the  code-bison model can solve FizzBuzz , a common in-\\nterview question for entry-level software developer positions:\\nquestion = \"\"\"\\nGiven an integer n, return a string array answer (1-indexed) where:\\nanswer[i] == \"FizzBuzz\" if i is divisible by 3 and 5.\\nanswer[i] == \"Fizz\" if i is divisible by 3.\\nanswer[i] == \"Buzz\" if i is divisible by 5.\\nanswer[i] == i (as a string) if none of the above conditions are true.\\n\"\"\"\\nllm = VertexAI(model_name=\"code-bison\")\\nllm_chain = LLMChain(prompt=prompt, llm=llm)\\nprint(llm_chain.run(question))\\nWe are getting this response:\\n```python\\nanswer = []\\nfor i in range(1, n + 1):\\n    if i % 3 == 0 and i % 5 == 0:\\n        answer.append(\"FizzBuzz\")\\n    elif i % 3 == 0:\\n        answer.append(\"Fizz\")\\n    elif i % 5 == 0:\\n        answer.append(\"Buzz\")\\n    else:\\n        answer.append(str(i))\\nreturn answer\\n```'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 102}, page_content='Getting Started with LangChain80\\nWould you hire code-bison for your team?\\nJina AI\\nJina AI, founded in February 2020 by Han Xiao and Xuanbin He, is a German AI company based \\nin Berlin that specializes in providing cloud-native neural search solutions with models for text, \\nimage, audio, and video. Their open-source neural search ecosystem enables businesses and de-\\nvelopers to easily build scalable and highly available neural search solutions, allowing for efficient \\ninformation retrieval. Recently, Jina AI launched Finetuner, a tool that enables the fine-tuning \\nof any deep neural network to specific use cases and requirements.\\nThe company raised $37.5 million in funding through three rounds, with their most recent fund-\\ning coming from a Series A round in November 2021. Notable investors in Jina AI include GGV \\nCapital and Canaan Partners.\\nYou can set up a login at https://chat.jina.ai/api.\\nOn the platform, we can set up APIs for different use cases such as image caption, text embedding, \\nimage embedding, visual question answering, visual reasoning, image upscale, or Chinese text \\nembedding.\\nHere, we are setting up a Visual Question Answering API with the recommended model:\\nFigure 3.3: Visual Question Answering API in Jina AI'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 103}, page_content='Chapter 3 81\\nWe get examples for client calls in Python and cURL, and a demo, where we can ask a question. \\nThis is cool, but unfortunately, these APIs are not available yet through LangChain. We can im -\\nplement such calls ourselves by subclassing the LLM class in LangChain as a custom LLM interface.\\nLet’s set up another chatbot, this time powered by Jina AI. We can generate the API token, which \\nwe can set as JINACHAT_API_KEY, at https://chat.jina.ai/api.\\nLet’s translate from English to French here:\\nfrom langchain.chat_models import JinaChat\\nfrom langchain.schema import HumanMessage\\nchat = JinaChat(temperature=0.)\\nmessages = [\\n    HumanMessage(\\n        content=\"Translate this sentence from English to French: I love \\ngenerative AI!\"\\n    )\\n]\\nchat(messages)\\nWe should be seeing :\\nAIMessage(content=\"J\\'adore l\\'IA générative !\", additional_kwargs={}, \\nexample=False).\\nWe can set different temperatures, where a low temperature makes the responses more predict-\\nable. In this case, it makes only a minor difference. We are starting the conversation with a system \\nmessage clarifying the purpose of the chatbot.\\nLet’s ask for some food recommendations:\\nfrom langchain.schema import SystemMessage\\nchat = JinaChat(temperature=0.)\\nchat(\\n    [\\n        SystemMessage(\\n            content=\"You help a user find a nutritious and tasty food to \\neat in one word.\"\\n        ),'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 104}, page_content='Getting Started with LangChain82\\n        HumanMessage(\\n            content=\"I like pasta with cheese, but I need to eat more \\nvegetables, what should I eat?\"\\n        )\\n    ]\\n)\\nI get this response in Jupyter – your answer could vary:\\nAIMessage(content=\\'A tasty and nutritious option could be a vegetable \\npasta dish. Depending on your taste, you can choose a sauce that \\ncomplements the vegetables. Try adding broccoli, spinach, bell peppers, \\nand zucchini to your pasta with some grated parmesan cheese on top. This \\nway, you get to enjoy your pasta with cheese while incorporating some \\nveggies into your meal.\\', additional_kwargs={}, example=False)\\nIt ignored the one-word instruction, but I liked reading the ideas. I think I should try this for my \\nson. With other chatbots, I got Ratatouille as a suggestion.\\nIt’s important to understand the difference in LangChain between LLMs and chat models. LLMs \\nare text completion models that take a string prompt as input and output a string completion. As \\nmentioned, chat models are like LLMs but are specifically designed for conversations. They take \\na list of chat messages as input, labeled with the speaker, and return a chat message as output.\\nBoth LLMs and chat models implement the base language model interface, which includes meth-\\nods such as predict() and predict_messages(). This shared interface allows for interchange -\\nability between diverse types of models in applications and between chat and LLM models.\\nReplicate\\nEstablished in 2019, Replicate Inc. is a San Francisco-based start-up that presents a streamlined \\nprocess to AI developers, where they can implement and publish AI models with minimal code \\ninput through the utilization of cloud technology. The platform works with private as well as \\npublic models and enables model inference and fine-tuning. The firm, deriving its most recent \\nfunding from a Series A funding round of which the invested total was $12.5 million, was spear-\\nheaded by Andreessen Horowitz, and involved the participation of Y Combinator, Sequoia, and \\nvarious independent investors.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 105}, page_content='Chapter 3 83\\nBen Firshman, who drove open-source product efforts at Docker, and Andreas Jansson, a former \\nmachine learning engineer at Spotify, co-founded Replicate Inc. with the mutual aspiration to \\neliminate the technical barriers that were hindering the mass acceptance of AI. Consequently, they \\ncreated Cog, an open-source tool that packs machine learning models into a standard produc -\\ntion-ready container that can run on any current operating system and automatically generate an \\nAPI. These containers can also be deployed on clusters of GPUs through the Replicate platform. As \\na result, developers can concentrate on other essential tasks, thereby enhancing their productivity.\\nYou can authenticate with your GitHub credentials at https://replicate.com/. If you then click \\non your user icon at the top left, you’ll find the API tokens – just copy the API key and make it \\navailable in your environment as REPLICATE_API_TOKEN. To run bigger jobs, you need to set up \\nyour credit card (under billing).\\nHere is a simple example for creating an image:\\nfrom langchain.llms import Replicate\\ntext2image = Replicate(\\n    model=\"stability-ai/stable-diffusion:db21e45d3f7023abc2a46ee38a23973f6\\ndce16bb082a930b0c49861f96d1e5bf\",\\n    input={\"image_dimensions\": \"512x512\"},\\n)\\nimage_url = text2image(\"a book cover for a book about creating generative \\nai applications in Python\")\\nReplicate has lots of models available on their platform: https://replicate.com/\\nexplore.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 106}, page_content='Getting Started with LangChain84\\nI got this image:\\nFigure 3.4: A book cover for a book about generative AI with Python – Stable Diffusion\\nI think it’s a nice image – is that an AI chip that creates art?\\nOthers\\nThere are a lot more providers, and we’ll encounter quite a few throughout the book. Sadly, as \\nyou’ll see, I faced issues with Azure and Anthropic, two major providers. Let’s still have a quick \\nlook at them!\\nAzure\\nAzure, the cloud computing platform run by Microsoft, integrates with OpenAI to provide powerful \\nlanguage models like GPT-3, Codex, and Embeddings. It offers access, management, and devel-\\nopment of applications and services through its global data centers for use cases such as writing \\nassistance, summarization, code generation, and semantic search. It provides capabilities  like \\nsoftware as a service (SaaS), platform as a service (PaaS), and infrastructure as a service (IaaS).\\nBy authenticating either through GitHub or Microsoft credentials, we can create an account on \\nAzure at https://azure.microsoft.com/.\\nWe can then create new API keys under Cognitive Services | Azure OpenAI. There are a few more \\nsteps involved, and personally, I found this process frustrating. After going through account val-\\nidation a few times, getting denied, and trying to contact Microsoft customer service, I gave up. \\nFor this reason, I don’t have a practical example with Azure. Your mileage might vary – if you are \\nalready using Microsoft services, this process could be pain-free for you.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 107}, page_content='Chapter 3 85\\nAfter setting up, the models should be accessible through the AzureOpenAI() class interface in \\nLangChain.\\nAnthropic\\nAnthropic is an AI start-up and public-benefit corporation based in the United States. It was found-\\ned in 2021 by former members of OpenAI, including siblings Daniela Amodei and Dario Amodei. \\nThe company specializes in developing general AI systems and language models with a focus on \\nresponsible AI usage. As of July 2023, Anthropic has raised $1.5 billion in funding. They have also \\nworked on projects like Claude, an AI chatbot like OpenAI’s ChatGPT, and have researched the \\ninterpretability of machine learning systems, specifically the Transformer architecture.\\nUnfortunately, Claude is not available to the general public (yet). You need to apply for access to \\nuse Claude and set the ANTHROPIC_API_KEY environment variable.\\nNext, let’s see how to run models locally.\\nExploring local models\\nWe can also run local models from LangChain. The advantages of running models locally are \\ncomplete control over the model and not sharing any data over the internet.\\nLet’s preface this with a note of caution: an LLM is big, which means that it’ll take up a lot of disk \\nspace or system memory. The use cases presented in this section should run even on old hardware, \\nlike an old MacBook; however, if you choose a big model, it can take an exceptionally long time \\nto run or may crash the Jupyter notebook. One of the main bottlenecks is memory requirement. \\nIn rough terms, if quantized (roughly, compressed; we’ll discuss quantization in Chapter 8, Cus-\\ntomizing LLMs and Their Output ), 1 billion parameters correspond to 1 GB of RAM (please note \\nthat not all models will come quantized).\\nYou can also run these models on hosted resources or services such as Kubernetes or Google Co-\\nlab. These will let you run on machines with a lot of memory and different hardware including \\nTensor Processing Units (TPUs) or GPUs.\\nWe’ll have a look here at Hugging Face’s transformers, llama.cpp, and GPT4All. These tools pro-\\nvide huge power and are full of great functionality too broad to cover in this chapter. Let’s start \\nby showing how we can run a model with the transformers library by Hugging Face.\\nPlease note that we don’t need an API token for local models!'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 108}, page_content='Getting Started with LangChain86\\nHugging Face Transformers\\nI’ll quickly show the general recipe for setting up and running a pipeline:\\nfrom transformers import pipeline\\nimport torch\\ngenerate_text = pipeline(\\n    model=\"aisquared/dlite-v1-355m\",\\n    torch_dtype=torch.bfloat16,\\n    trust_remote_code=True,\\n    device_map=\"auto\",\\n    framework=\"pt\"\\n)\\ngenerate_text(\"In this chapter, we\\'ll discuss first steps with generative \\nAI in Python.\")\\nRunning the preceding code will download everything that’s needed for the model such as the \\ntokenizer and model weights from Hugging Face. This model is quite small (355 million param-\\neters) but relatively performant and instruction-tuned for conversations. We can then run a text \\ncompletion to give us some inspiration for this chapter.\\nTo plug this pipeline into a LangChain agent or chain, we can use it the same way that we’ve seen \\nin the other examples in this chapter:\\nfrom langchain import PromptTemplate, LLMChain\\ntemplate = \"\"\"Question: {question}\\nAnswer: Let\\'s think step by step.\"\"\"\\nprompt = PromptTemplate(template=template, input_variables=[\"question\"])\\nllm_chain = LLMChain(prompt=prompt, llm=generate_text)\\nI haven’t included accelerate in the main requirements, but I’ve included the \\ntransformers library. If you don’t have all libraries installed, make sure you execute \\nthis command:\\npip install transformers accelerate torch'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 109}, page_content='Chapter 3 87\\nquestion = \"What is electroencephalography?\"\\nprint(llm_chain.run(question))\\nIn this example, we also see the use of a PromptTemplate that gives specific instructions for the task.\\nllama.cpp is a C++ port of Facebook’s LLaMA, LLaMA 2, and other derivative models with a \\nsimilar architecture. Let’s have a look at this next.\\nllama.cpp\\nWritten and maintained by Georgi Gerganov, llama.cpp is a C++ toolkit that executes models \\nbased on architectures based on or like LLaMA, one of the first large open-source models, which \\nwas released by Meta, and which spawned the development of many other models in turn. One \\nof the main use cases of llama.cpp is to run models efficiently on the CPU; however, there are \\nalso some options for GPU.\\nPlease note that you need to have an md5 checksum tool installed. This is included by default \\nin several Linux distributions such as Ubuntu. On macOS, you can install it with brew like this:\\nbrew install md5sha1sum\\nWe need to download the llama.cpp repository from GitHub. You can do this online by choosing \\none of the download options on GitHub, or you can use a git command from the terminal like this:\\ngit clone https://github.com/ggerganov/llama.cpp.git\\nThen we need to install the Python requirements, which we can do with the pip package installer \\n– let’s also switch to the llama.cpp project root directory for convenience:\\ncd llama.cpp\\npip install -r requirements.txt\\nYou might want to create a Python environment before you install the requirements, but this is \\nup to you. In my case, I received an error message at the end that a few libraries were missing, so \\nI had to execute this command:\\npip install \\'blosc2==2.0.0\\' cython FuzzyTM\\n Now we need to compile llama.cpp. We can parallelize the build with 4 processes:\\nmake -C . -j4 # runs make in subdir with 4 processes'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 110}, page_content='Getting Started with LangChain88\\nTo get the Llama model weights, you need to sign up with the T&Cs and wait for a registration \\nemail from Meta. There are tools such as the llama model downloader in the pyllama project, but \\nplease be advised that they might not conform to the license stipulations by Meta.\\nThere are also many other models with more permissive licensing such as Falcon or Mistral, Vi-\\ncuna, OpenLLaMA, or Alpaca. Let’s assume you download the model weights and the tokenizer \\nmodel for the OpenLLaMA 3B model using the link on the llama.cpp GitHub page. The model \\nfile should be about 6.8 Gigabyes big, the tokenizer is much smaller. You can move the two files \\ninto the models/3B directory.\\nYou can download models in much bigger sizes such as 13B, 30B, and 65B; however, a note of \\ncaution is in order here: these models are big both in terms of memory and disk space. We have \\nto convert the model to llama.cpp format, which is called ggml, using the convert script:\\npython3 convert.py models/3B/ --ctx 2048. \\nThen we can optionally quantize the models to save memory when doing inference. Quantization \\nrefers to reducing the number of bits that are used to store weight:\\n./quantize ./models/3B/ggml-model-f16.gguf ./models/3B/ggml-model-q4_0.bin \\nq4_0\\nThis last file is much smaller than the previous files and will take up much less space in memory \\nas well, which means that you can run it on smaller machines. Once we have chosen a model that \\nwe want to run, we can integrate it into an agent or a chain, for example, as follows:\\nllm = LlamaCpp(\\n    model_path=\"./ggml-model-q4_0.bin\",\\n    verbose=True\\n)\\nGPT4All Is a fantastic tool that not only includes running but also serving and customizing models.\\nGPT4All\\nThis tool is closely related to llama.cpp, and it’s based on an interface with llama.cpp. Compared \\nto llama.cpp, however, it’s much more convenient to use and much easier to install. The setup \\ninstructions for this book already include the gpt4all library, which is needed.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 111}, page_content='Chapter 3 89\\nAs for model support, GPT4All supports a large array of Transformer architectures:\\n• GPT-J\\n• LLaMA (via llama.cpp)\\n• Mosaic ML’s MPT architecture\\n• Replit\\n• Falcon\\n• BigCode’s StarCoder\\nYou can find a list of all available models on the project website, where you can also see their \\nresults in important benchmarks: https://gpt4all.io/.\\nHere’s a quick example of text generation with GPT4All:\\nfrom langchain.llms import GPT4All\\nmodel = GPT4All(model=\"mistral-7b-openorca.Q4_0.gguf\", n_ctx=512, n_\\nthreads=8)\\nresponse = model(\\n    \"We can run large language models locally for all kinds of \\napplications, \"\\n)\\nExecuting this should first download (if not downloaded yet) the model, which is one of the best \\nchat model available through GPT4All, pre-trained by the French startup Mistral AI, and fine-\\ntuned by the OpenOrca AI initiative. This model requires 3.83 GB of harddisk to store and 8 GB of \\nRAM to run. Then we should hopefully see some convincing arguments for running LLMs locally.\\nThis should serve as a first introduction to integrations with local models. In the next section, \\nwe’ll discuss building a text classification application in LangChain to assist customer service \\nagents. The goal is to categorize customer emails based on intent, extract sentiment, and generate \\nsummaries to help agents understand and respond faster.\\nBuilding an application for customer service\\nCustomer service agents are responsible for answering customer inquiries, resolving issues, and \\naddressing complaints. Their work is crucial for maintaining customer satisfaction and loyalty, \\nwhich directly affects a company’s reputation and financial success.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 112}, page_content='Getting Started with LangChain90\\nGenerative AI can assist customer service agents in several ways:\\n• Sentiment classification: This helps identify customer emotions and allows agents to \\npersonalize their responses.\\n• Summarization: This enables agents to understand the key points of lengthy customer \\nmessages and save time.\\n• Intent classification: Similar to summarization, this helps predict the customer’s purpose \\nand allows for faster problem-solving.\\n• Answer suggestions: This provides agents with suggested responses to common inquiries, \\nensuring that accurate and consistent messaging is provided.\\nThese approaches combined can help customer service agents respond more accurately and in \\na timely manner, improving customer satisfaction. Customer service is crucial for maintaining \\ncustomer satisfaction and loyalty. Generative AI can help agents in several ways – sentiment \\nanalysis to gauge emotion, summarization to identify key points, and intent classification to \\ndetermine purpose. Combined, these can enable more accurate, timely responses.\\nLangChain provides the flexibility to leverage different models. LangChain comes with many \\nintegrations that can enable us to tackle a wide range of text problems. We have a choice between \\nmany different integrations to perform these tasks.\\nWe can access all kinds of models for open-domain classification and sentiment and smaller \\ntransformer models through Hugging Face for focused tasks. We’ll build a prototype that uses \\nsentiment analysis to classify email sentiment, summarization to condense lengthy text, and \\nintent classification to categorize the issue.\\nGiven a document such as an email, we want to classify it into different categories related to \\nintent, extract the sentiment, and provide a summary. We will work on other projects for ques-\\ntion-answering in Chapter 5, Building a Chatbot Like ChatGPT.\\nWe could ask any LLM to give us an open-domain (any category) classification or choose between \\nmultiple categories. In particular, because of their large training size, LLMs are enormously pow-\\nerful models, especially when given few-shot prompts, for sentiment analysis that don’t need any \\nadditional training. This was analyzed by Zengzhi Wang and others in their April 2023 study, Is \\nChatGPT a Good Sentiment Analyzer? A Preliminary Study.\\nA prompt for an LLM for sentiment analysis could be something like this:\\nGiven this text, what is the sentiment conveyed? Is it positive, neutral, \\nor negative?'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 113}, page_content='Chapter 3 91\\nText: {sentence}\\nSentiment:\\nLLMs can also be highly effective at summarization, much better than any previous models. \\nThe downside can be that these model calls are slower than more traditional machine learning \\nmodels and more expensive.\\nIf we want to try out more traditional or smaller models, we can rely on libraries such as spaCy \\nor access them through specialized providers. Cohere and other providers have text classification \\nand sentiment analysis as part of their capabilities. For example, NLP Cloud’s model list includes \\nspaCy and many others: https://docs.nlpcloud.com/#models-list.\\nMany Hugging Face models are supported for these tasks, including:\\n• Document question-answering\\n• Summarization\\n• Text classification\\n• Text question-answering\\n• Translation\\nWe can execute these models either locally by running a pipeline in transformer, remotely on the \\nHugging Face Hub server (HuggingFaceHub), or as a tool through the load_huggingface_tool() \\nloader.\\nHugging Face contains thousands of models, many fine-tuned for particular domains. For example, \\nProsusAI/finbert is a BERT model that was trained on a dataset called Financial PhraseBank and \\ncan analyze the sentiment of financial text. We could also use any local model. For text classifi -\\ncation, the models tend to be much smaller, so this would be less of a drag on resources. Finally, \\ntext classification could also be a case for embeddings, which we’ll discuss in Chapter 5, Building \\na Chatbot Like ChatGPT.\\nI’ve decided to try and manage as much as I can with smaller models that I can find on Hugging \\nFace for this exercise.\\nWe can list the 5 most downloaded models on Hugging Face Hub for text classification through \\nthe Hugging Face API:\\nfrom huggingface_hub import list_models\\ndef list_most_popular(task: str):'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 114}, page_content='Getting Started with LangChain92\\n    for rank, model in enumerate(\\n        list_models(filter=task, sort=\"downloads\", direction=-1)\\n):\\n        if rank == 5:\\n            break\\n        print(f\"{model.id}, {model.downloads}\\\\n\")\\nlist_most_popular(\"text-classification\")\\nLet’s see the list:\\nModel Downloads\\ndistilbert-base-uncased-finetuned-sst-2-english 40,672,289\\ncardiffnlp/twitter-roberta-base-sentiment 9,292,338\\nMoritzLaurer/DeBERTa-v3-base-mnli-fever-anli 7,907,049\\ncardiffnlp/twitter-roberta-base-irony 7,023,579\\nSamLowe/roberta-base-go_emotions 6,706,653\\nTable 3.2: The most popular text classification models on Hugging Face Hub\\nGenerally, we should see that these models are about small ranges of categories such as sentiment, \\nemotions, irony, or well-formedness. Let’s use a sentiment model with a customer email, which \\nshould be a common use case in customer service.\\nI’ve asked GPT-3.5 to put together a rambling customer email complaining about a coffee machine \\n– I’ve shortened it a bit here. You can find the full email on GitHub. Let’s see what our sentiment \\nmodel has to say:\\nfrom transformers import pipeline\\ncustomer_email = \"\"\"\\nI am writing to pour my heart out about the recent unfortunate experience \\nI had with one of your coffee machines that arrived broken. I anxiously \\nunwrapped the box containing my highly anticipated coffee machine. \\nHowever, what I discovered within broke not only my spirit but also any \\nsemblance of confidence I had placed in your brand.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 115}, page_content='Chapter 3 93\\nIts once elegant exterior was marred by the scars of travel, resembling a \\nwar-torn soldier who had fought valiantly on the fields of some espresso \\nbattlefield. This heartbreaking display of negligence shattered my dreams \\nof indulging in daily coffee perfection, leaving me emotionally distraught \\nand inconsolable\\n\"\"\"\\nsentiment_model = pipeline(\\n    task=\"sentiment-analysis\",\\n    model=\"cardiffnlp/twitter-roberta-base-sentiment\"\\n)\\nprint(sentiment_model(customer_email))\\nThe sentiment model we are using here, Twitter-roBERTa-base, was trained on tweets, so it \\nmight not be the most adequate use case. Apart from emotion sentiment analysis, this model can \\nalso perform other tasks such as emotion recognition (anger, joy, sadness, or optimism), emoji \\nprediction, irony detection, hate speech detection, offensive language identification, and stance \\ndetection (favor, neutral, or against).\\nFor the sentiment analysis, we’ll get a rating and a numeric score that expresses confidence in \\nthe label. These are the labels:\\n• 0 – negative\\n• 1 – neutral\\n• 2 – positive\\nPlease make sure you have all the dependencies installed according to instructions in order to \\nexecute this. I am getting this result:\\n[{\\'label\\': \\'LABEL_0\\', \\'score\\': 0.5822020173072815}]\\nNot a happy camper.\\nFor comparison, if the email says “I am so angry and sad, I want to kill myself,” we should get a \\nscore of close to 0.98 for the same label. We could try out other models or train better models \\nonce we have established metrics to work against.\\nLet’s move on!'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 116}, page_content='Getting Started with LangChain94\\nHere are the 5 most popular models for summarization as well (downloads at the time of writing, \\nOctober 2023):\\nModel Downloads\\nfacebook/bart-large-cnn 4,637,417\\nt5-small 2,492,451\\nt5-base 1,887,661\\nsshleifer/distilbart-cnn-12-6 715,809\\nt5-large 332,854\\nTable 3.3: The most popular summarization models on Hugging Face Hub\\nAll these models have a small footprint, which is nice, but to apply them in earnest, we should \\nmake sure they are reliable enough.\\nLet’s execute the summarization model remotely on a server. Please note that you need to have \\nyour HUGGINGFACEHUB_API_TOKEN set for this to work:\\nfrom langchain import HuggingFaceHub\\nsummarizer = HuggingFaceHub(\\n    repo_id=\"facebook/bart-large-cnn\",\\n    model_kwargs={\"temperature\":0, \"max_length\":180}\\n)\\ndef summarize(llm, text) -> str:\\n    return llm(f\"Summarize this: {text}!\")\\nsummarize(summarizer, customer_email)\\nAfter executing this, I see this summary:\\nA customer\\'s coffee machine arrived ominously broken, evoking a profound \\nsense of disbelief and despair. \"This heartbreaking display of negligence \\nshattered my dreams of indulging in daily coffee perfection, leaving me \\nemotionally distraught and inconsolable,\" the customer writes. \"I hope \\nthis email finds you amidst an aura of understanding, despite the tangled \\nmess of emotions swirling within me as I write to you,\" he adds.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 117}, page_content='Chapter 3 95\\nThis summary is just passable, but not very convincing. There is still a lot of rambling in the sum-\\nmary. We could try other models or just go for an LLM with a prompt asking to summarize. We’ll \\nlook at summarization in much more detail in Chapter 4, Building Capable Assistants. Let’s move on.\\nIt could be quite useful to know what kind of issue the customer is writing about. Let’s ask Vertex \\nAI:\\nfrom langchain.llms import VertexAI\\nfrom langchain import PromptTemplate, LLMChain\\ntemplate = \"\"\"Given this text, decide what is the issue the customer is \\nconcerned about. Valid categories are these:\\n* product issues\\n* delivery problems\\n* missing or late orders\\n* wrong product\\n* cancellation request\\n* refund or exchange\\n* bad support experience\\n* no clear reason to be upset\\nText: {email}\\nCategory:\\n\"\"\"\\nprompt = PromptTemplate(template=template, input_variables=[\"email\"])\\nllm = VertexAI()\\nllm_chain = LLMChain(prompt=prompt, llm=llm, verbose=True)\\nprint(llm_chain.run(customer_email))\\nWe get product issues back, which is correct for the long email example that I am using here.\\nBefore you execute the following code, make sure you have authenticated with GCP \\nand you’ve set your GCP project according to the instructions mentioned in the \\nsection about Vertex AI.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 118}, page_content='Getting Started with LangChain96\\nI hope it was exciting to see how quickly we can throw a few models and tools together in Lang-\\nChain to get something that looks actually useful. With thoughtful implementation, such AI \\nautomation can complement human agents – handling frequent questions to allow focusing \\non complex problems. Overall, this demonstrates generative AI’s potential to enhance customer \\nservice workflows.\\nWe could easily expose this in a graphical interface for customer service agents to see and interact \\nwith. This is something we will do in the next chapter.\\nLet’s wrap up!\\nSummary\\nIn this chapter, we walked through four distinct ways of installing LangChain and other libraries \\nneeded in this book as an environment. Then, we introduced several providers of models for text \\nand images. For each of them, we explained where to get the API token, and demonstrated how \\nto call a model.\\nFinally, we developed an LLM app for text categorization (intent classification) and sentiment \\nanalysis in a use case for customer service. This showcases LangChain’s ease in orchestrating \\nmultiple models to create useful applications. By chaining together various functionalities in \\nLangChain, we can help reduce response times in customer service and make sure answers are \\naccurate and to the point.\\nIn Chapter 4, Building Capable Assistants and Chapter 5, Building a Chatbot Like ChatGPT, we’ll dive \\nmore into use cases such as question answering in chatbots through augmentation with tools \\nand retrieval.\\nQuestions\\nPlease look to see whether you can provide answers to these questions. I’d recommend you go \\nback to the corresponding sections of this chapter if you are unsure about any of them:\\n1. How do you install LangChain?\\n2. List at least 4 cloud providers of LLMs apart from OpenAI!\\n3. What are Jina AI and Hugging Face?\\n4. How do you generate images with LangChain?\\n5. How do you run a model locally on your own machine rather than through a service?\\n6. How do you perform text classification in LangChain?\\n7. How can we help customer service agents in their work through generative AI?'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 119}, page_content='Chapter 3 97\\nJoin our community on Discord\\nJoin our community’s Discord space for discussions with the authors and other readers:\\nhttps://packt.link/lang'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 120}, page_content=''),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 121}, page_content='4\\nBuilding Capable Assistants\\nAs LLMs continue to advance, a key challenge is transforming their impressive fluency into reliably \\ncapable assistants. This chapter explores methods for instilling greater intelligence, productivity, \\nand trustworthiness in LLMs. The unifying theme across these approaches is enhancing LLMs \\nthrough prompts, tools, and structured reasoning techniques. We’ll have sample applications \\nthat demonstrate these techniques in this chapter.\\nWe will begin by addressing the critical weakness of hallucinated content through automatic \\nfact-checking. By verifying claims against the available evidence, we can reduce the spread of \\nmisinformation. We will continue by discussing a key strength of LLMs with important appli-\\ncations – summarization, which we’ll go into with the integration of prompts at different levels \\nof sophistication, and the map reduce approach for very long documents. We will then move on \\nto information extraction from documents with function calls, which leads to the topic of tool \\nintegrations. We’ll implement an application that showcases how connecting external data and \\nservices can augment LLMs’ limited world knowledge. Finally, we will further extend this appli-\\ncation through the application of reasoning strategies.\\nIn short, this chapter covers:\\n• Mitigating hallucinations through fact-checking\\n• Summarizing information\\n• Extracting information from documents\\n• Answering questions with tools\\n• Exploring reasoning strategies\\nLet’s get started with addressing hallucinations through automatic fact-checking!'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 122}, page_content='Building Capable Assistants100\\nMitigating hallucinations through fact-checking\\nAs discussed in previous chapters, hallucination in LLMs refers to the generated text being un-\\nfaithful or nonsensical compared to the input. It contrasts with faithfulness, where outputs stay \\nconsistent with the source. Hallucinations can spread misinformation like disinformation, rumors, \\nand deceptive content. This poses threats to society, including distrust in science, polarization, \\nand democratic processes.\\nJournalism and archival studies have researched misinformation extensively. Fact-checking ini-\\ntiatives provide training and resources to journalists and independent checkers, allowing expert \\nverification at scale. Addressing false claims is crucial to preserving information integrity and \\ncombatting detrimental societal impacts.\\nOne technique to address hallucinations is automatic fact-checking – verifying claims made by \\nLLMs against evidence from external sources. This allows for catching incorrect or unverified \\nstatements.\\nFact-checking involves three main stages:\\n1. Claim detection: Identify parts needing verification\\n2. Evidence retrieval: Find sources supporting or refuting the claim\\n3. Verdict prediction: Assess claim veracity based on evidence\\nAlternative terms for the last two stages are justification production and verdict prediction.\\nWe can see the general idea of these three stages illustrated in the following diagram (source – \\nhttps://github.com/Cartus/Automated-Fact-Checking-Resources by Zhijiang Guo):\\nFigure 4.1: Automatic fact-checking pipeline in three stages'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 123}, page_content=\"Chapter 4 101\\nPre-trained LLMs contain extensive world knowledge that can be prompted for facts. Additionally, \\nexternal tools can search knowledge bases, Wikipedia, textbooks, and corpora for evidence. By \\ngrounding claims in data, fact-checking makes LLMs more reliable.\\nPre-trained LLMs contain extensive world knowledge from their training data. Starting with the \\n24-layer BERT-Large in 2018, language models have been pre-trained on large knowledge bases \\nsuch as Wikipedia; therefore, they would be able to answer knowledge questions from Wikipedia \\nor – since their training set increasingly includes other sources – the internet, textbooks, arXiv, \\nand GitHub.\\nWe can prompt them with masking and other techniques to retrieve facts for evidence. For exam-\\nple, to answer the question “Where is Microsoft’s headquarters located?”, the question would be \\nrewritten as “Microsoft’s headquarters is in [MASK]” and fed into a language model for the answer.\\nAlternatively, we can integrate external tools to search knowledge bases, Wikipedia, textbooks, \\nand other corpora. The key idea is verifying hallucinated claims by grounding them in factual \\ndata sources.\\nAutomatic fact-checking provides a way to make LLMs more reliable by checking that their re -\\nsponses align with real-world evidence. In the next sections, we’ll demonstrate this approach.\\nIn LangChain, we have a chain available for fact-checking with prompt chaining, where a mod-\\nel actively questions the assumptions that went into a statement. In this self-checking chain, \\nLLMCheckerChain, the model is prompted sequentially – first, to make the assumptions explicit, \\nwhich looks like this:\\nHere's a statement: {statement}\\\\nMake a bullet point list of the \\nassumptions you made when producing the above statement.\\\\n\\nPlease note that this is a string template, where the elements in curly brackets will be replaced \\nby variables. Next, these assumptions are fed back to the model in order to check them one by \\none with a prompt like this:\\nHere is a bullet point list of assertions:\\n    {assertions}\\n    For each assertion, determine whether it is true or false. If it is \\nfalse, explain why.\\\\n\\\\n\\nFinally, the model is tasked to make a final judgment:\\nIn light of the above facts, how would you answer the question \\n'{question}'\"),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 124}, page_content='Building Capable Assistants102\\nLLMCheckerChain does this all by itself, as this example shows:\\nfrom langchain.chains import LLMCheckerChain\\nfrom langchain.llms import OpenAI\\nllm = OpenAI(temperature=0.7)\\ntext = \"What type of mammal lays the biggest eggs?\"\\nchecker_chain = LLMCheckerChain.from_llm(llm, verbose=True)\\nchecker_chain.run(text)\\nThe model can return different results to this question, some of which are wrong, and some of \\nwhich it would correctly identify as false. When I was trying this out, I got results such as the blue \\nwhale, the North American beaver, and the extinct Giant Moa in response to my question \"What \\ntype of mammal lays the biggest eggs?\". The following is the right answer:\\nMonotremes, a type of mammal found in Australia and parts of New Guinea, \\nlay the largest eggs in the mammalian world. The eggs of the American \\nechidna (spiny anteater) can grow as large as 10 cm in length, and \\ndunnarts (mouse-sized marsupials found in Australia) can have eggs that \\nexceed 5 cm in length.\\n• Monotremes can be found in Australia and New Guinea\\n• The largest eggs in the mammalian world are laid by monotremes\\n• The American echidna lays eggs that can grow to 10 cm in length\\n• Dunnarts lay eggs that can exceed 5 cm in length\\n• Monotremes can be found in Australia and New Guinea – True\\n• The largest eggs in the mammalian world are laid by monotremes – True\\n• The American echidna lays eggs that can grow to 10 cm in length – False, \\nthe American echidna lays eggs that are usually between 1 to 4 cm in \\nlength.\\n• Dunnarts lay eggs that can exceed 5 cm in length – False, dunnarts lay \\neggs that are typically between 2 to 3 cm in length.\\nThe largest eggs in the mammalian world are laid by monotremes, which can \\nbe found in Australia and New Guinea. Monotreme eggs can grow to 10 cm in \\nlength.\\n> Finished chain.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 125}, page_content='Chapter 4 103\\nSo, while this technique does not guarantee correct answers, it can put a stop to some incorrect \\nresults. Fact-checking approaches involve decomposing claims into smaller checkable queries, \\nwhich can be formulated as question-answering tasks. Tools designed for searching domain \\ndatasets can assist fact-checkers in finding evidence effectively. Off-the-shelf search engines \\nlike Google and Bing can also retrieve both topically and evidentially relevant content to capture \\nthe veracity of a statement accurately. We’ll apply this approach to return results based on web \\nsearches and other applications of this chapter.\\nIn the next section, we’ll discuss automating the process of summarizing texts and longer doc-\\numents such as research papers.\\nSummarizing information\\nIn today’s fast-paced business and research landscape, keeping up with the ever-increasing vol-\\nume of information can be a daunting task. For engineers and researchers in fields like computer \\nscience and artificial intelligence, staying updated with the latest developments is crucial. How-\\never, reading and comprehending numerous papers can be time-consuming and labor-intensive. \\nThis is where automation comes into play. As engineers, we are driven by the desire to build and \\ninnovate and avoid repetitive tasks by automating them through the creation of pipelines and \\nprocesses. This approach, often mistaken for laziness, allows engineers to focus on more complex \\nchallenges and utilize their skills more efficiently.\\nLLMs excel at condensing text through their strong language understanding abilities. We will \\nexplore techniques for summarization using LangChain at increasing levels of sophistication.\\nBasic prompting\\nFor summarizing a couple of sentences, basic prompting works well. Simply instruct the LLM on \\nthe desired length and provide a text:\\nfrom langchain import OpenAI\\nprompt = \"\"\"\\nSummarize this text in one sentence:\\n{text}\\n\"\"\"\\nllm = OpenAI()\\nsummary = llm(prompt.format(text=text))'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 126}, page_content='Building Capable Assistants104\\nThis is similar to what we saw in Chapter 3, Getting Started with LangChain. text is a string variable \\nthat can be any text that we want to summarize.\\nWe can also use the LangChain decorator syntax, which is implemented in the LangChain \\ndecorators library, which you should have installed together with all the other dependencies if \\nyou followed the instructions in Chapter 3, Getting Started with LangChain.\\nLangChain Decorators provides a more Pythonic interface for defining and executing prompts \\ncompared to base LangChain, making it easier to leverage the power of LLMs. Function decora-\\ntors translate prompt documentation into executable code, enabling multiline definitions and \\nnatural code flow.\\nHere’s a decorator example for summarization:\\nfrom langchain_decorators import llm_prompt\\n@llm_prompt\\ndef summarize(text:str, length=\"short\") -> str:\\n    \"\"\"\\n    Summarize this text in {length} length:\\n    {text}\\n    \"\"\"\\n    return\\nsummary = summarize(text=\"let me tell you a boring story from when I was \\nyoung...\")\\nThe output, the value of the summary variable, I am getting is The speaker is about to share \\na story from their youth . You can try more meaningful and longer examples for summari -\\nzation yourself.\\nThe @llm_prompt decorator translates the docstring into a prompt and handles executing it. Pa-\\nrameters are cleanly passed in and outputs are parsed. This abstraction enables prompting in a \\nnatural Python style while handling the complexity behind the scenes, making it easy to focus \\non creating effective prompts. By providing this intuitive interface, LangChain Decorators unlock \\nthe power of LLMs for developers.\\nPrompt templates\\nFor dynamic inputs, prompt templates enable inserting text into predefined prompts. Prompt \\ntemplates allow variable length limits and modular prompt design.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 127}, page_content='Chapter 4 105\\nWe can implement this in LangChain Expression Language (LCEL):\\nfrom langchain import PromptTemplate, OpenAI\\nfrom langchain.schema import StrOutputParser\\nllm = OpenAI()\\nprompt = PromptTemplate.from_template(\\n    \"Summarize this text: {text}?\"\\n)\\nrunnable = prompt | llm | StrOutputParser()\\nsummary = runnable.invoke({\"text\": text})\\nLCEL provides a declarative way to compose chains that is more intuitive and productive than \\ndirectly writing code. Key benefits of LCEL include built-in support for asynchronous processing, \\nbatching, streaming, fallbacks, parallelism, and seamless integration with LangSmith tracing.\\nIn this case, runnable is a chain, where the prompt template, the LLM, and the output parser are \\npiped into one another.\\nChain of density\\nResearchers at Salesforce (Adams and colleagues, 2023; From Sparse to Dense: GPT-4 Summariza-\\ntion with Chain of Density Prompting) have developed a prompt-guided technique called Chain of \\nDensity (CoD) to incrementally increase the information density of GPT-4 generated summaries \\nwhile controlling length.\\nThis is the prompt to use with CoD:\\ntemplate = \"\"\"Article: { text }\\nYou will generate increasingly concise, entity-dense summaries of the \\nabove article.\\nRepeat the following 2 steps 5 times.\\nStep 1. Identify 1-3 informative entities (\";\" delimited) from the article \\nwhich are missing from the previously generated summary.\\nStep 2. Write a new, denser summary of identical length which covers every \\nentity and detail from the previous summary plus the missing entities.\\nA missing entity is:\\n- relevant to the main story,\\n- specific yet concise (5 words or fewer),\\n- novel (not in the previous summary),\\n- faithful (present in the article),\\n- anywhere (can be located anywhere in the article).'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 128}, page_content='Building Capable Assistants106\\nGuidelines:\\n- The first summary should be long (4-5 sentences, ~80 words) yet highly \\nnon-specific, containing little information beyond the entities marked \\nas missing. Use overly verbose language and fillers (e.g., \"this article \\ndiscusses\") to reach ~80 words.\\n- Make every word count: rewrite the previous summary to improve flow and \\nmake space for additional entities.\\n- Make space with fusion, compression, and removal of uninformative \\nphrases like \"the article discusses\".\\n- The summaries should become highly dense and concise yet self-contained, \\ni.e., easily understood without the article.\\n- Missing entities can appear anywhere in the new summary.\\n- Never drop entities from the previous summary. If space cannot be made, \\nadd fewer new entities.\\nRemember, use the exact same number of words for each summary.\\nAnswer in JSON. The JSON should be a list (length 5) of dictionaries whose \\nkeys are \"Missing_Entities\" and \"Denser_Summary\".\\n\"\"\"\\nPlease note that you can easily adapt this to any kind of content and provide a different set of \\nguidelines to suit other applications.\\nThe CoD prompt instructs highly powered LLMs such as GPT-4 to produce an initial sparse, ver-\\nbose summary of an article containing only a few entities. It then iteratively identifies 1–3 missing \\nentities and fuses them into a rewrite of the previous summary in the same number of words.\\nThis repeated rewriting under length constraint forces increasing abstraction, fusion of details, \\nand compression to make room for additional entities in each step. The authors measure statis-\\ntics like entity density and source sentence alignment to characterize the densification effects.\\nThrough five iterative steps, summaries become highly condensed with more entities per token \\npacked in through creative rewriting. The authors conduct both human preference studies and \\nGPT-4 scoring to evaluate the impact on overall quality across the density spectrum.\\nThe results reveal a trade-off between informativeness gained through density and declining \\ncoherence from excessive compression. Optimal density balances concision and clarity, with too \\nmany entities overwhelming expression. This method and analysis sheds light on controlling \\ninformation density in AI text generation.\\nPlease try this out for yourself!'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 129}, page_content='Chapter 4 107\\nMap-Reduce pipelines\\nLangChain supports a map reduce approach  for processing documents using LLMs, which al -\\nlows for efficient processing and analysis of documents. A chain can be applied to each document \\nindividually and then we combine the outputs into a single document.\\nTo summarize long documents, we can first split the document into smaller parts (chunks) that \\nare suitable for the token context length of the LLM, and then a map-reduce chain can summarize \\nthese chunks independently before recombining. This scales summarization to any length of text \\nwhile controlling chunk size.\\nThe key steps are:\\n1. Map: Each document is passed through a summarization chain (LLM chain).\\n2. Collapse (optional): The summarized documents are combined into a single document.\\n3. Reduce: The collapsed document goes through a final LLM chain to produce the output.\\nSo, the map step applies a chain to each document in parallel. The reduce step aggregates the \\nmapped outputs and generates the final result.\\nOptional collapsing, which may also involve utilizing LLMs, makes sure the data fits within se -\\nquence length limits. This compression step can be performed recursively if needed.\\nThis is illustrated in the figure here:\\nFigure 4.2: Map reduce chain in LangChain'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 130}, page_content='Building Capable Assistants108\\nThis approach’s implications are that it allows the parallel processing of documents and enables \\nthe use of LLMs for reasoning, generating, or analyzing individual documents and combining \\ntheir outputs.\\nHere’s a simple example of loading a PDF document and summarizing it:\\nfrom langchain.chains.summarize import load_summarize_chain\\nfrom langchain import OpenAI\\nfrom langchain.document_loaders import PyPDFLoader\\npdf_file_path = \"<pdf_file_path>\"\\npdf_loader = PyPDFLoader(pdf_file_path)\\ndocs = pdf_loader.load_and_split()\\nllm = OpenAI()\\nchain = load_summarize_chain(llm, chain_type=\"map_reduce\")\\nchain.run(docs)\\nThe variable pdf_file_path is a string with the path of a PDF file. Please replace the file path \\nwith the path to a PDF document.\\nThe default prompt for both the map and reduce steps is this:\\nWrite a concise summary of the following:\\n{text}\\nCONCISE SUMMARY:\\nWe can specify any prompt for each step. In the text summarization application developed for \\nthis chapter on GitHub, we can see how to pass other prompts. On LangChainHub, we can see \\nthe question-answering-with-sources prompt, which takes a reduce/combine prompt like this:\\nGiven the following extracted parts of a long document and a question, \\ncreate a final answer with references (\\\\\"SOURCES\\\\\"). \\\\nIf you don\\'t know \\nthe answer, just say that you don\\'t know. Don\\'t try to make up an answer.\\\\\\nnALWAYS return a \\\\\"SOURCES\\\\\" part in your answer.\\\\n\\\\nQUESTION: {question}\\\\\\nn=========\\\\nContent: {text}\\nIn the preceding prompt, we could formulate a concrete question, but equally, we could give the \\nLLM a more abstract instruction to extract assumptions and implications. '),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 131}, page_content='Chapter 4 109\\nThe text would be the summaries from the map steps. An instruction like that would help against \\nhallucinations. Other examples of instructions could be translating the document into a different \\nlanguage or rephrasing in a certain style.\\nBy changing the prompt, we can ask any question to be answered from these documents. This \\ncan be built out into an automation tool that can quickly summarize the content of long texts \\nin a more digestible format, as you should be able to tell from the summarize package in the \\nbook’s GitHub repository, which shows how to focus on different perspectives and structures of \\nthe response (adapted from David Shapiro).\\nThe tool on GitHub will summarize the core assertions, implications, and mechanics of a paper \\nin a more concise and simplified manner. It can also answer specific questions about the paper, \\nmaking it a valuable resource for literature reviews and accelerating scientific research. Overall, \\nthe approach aims to benefit researchers by providing a more efficient and accessible way to stay \\nupdated on the latest research.\\nOnce we start making a lot of calls, especially in the map step, if we use a cloud provider, we’ll \\nsee tokens and, therefore, costs increase. It’s time to give this some visibility!\\nMonitoring token usage\\nWhen using LLMs, especially in long loops such as with map operations, it’s important to track \\nthe token usage and understand how much money you are spending.\\nFor any serious usage of generative AI, we need to understand the capabilities, pricing options, \\nand use cases for different language models. All cloud providers provide different models that \\ncater to various NLP needs. For example, OpenAI exposes powerful language models suitable \\nfor solving complex problems with NLP and offers flexible pricing options based on the size and \\nnumber of tokens used.\\nThoughtful prompt engineering with LangChain provides powerful summarization \\ncapabilities using LLMs. A few practical tips are:\\n• Start with simpler approaches and move to map-reduce if needed\\n• Tune chunk size to balance context limits and parallelism\\n• Customize map and reduce prompts for the best results\\n• Compress or recursively reduce chunks to fit context limits'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 132}, page_content='Building Capable Assistants110\\nFor example, ChatGPT models, like GPT-3.5-Turbo, specialize in dialogue applications such as \\nchatbots and virtual assistants. They excel at generating responses with accuracy and fluency. \\nDifferent models within the InstructGPT family, designed for single-turn instruction following, \\nsuch as Ada and Davinci, offer varying levels of speed and power. Ada is the fastest model, suit -\\nable for applications where speed is crucial, while Davinci is the most powerful model, capable \\nof handling complex instructions. The pricing of models depends on the model’s capabilities and \\nranges from low-cost options like Ada to more expensive options like Davinci.\\nOpenAI provides DALL·E, Whisper, and API services for various applications, such as image gener-\\nation, speech transcription, translation, and access to language models. DALL·E is an AI-powered \\nimage generation model that can be seamlessly integrated into apps for generating and editing \\nnovel images and art. OpenAI offers three tiers of resolution, allowing users to choose the level \\nof detail they need. Higher resolutions offer more complexity and detail, while lower resolutions \\nprovide a more abstract representation. The price per image varies based on the resolution.\\nWhisper is an AI tool that can transcribe speech into text and translate multiple languages into \\nEnglish. It helps capture conversations, facilitates communication, and improves understanding \\nacross languages. The cost of using Whisper is based on a per-minute rate.\\nWe can track the token usage in OpenAI models by hooking into the OpenAI callback:\\nfrom langchain import OpenAI, PromptTemplate\\nfrom langchain.callbacks import get_openai_callback\\nllm_chain = PromptTemplate.from_template(\"Tell me a joke about {topic}!\") \\n| OpenAI()\\nwith get_openai_callback() as cb:\\n    response = llm_chain.invoke(dict(topic=\"light bulbs\"))\\n    print(response)\\n    print(f\"Total Tokens: {cb.total_tokens}\")\\n    print(f\"Prompt Tokens: {cb.prompt_tokens}\")\\n    print(f\"Completion Tokens: {cb.completion_tokens}\")\\n    print(f\"Total Cost (USD): ${cb.total_cost}\")\\nWe should see an output with the costs and tokens. I am getting this output when I run this:\\nQ: How many light bulbs does it take to change people\\'s minds?\\nA: Depends on how stubborn they are!\\nTotal Tokens: 36\\nPrompt Tokens: 8'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 133}, page_content='Chapter 4 111\\nCompletion Tokens: 28\\nTotal Cost (USD): $0.00072\\nYou can change the parameters of the model and the prompt, and you should see costs and tokens \\nchanging as a consequence.\\nThere are two other ways of getting the token usage. As an alternative to the OpenAI callback, \\nthe generate() method of the llm class returns a response of type LLMResult instead of a string. \\nThis includes token usages and finish reason, for example (from the LangChain docs):\\ninput_list = [\\n    {\"product\": \"socks\"},\\n    {\"product\": \"computer\"},\\n    {\"product\": \"shoes\"}\\n]\\nllm_chain.generate(input_list)\\nThe result looks like this:\\n    LLMResult(generations=[[Generation(text=\\'\\\\n\\\\nSocktastic!\\', generation_\\ninfo={\\'finish_reason\\': \\'stop\\', \\'logprobs\\': None})], [Generation(text=\\'\\\\n\\\\\\nnTechCore Solutions.\\', generation_info={\\'finish_reason\\': \\'stop\\', \\n\\'logprobs\\': None})], [Generation(text=\\'\\\\n\\\\nFootwear Factory.\\', generation_\\ninfo={\\'finish_reason\\': \\'stop\\', \\'logprobs\\': None})]], llm_output={\\'token_\\nusage\\': {\\'prompt_tokens\\': 36, \\'total_tokens\\': 55, \\'completion_tokens\\': \\n19}, \\'model_name\\': \\'text-davinci-003\\'})\\nFinally, the chat completions response format in the OpenAI API includes a usage object with \\ntoken information; for example, it could look like this (excerpt):\\n  {\\n  \"model\": \"gpt-3.5-turbo-0613\",\\n  \"object\": \"chat.completion\",\\n  \"usage\": {\\n    \"completion_tokens\": 17,\\n    \"prompt_tokens\": 57,\\n    \"total_tokens\": 74\\n  }\\n}'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 134}, page_content='Building Capable Assistants112\\nThis can be extremely helpful for understanding how much money you are spending on distinct \\nparts of your application. In Chapter 9, Generative AI in Production. we’ll look at LangSmith and \\nsimilar tools that provide additional observability of LLMs in action, including their token usage.\\nNext, we’ll look at how to extract certain pieces of information from documents using OpenAI \\nfunctions with LangChain.\\nExtracting information from documents\\nIn June 2023, OpenAI announced updates to OpenAI’s API, including new capabilities for function \\ncalling, which enhanced functionality. OpenAI’s addition of function calling builds on instruc-\\ntion tuning. By describing functions in a schema, developers can tune LLMs to return structured \\noutputs adhering to that schema – for example, extracting entities from text by outputting them \\nin a predefined JSON format.\\nFunction calling enables developers to create chatbots that can answer questions using external \\ntools or OpenAI plugins. It also allows for converting natural language queries into API calls or \\ndatabase queries and extracting structured data from text.\\nDevelopers can now describe functions to the gpt-4-0613 and gpt-3.5-turbo-0613 models and \\nhave the models intelligently generate a JSON object containing arguments to call those functions. \\nThis feature aims to enhance the connection between GPT models and external tools and APIs, \\nproviding a reliable way to retrieve structured data from the models.\\nThe mechanics of the update involve using new API parameters, namely functions, in the /v1/\\nchat/completions endpoint. The functions parameter is defined through a name, description, \\nparameters, and the function to call itself. Developers can describe functions to the model using \\nJSON schema and specify the desired function to be called.\\nIn LangChain, we can use these function calls in OpenAI for information extraction or for calling \\nplugins. For information extraction, we can obtain specific entities and their properties from a \\ntext and their properties from a document in an extraction chain with OpenAI chat models. For \\nexample, this can help identify the people mentioned in the text. By using the OpenAI functions \\nparameter and specifying a schema, it ensures that the model outputs the desired entities and \\nproperties with their appropriate types.\\nThe implications of this approach are that it allows for precise extraction of entities by defining \\na schema with the desired properties and their types. It also enables specifying which properties \\nare required and which are optional.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 135}, page_content='Chapter 4 113\\nThe default format for the schema is a dictionary, but we can also define properties and their types \\nin Pydantic, a popular parsing library, providing control and flexibility in the extraction process.\\nHere’s an example of a desired schema for information in a Curriculum Vitae (CV):\\nfrom typing import Optional\\nfrom pydantic import BaseModel\\nclass Experience(BaseModel):\\n    start_date: Optional[str]\\n    end_date: Optional[str]\\n    description: Optional[str]\\nclass Study(Experience):\\n    degree: Optional[str]\\n    university: Optional[str]\\n    country: Optional[str]\\n    grade: Optional[str]\\nclass WorkExperience(Experience):\\n    company: str\\n    job_title: str\\nclass Resume(BaseModel):\\n    first_name: str\\n    last_name: str\\n    linkedin_url: Optional[str]\\n    email_address: Optional[str]\\n    nationality: Optional[str]\\n    skill: Optional[str]\\n    study: Optional[Study]\\n    work_experience: Optional[WorkExperience]\\n    hobby: Optional[str]\\nWe can use this for information extraction from a CV.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 136}, page_content='Building Capable Assistants114\\nPlease note that you should set up your environment according to the instructions in Chapter 3, \\nGetting Started with LangChain. I’ve found it most convenient to import my config module here \\nand execute setup_environment(). This adds two extra lines to the beginning of the code:\\nfrom config import setup_environment\\nsetup_environment()\\nThis is my advice – you can take it or leave it.\\nHere’s an example CV from https://github.com/xitanggg/open-resume:\\nFigure 4.3: Extract of an example CV\\nWe are going to try to parse the information from this resume.\\nUtilizing the create_extraction_chain_pydantic() function in LangChain, we can provide \\nour schema as input, and an output will be an instantiated object that adheres to it. In its most \\nsimple terms, we can try this code snippet:\\nfrom langchain.chains import create_extraction_chain_pydantic\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.document_loaders import PyPDFLoader'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 137}, page_content='Chapter 4 115\\npdf_file_path = \"<pdf_file_path>\"\\npdf_loader = PyPDFLoader(pdf_file_path)\\ndocs = pdf_loader.load_and_split()\\n# please note that function calling is not enabled for all models!\\nllm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0613\")\\nchain = create_extraction_chain_pydantic(pydantic_schema=Resume, llm=llm)\\nchain.run(docs)\\nPlease note that the pdf_file_path variable should be the relative or absolute path to a pdf file. \\nWe should get an output like this:\\n[Resume(first_name=\\'John\\', last_name=\\'Doe\\', linkedin_url=\\'linkedin.com/\\nin/john-doe\\', email_address=\\'hello@openresume.com\\', nationality=None, \\nskill=\\'React\\', study=None, work_experience=WorkExperience(start_date=\\'May \\n2023\\', end_date=\\'Present\\', description=\\'Lead a cross-functional team of \\n5 engineers in developing a search bar, which enables thousands of daily \\nactive users to search content across the entire platform. Create stunning \\nhome page product demo animations that drives up sign up rate by 20%. \\nWrite clean code that is modular and easy to maintain while ensuring 100% \\ntest coverage.\\', company=\\'ABC Company\\', job_title=\\'Software Engineer\\'), \\nhobby=None)]\\nThis result is far from perfect – only one work experience gets parsed out. But it’s a good start given \\nthe little effort we’ve put in so far. For a complete example, please refer to the GitHub repository. \\nWe could add more functionality, for example, to guess personality or leadership capability.\\nOpenAI injects these function calls into the system message in a certain syntax, which their \\nmodels have been optimized for. This implies that functions count against the context limit and \\nare correspondingly billed as input tokens.\\nLangChain natively has the functionality to inject function calls as prompts. This means we can \\nuse models from providers other than OpenAI for function calls within LLM apps. We’ll look at \\nthis now, and we’ll build this into an interactive web app with Streamlit.\\nInstruction tuning and function calling allow models to produce callable code. This leads to tool \\nintegrations, where LLM agents can execute these function calls to connect LLMs with live data, \\nservices, and runtime environments. In the next section, we’ll discuss how tools can augment \\ncontext by retrieving external knowledge sources to enhance understanding.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 138}, page_content='Building Capable Assistants116\\nAnswering questions with tools\\nLLMs are trained on general corpus data and may not be as effective for tasks that require do -\\nmain-specific knowledge. On their own, LLMs can’t interact with the environment and access \\nexternal data sources; however, LangChain provides a platform for creating tools that access \\nreal-time information and perform tasks such as weather forecasting, making reservations, sug-\\ngesting recipes, and managing tasks. Tools within the framework of agents and chains allow for \\nthe development of applications powered by LLMs that are data-aware and agentic and open \\nup a wide range of approaches to solving problems with LLMs, expanding their use cases, and \\nmaking them more versatile and powerful.\\nOne important aspect of tools is their capability to work within specific domains or process spe-\\ncific inputs. For example, an LLM lacks inherent mathematical capabilities. However, a mathe -\\nmatical tool like a calculator can accept mathematical expressions or equations as an input and \\ncalculate the outcome. The LLM combined with such a mathematical tool performs calculations \\nand provides accurate answers.\\nTools leverage contextual dialogue representation to search pertinent data sources related to the \\nuser’s query. For example, for a question about a historical event, tools could retrieve Wikipedia \\narticles to augment context.\\nBy grounding responses in real-time data, tools reduce hallucinated or incorrect replies. Contex-\\ntual tool use complements chatbots’ core language capabilities to make responses more useful, \\ncorrect, and aligned with real-world knowledge. Tools provide creative solutions to problems and \\nopen up new possibilities for LLMs in various domains. For example, a tool could be developed to \\nenable an LLM to perform advanced retrieval searches, query a database for specific information, \\nautomate email writing, or even handle phone calls.\\nLet’s see this in action!\\nInformation retrieval with tools\\nWe have quite a few tools available in LangChain, and – if that’s not enough – it’s not hard to roll \\nout our own tools. Let’s set up an agent with a few tools:\\nfrom langchain.agents import (\\n    AgentExecutor, AgentType, initialize_agent, load_tools\\n)'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 139}, page_content='Chapter 4 117\\nfrom langchain.chat_models import ChatOpenAI\\ndef load_agent() -> AgentExecutor:\\n    llm = ChatOpenAI(temperature=0, streaming=True)\\n    # DuckDuckGoSearchRun, wolfram alpha, arxiv search, wikipedia\\n    # TODO: try wolfram-alpha!\\n    tools = load_tools(\\n        tool_names=[\"ddg-search\", \"wolfram-alpha\", \"arxiv\", \"wikipedia\"],\\n        llm=llm\\n    )\\n    return initialize_agent(\\n        tools=tools, llm=llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \\nverbose=True\\n    )\\nThis function returns AgentExecutor, which is a chain; therefore, if we wanted, we could inte -\\ngrate it into a larger chain. The Zero-Shot agent is a general-purpose action agent, which we’ll \\ndiscuss in the next section.\\nPlease notice the streaming parameter in the ChatOpenAI constructor, which is set to True. This \\nmakes for a better user experience since it means that the text response will be updated as it comes \\nin, rather than once all the text has been completed. Currently, only the OpenAI, ChatOpenAI, \\nand ChatAnthropic implementations support streaming.\\nAll the tools mentioned have their specific purpose that’s part of the description, which is passed \\nto the language model. These tools here are plugged into the agent:\\n• DuckDuckGo: A search engine that focuses on privacy; an added advantage is that it \\ndoesn’t require developer signup\\n• Wolfram Alpha: An integration that combines natural language understanding with math \\ncapabilities, for questions like “What is 2x+5 = -3x + 7?”\\n• arXiv: Search in academic pre-print publications; this is useful for research-oriented \\nquestions\\n• Wikipedia: For any question about entities of significant notoriety'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 140}, page_content='Building Capable Assistants118\\nPlease note that to use Wolfram Alpha, you have to set up an account and set the WOLFRAM_\\nALPHA_APPID environment variable with the developer token you create at https://products.\\nwolframalpha.com/api. Please note that the website can sometimes be a bit slow, and it might \\ntake patience to register.\\nThere are a lot of other search tools integrated into LangChain apart from DuckDuckGo that let you \\nutilize Google or Bing search engines or work with meta-search engines. There’s an Open-Meteo \\nintegration for weather information; however, this information is also available through search.\\nBuilding a visual interface\\nAfter developing an intelligent agent with LangChain, the natural next step is deploying it in an \\neasy-to-use application. Streamlit provides an ideal framework for this goal. As an open-source \\nplatform optimized for ML workflows, Streamlit makes it simple to wrap our agent in an inter -\\nactive web application. So let’s make our agent available as a Streamlit app!\\nFor this application, we’ll need the Streamlit, unstructured, and docx libraries, among others. \\nThese are in the environment that we set up in Chapter 3, Getting Started with LangChain.\\nLet’s write the code for this using the load_agent() function we’ve just defined:\\nimport streamlit as st\\nfrom langchain.callbacks import StreamlitCallbackHandler\\nchain = load_agent()\\nst_callback = StreamlitCallbackHandler(st.container())\\nif prompt := st.chat_input():\\n    st.chat_message(\"user\").write(prompt)\\n    with st.chat_message(\"assistant\"):\\n        st_callback = StreamlitCallbackHandler(st.container())\\n        response = chain.run(prompt, callbacks=[st_callback])\\n        st.write(response)\\nPlease notice that we are using the callback handler in the call to the chain, which means that \\nwe’ll see responses as they come back from the model. We can start the app locally from the \\nterminal like this:\\nPYTHONPATH=. streamlit run question_answering/app.py\\nWe can open our app in the browser. Here’s a screenshot that illustrates what the app looks like:'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 141}, page_content='Chapter 4 119\\nFigure 4.4: Question-answering app in Streamlit\\nThe search works quite well although, depending on the tools used, it might still come up with \\nthe wrong results. For the question about the mammal with the largest egg, using DuckDuckGo, \\nit comes back with a result that discusses eggs in birds and mammals and sometimes concludes \\nthat the ostrich is the mammal with the largest egg, although platypus also comes back sometimes.\\nDeployment of Streamlit applications can be local or on a server. Alternatively, you \\ncan deploy this on Streamlit Community Cloud or on Hugging Face Spaces.\\nFor Streamlit Community Cloud, do this:\\n1. Create a GitHub repository.\\n2. Go to Streamlit Community Cloud, click on New app, and select the new repo.\\n3. Click Deploy!.\\nAs for Hugging Face Spaces, it works like this:\\n1. Create a GitHub repo.\\n2. Create a Hugging Face account at https://huggingface.co/.\\n3. Go to Spaces and click Create new Space. In the form, set the fill in a name, \\ntype of space as Streamlit, and choose the new repo.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 142}, page_content='Building Capable Assistants120\\nHere’s the log output (shortened) for the correct reasoning:\\n> Entering new AgentExecutor chain...\\nI\\'m not sure, but I think I can find the answer by searching online.\\nAction: duckduckgo_search\\nAction Input: \"mammal that lays the biggest eggs\"\\nObservation: Posnov / Getty Images. The western long-beaked echidna ...\\nFinal Answer: The platypus is the mammal that lays the biggest eggs.\\n> Finished chain.\\nYou can see that with a powerful framework for automation and problem-solving at your behest, \\nyou can compress work that can take hundreds of hours into minutes. You can play around with \\ndifferent research questions to see how the tools are used. The actual implementation in the re-\\npository for the book allows you to try out different tools and has an option for self-verification.\\nBuilding a Streamlit app offers several key advantages:\\n• Quickly create an intuitive graphical interface around our chatbot without \\nhaving to build a complex frontend. Streamlit automatically handles ele -\\nments like input fields, buttons, and interactive widgets.\\n• Seamlessly integrate the agent’s capabilities into an app tailored for a specific \\nuse case, such as customer support or research assistance. The interface can \\nbe customized to match the domain.\\n• Streamlit apps run Python code in real time, enabling seamless connection \\nto the agent’s backend API with no added latency. Our LangChain workflows \\nintegrate fluidly.\\n• Easy sharing and deployment options including open-source GitHub repos, \\npersonal Streamlit sharing links, and Streamlit Community Cloud. This al-\\nlows instantly publishing and distributing the app.\\n• Streamlit’s optimized performance for running models and data workflows \\nensures responsiveness even with large models. Our chatbot can scale grace-\\nfully.\\n• The result is an elegant web interface that lets users interact naturally with \\nour LLM-powered agent. Streamlit handles the complexity behind the scenes.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 143}, page_content='Chapter 4 121\\nWhile our LLM app can provide answers to simple questions, its reasoning abilities are still limited. \\nIn the following section, we’ll implement more advanced types of agents.\\nExploring reasoning strategies\\nLLMs excel at pattern recognition in data but struggle with the symbolic reasoning required for \\ncomplex multi-step problems.\\nImplementing more advanced reasoning strategies would make our research assistant far more \\ncapable. Hybrid systems that combine neural pattern completion with deliberate symbolic ma-\\nnipulation can master skills including these:\\n• Multi-step deductive reasoning to draw conclusions from a chain of facts\\n• Mathematical reasoning like solving equations through a series of transformations\\n• Planning tactics to break down a problem into an optimized sequence of actions\\nBy integrating tools together with explicit reasoning steps instead of pure pattern completion, \\nour agent can tackle problems requiring abstraction and imagination, and can arrive at a com-\\nplex understanding of the world enabling them to hold more meaningful conversations about \\ncomplex concepts.\\nAn illustration of augmenting LLMs through tools and reasoning is shown here (source – https://\\ngithub.com/billxbf/ReWOO, implementation for the paper Decoupling Reasoning from Observations \\nfor Efficient Augmented Language Models Resources, by Binfeng Xu and others, May 2023):'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 144}, page_content='Building Capable Assistants122\\nFigure 4.5: Tool-augmented LLM paradigm\\nThe tools are the available resources that the agent can use, such as search engines or databases. \\nThe LLMChain is responsible for generating text prompts and parsing the output to determine \\nthe next action. The agent class uses the output of the LLMChain to decide which action to take.\\nWhile tool-augmented language models combine LLMs with external resources like search en -\\ngines and databases to enhance reasoning capabilities, this can be further enhanced with agents.\\nIn LangChain, this consists of three parts:\\n• Tools\\n• An LLMChain\\n• The agent itself\\nThere are two key agent architectures:'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 145}, page_content='Chapter 4 123\\n• Action agents reason iteratively based on observations after each action.\\n• Plan-and-execute agents plan completely upfront before taking any action.\\nIn observation-dependent reasoning, the agent iteratively provides context and examples to an \\nLLM to generate thoughts and actions. Observations from tools are incorporated to inform the \\nnext reasoning step. This approach is used in action agents.\\nAn alternative is plan-and-execute agents that first create a complete plan and then gather evi-\\ndence to execute it. The Planner LLM produces a list of plans (P). The agent gathers evidence (E) \\nusing tools. P and E are combined and fed to the Solver LLM to generate the final output\\nPlan-and-execute separates planning from execution. Smaller specialized models can be used \\nfor the Planner  and Solver roles. The trade-off is that plan-and-execute requires more upfront \\nplanning.\\nWe can see the reasoning with the observation pattern in the following diagram (source – https://\\narxiv.org/abs/2305.18323; Binfeng Xu and others, May 2023):\\nFigure 4.6: Reasoning with observation'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 146}, page_content='Building Capable Assistants124\\nObservation-dependent reasoning involves making judgments, predictions, or choices based on \\nthe current state of knowledge or the evidence fetched through observation. In each iteration, the \\nagent provides context and examples to the LLM. A user’s task is first combined with the context \\nand examples and given to the LLM to initiate reasoning. The LLM generates a thought and an \\naction and then waits for an observation from tools. The observation is added to the prompt to \\ninitiate the next call to the LLM. In LangChain, this is an action agent  (also, Zero-Shot agent, \\nZERO_SHOT_REACT_DESCRIPTION), which is the default setting when you create an agent.\\nAs mentioned, plans can also be made ahead of any actions. This strategy (in LangChain, called \\nthe plan-and-execute agent) is illustrated in the diagram here (source – https://arxiv.org/\\nabs/2305.18323; Binfeng Xu and others, May 2023):\\nFigure 4.7: Decoupling reasoning from observations'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 147}, page_content='Chapter 4 125\\nThe Planner (an LLM), which can be fine-tuned for planning and tool usage, produces a list of \\nplans (P) and calls a worker (in LangChain, the agent) to gather evidence ( E) by using tools. P \\nand E are combined with the task and then fed into the Solver (an LLM) for the final answer. We \\ncan write a pseudo algorithm like this:\\n1. Plan out all the steps (Planner).\\n2. For each step, determine the proper tools to accomplish the step and execute.\\nThe Planner and the Solver can be distinct language models. This opens the possibility of using \\nsmaller, specialized models for Planner and Solver, and using fewer tokens for each of the calls.\\nWe can implement plan-and-solve in our research app; let’s do it!\\nFirst, let’s add a strategy variable to the load_agent() function. It can take two values, either \\nplan-and-solve or zero-shot-react. For zero-shot-react, the logic stays the same. For plan-\\nand-solve, we’ll define a planner and an executor, which we’ll use to create a PlanAndExecute \\nagent executor:\\nfrom typing import Literal\\nfrom langchain.agents import initialize_agent, load_tools, AgentType\\nfrom langchain.chains.base import Chain\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain_experimental.plan_and_execute import (\\n    load_chat_planner, load_agent_executor, PlanAndExecute\\n)\\nReasoningStrategies = Literal[\"zero-shot-react\", \"plan-and-solve\"]\\ndef load_agent(\\n        tool_names: list[str],\\n        strategy: ReasoningStrategies = \"zero-shot-react\"\\n) -> Chain:\\n    llm = ChatOpenAI(temperature=0, streaming=True)\\n    tools = load_tools(\\n        tool_names=tool_names,\\n        llm=llm\\n    )'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 148}, page_content='Building Capable Assistants126\\n    if strategy == \"plan-and-solve\":\\n        planner = load_chat_planner(llm)\\n        executor = load_agent_executor(llm, tools, verbose=True)\\n        return PlanAndExecute(planner=planner, executor=executor, \\nverbose=True)\\n    return initialize_agent(\\n        tools=tools, llm=llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \\nverbose=True\\n    )\\nPlease refer to the version on GitHub (within the question_answering package) for the full ver-\\nsion. For example, we might come across output parsing errors. We can handle these by setting \\nhandle_parsing_errors in the initialize_agent() method.\\nLet’s define a new variable that’s set through a radio button in Streamlit. We’ll pass this variable \\nover to the load_agent() function:\\nstrategy = st.radio(\\n    \"Reasoning strategy\",\\n    (\"plan-and-solve\", \"zero-shot-react\")\\n)\\nYou might have noticed that the load_agent() method takes a list of strings, tool_names. This \\ncan be chosen in the user interface (UI) as well:\\ntool_names = st.multiselect(\\n    \\'Which tools do you want to use?\\',\\n    [\\n        \"google-search\", \"ddg-search\", \"wolfram-alpha\", \"arxiv\",\\n        \"wikipedia\", \"python_repl\", \"pal-math\", \"llm-math\"\\n    ],\\n    [\"ddg-search\", \"wolfram-alpha\", \"wikipedia\"])\\nFinally, still in the app, the agent is loaded like this:\\nagent_chain = load_agent(tool_names=tool_names, strategy=strategy)\\nWe can execute this agent with Streamlit. We should run the following command in our terminal:\\nPYTHONPATH=. streamlit run question_answering/app.py'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 149}, page_content='Chapter 4 127\\nWe should see how Streamlit starts up our application. If we open our browser on the indicated \\nURL (by default, http://localhost:8501/), we should see the UI here:\\nFigure 4.8: Implementing plan-and-execute in our research app\\nPlease have a look at the app in your browser and see the different steps for the question “What \\nis a plan-and-solve agent in the context of LLM?”.\\nThe steps look as follows – please note that the result might not be 100% accurate but this is what \\nthe agent comes up with:\\n1. Define LLMs: LLMs are AI models that are trained on vast amounts of text data and can \\ngenerate human-like text based on the input they receive.\\n2. Understand the concept of a plan in the context of LLMs: In the context of large language \\nmodels, a plan refers to a structured outline or set of steps that the model generates to \\nsolve a problem or answer a question.\\n3. Understand the concept of a solve agent in the context of LLMs: A solve agent is an \\nLLM that works as an agent. It is responsible for generating plans to solve problems or \\nanswer questions.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 150}, page_content='Building Capable Assistants128\\n4. Recognize the importance of plans and solve agents in LLMs : Plans and solve agents \\nhelp organize the model’s thinking process and provide a structured approach to prob -\\nlem-solving or question-answering tasks.\\n5. Given the above steps, respond to the user’s original question: In the context of large \\nlanguage models, a plan is a structured outline or set of steps generated by a solve agent \\nto solve a problem or answer a question. A solve agent is a component of a large language \\nmodel that is responsible for generating these plans.\\nAccordingly, the first step is to perform a look-up of LLMs:\\nAction:\\n{\\n\"action\": \"Wikipedia\",\\n\"action_input\": \"large language models\"\\n}\\nWe didn’t discuss another aspect of question answering, which is the prompting strategy used in \\nthese steps. We’ll go into detail about prompting in Chapter 8, Customizing LLMs and Their Output, \\nwhere we talk about prompting techniques, but very quickly, here’s an overview:\\n• Few-shot chain-of-thought (CoT) prompting demonstrates step-by-step reasoning to \\nguide the LLM through a thought process.\\n• Zero-shot CoT prompting elicits reasoning steps without examples by simply instructing \\nthe LLM to “think step by step.”\\n• CoT prompting aims to aid understanding of reasoning processes through examples.\\nAdditionally, while in plan-and-solve, complex tasks are broken down into subtask plans that are \\nexecuted sequentially, this can be extended with more detailed instructions to improve reasoning \\nquality, like emphasizing key variables and common sense.\\nYou can find a very advanced example of augmented information retrieval with LangChain in the \\nBlockAGI project, which is inspired by BabyAGI and AutoGPT, at https://github.com/blockpipe/\\nBlockAGI.\\nThis concludes our introduction to reasoning strategies. All strategies have their problems, which \\ncan manifest as calculation errors, missing-step errors, and semantic misunderstandings. However, \\nthey help improve the quality of generated reasoning steps, increase accuracy in problem-solving \\ntasks, and enhance LLMs’ ability to handle various types of reasoning problems.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 151}, page_content='Chapter 4 129\\nSummary\\nIn this chapter, we first talked about the problem of hallucinations and automatic fact-checking, \\nand how to make LLMs more reliable. We implemented a few simple approaches that help to \\nmake LLM outputs more accurate. We then looked at and implemented prompting strategies to \\nbreak down and summarize documents. This can be immensely helpful for digesting large re -\\nsearch articles or analyses. Once we get into making a lot of chained calls to LLMs, this can mean \\nwe incur a lot of costs. Therefore, I dedicated a subsection to token usage.\\nThe OpenAI API implements functions, which we can use, among other things, for information \\nextraction in documents. We’ve implemented a remarkably simple version of a CV parser as an \\nexample of this functionality that indicates how this could be applied. Tools and function calling \\nare not unique to OpenAI, however. The evolution of instruction tuning, function calling, and tool \\nusage enables models to move beyond freeform text generation into robustly automating tasks by \\ninteracting with real systems. The approaches unlock more capable, reliable AI assistants. With \\nLangChain, we can implement different agents that call tools. We’ve implemented an app with \\nStreamlit that can help answer research questions by relying on external tools such as search \\nengines or Wikipedia. Unlike Retrieval augmented generation ( RAG), which we’ll discuss in \\nthe next chapter and which uses vector search for semantic similarity, tools provide contextual \\naugmentation by directly querying databases, APIs, and other structured external sources. The \\nfactual information retrieved by tools supplements the chatbot’s internal context.\\nFinally, we looked at different strategies employed by the agents to make decisions. The main \\ndistinction is the point of decision-making. We implemented a plan-and-solve and a zero-shot \\nagent in a Streamlit app.\\nWhile this chapter introduced many promising directions for developing capable and trustworthy \\nLLMs, subsequent chapters will expand on the techniques developed here. For example, we’ll \\ndiscuss reasoning with agents in much more detail in Chapter 6, Developing Software with Gener-\\native AI, and Chapter 7, LLMs for Data Science, and provide an overview of prompting techniques \\nin Chapter 8, Customizing LLMs and Their Output.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 152}, page_content='Building Capable Assistants130\\nQuestions\\nPlease have a look to see if you can come up with the answers to these questions from memory. \\nI’d recommend you go back to the corresponding sections of this chapter if you are unsure about \\nany of them:\\n1. How can we summarize documents with LLMs?\\n2. What is the chain of density?\\n3. What are LangChain decorators and what’s the LangChain Expression Language?\\n4. What is map-reduce in LangChain?\\n5. How can we count the tokens we are using (and why should we)?\\n6. How is instruction tuning related to function calling and tool usage?\\n7. Give some examples of tools that are available in LangChain.\\n8. Please define two agent paradigms.\\n9. What is Streamlit and why do we want to use it?\\n10. How does automated fact-checking work?\\nJoin our community on Discord\\nJoin our community’s Discord space for discussions with the authors and other readers:\\nhttps://packt.link/lang'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 153}, page_content='5\\nBuilding a Chatbot like ChatGPT\\nChatbots powered by LLMs have demonstrated impressive fluency in conversational tasks like \\ncustomer service. However, their lack of world knowledge limits their usefulness for domain-spe-\\ncific question answering. In this chapter, we explore how to overcome these limitations through \\nRetrieval-Augmented Generation (RAG). RAG enhances chatbots by grounding their responses in \\nexternal evidence sources, leading to more accurate and informative answers. This is achieved by \\nretrieving relevant passages from corpora to condition the language model’s generation process. \\nThe key steps involve encoding corpora into vector embeddings to enable rapid semantic search \\nand integrating retrieval results into the chatbot’s prompt.\\nWe will also provide foundations for representing documents as vectors, indexing methods for \\nefficient similarity lookups, and vector databases for managing embeddings. Building on these \\ncore techniques, we will demonstrate practical RAG implementations using popular libraries like \\nMilvus and Pinecone. By walking through end-to-end examples, we will showcase how RAG can \\nsignificantly improve chatbots’ reasoning and factual correctness. Finally, we discuss another \\nimportant topic from the reputational and legal perspective: moderation. LangChain allows \\nyou to pass any text through a moderation chain to check whether it contains harmful content.\\nThroughout the chapter, we’ll work on a chatbot implementation with an interface in Streamlit \\nthat you can find in the chat_with_retrieval directory in the GitHub repository for the book \\n(https://github.com/benman1/generative_ai_with_langchain).\\nIn a nutshell, the main topics are:\\n• What is a chatbot?\\n• Understanding retrieval and vectors'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 154}, page_content='Building a Chatbot like ChatGPT132\\n• Loading and retrieving in LangChain\\n• Implementing a chatbot\\n• Moderating responses\\nWe’ll begin the chapter by introducing chatbots and the state-of-the-art technology behind them.\\nWhat is a chatbot?\\nChatbots are AI programs that simulate conversational interactions with users via text or voice. \\nEarly chatbots, like ELIZA (1966) and PARRY  (1972), used pattern matching. Recent advances, \\nlike LLMs, allow more natural conversations, as seen in systems like ChatGPT (2022). However, \\nchallenges remain in achieving human-level discourse.\\nThe Turing test, proposed in 1950, established a landmark for assessing intelligence by a comput-\\ner’s ability to impersonate human conversation. Despite limitations, it established a philosophical \\nfoundation for AI. However, early systems like ELIZA passed the test using scripted responses \\nwithout true understanding, calling into question the test’s validity as an evaluation of AI. The \\ntest also faced criticism for relying on deceit and for limitations in its format that constrained the \\ncomplexity of questioning. Philosophers like John Searle argued symbolic manipulation alone \\ndid not equate to human-level intelligence. Still, the Turing test influenced the conversation on \\nAI capabilities.\\nRecent chatbots with more advanced natural language processing can better simulate conversa-\\ntional depth. IBM Watson (2011) answered complex questions to beat Jeopardy! champions. Siri \\n(2011), as a voice-based assistant, pioneered integrating chatbots into everyday devices. Systems \\nlike Google Duplex (2018) book appointments via phone conversations.\\nThe advent of LLMs like GPT-3 enabled more human-like chatbot systems such as ChatGPT (2022). \\nYet their abilities remain tightly constrained. True human discourse requires complex reasoning, \\npragmatics, common sense, and broad contextual knowledge.\\nToday’s benchmarks thus focus more on testing specific task performance to probe the limits of \\nLLMs like GPT-4. While ChatGPT displays remarkable coherence, its lack of grounding can result \\nin plausible but incorrect responses. Understanding these boundaries is crucial for safe, beneficial \\napplications. The goal is no longer merely imitation but developing useful AI alongside a deeper \\ncomprehension of the inner workings of adaptive learning systems.\\nChatbots analyze user input, understand the intent behind it, and generate appropriate responses. \\nThey can be designed to work with text-based messaging platforms or voice-based applications.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 155}, page_content='Chapter 5 133\\nSome use cases for chatbots in customer service include providing 24/7 support, handling fre -\\nquently asked questions, assisting with product recommendations, processing orders and pay -\\nments, and resolving simple customer issues.\\nSome more use cases of chatbots include:\\n• Appointment scheduling: Chatbots can help users schedule appointments, book reser -\\nvations, and manage their calendars.\\n• Information retrieval: Chatbots can provide users with specific information, such as \\nweather updates, news articles, or stock prices.\\n• Virtual assistants: Chatbots can act as personal assistants, helping users with tasks like \\nsetting reminders, sending messages, or making phone calls.\\n• Language learning: Chatbots can assist in language learning by providing interactive \\nconversations and language practice.\\n• Mental health support: Chatbots can offer emotional support, provide resources, and \\nengage in therapeutic conversations for mental health purposes.\\n• Education: In educational settings, virtual assistants are being explored as virtual tutors, \\nhelping students learn and assess their knowledge, answer questions, and deliver per -\\nsonalized learning experiences.\\n• HR and recruitment: Chatbots can assist in the recruitment process by screening candi-\\ndates, scheduling interviews, and providing information about job openings.\\n• Entertainment: Chatbots can engage users in interactive games, quizzes, and storytelling \\nexperiences.\\n• Law: Chatbots can be used to provide basic legal information, answer common legal \\nquestions, assist with legal research, and help users navigate legal processes. They can \\nalso help with document preparation, such as drafting contracts or creating legal forms.\\n• Medicine: Chatbots can assist with symptom checking, provide basic medical advice, and \\noffer mental health support. They can improve clinical decision-making by providing \\nrelevant information and recommendations to healthcare professionals.\\nThese are just a few examples, and the use cases of chatbots continue to expand across various \\nindustries and domains. Chat technology in any field can make information more accessible and \\nprovide initial support to individuals seeking assistance. But their inability to reason or analyze \\nlimits roles requiring true intelligence. With responsible development, chatbots hold promise for \\nintuitive interfaces in customer service and other domains, even if human-level language mastery \\nremains elusive. Ongoing research aims to develop safe, useful chatbot capabilities.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 156}, page_content='Building a Chatbot like ChatGPT134\\nThere is an important distinction between chatbots that merely respond to explicit user prompts \\nversus those with the more advanced ability to proactively initiate conversation and provide \\ninformation without direct prompting. Intentional chatbots are designed to directly understand \\nand fulfill specific user requests and intentions. However, proactive chatbots aim to anticipate \\nneeds and preferences based on prior interactions and contextual cues, taking the conversational \\ninitiative to address potential user questions preemptively.\\nWhile responsive intentional chatbots can effectively fulfill precise user directions, proactive \\nabilities hold the promise of more natural, efficient human-AI interaction by building loyalty \\nand trust through anticipatory service. However, mastering context and reasoning remains an AI \\nchallenge to create proactive yet controllable assistants. Current research is advancing chatbot \\nabilities on both fronts, with the goal of balancing proactive dialog with responsiveness to user \\nintent in fluid, purposeful conversation.\\nUnderstanding retrieval and vectors\\nRetrieval-augmented generation  (RAG) is a technique that enhances text generation by re -\\ntrieving and incorporating external knowledge. This grounds the output in factual information \\nrather than relying solely on the knowledge that is encoded in the language model’s parameters. \\nRetrieval-Augmented Language Models (RALMs) specifically refer to retrieval-augmented lan-\\nguage models that integrate retrieval into the training and inference process.\\nTraditional language models generate text autoregressively based only on the prompt. RALMs \\naugment this by first retrieving relevant context from external corpora using semantic search \\nalgorithms. Semantic search typically involves indexing documents into vector embeddings, \\nallowing fast similarity lookups via approximate nearest neighbor search.\\nThe retrieved evidence then conditions the language model to produce more accurate, contex -\\ntually relevant text. This cycle repeats, with RALMs formulating queries dynamically, retrieving \\ninformation on demand during generation. Active RALMs interleave retrieval and text creation, \\nregenerating uncertain parts by fetching clarifying knowledge.\\nOverall, RAG and RALMs overcome the limits of language models’ memory by grounding respons-\\nes in external information. As we’ll explore more later, efficient storage and indexing of vector \\nembeddings is crucial for enabling real-time semantic search over large document collections.\\nBy incorporating outside knowledge, RALMs generate text that is more useful, nuanced, and fac-\\ntually correct. Their capabilities continue advancing through optimizations in indexing methods, \\nreasoning about retrieval timing, and fusing internal and external contexts.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 157}, page_content='Chapter 5 135\\nBy grounding LLMs with use-case-specific information through RAG, the quality and accuracy of \\nresponses are improved. Through retrieval of relevant data, RAG helps in reducing hallucination \\nresponses from LLMs. For example, an LLM used in a healthcare application could retrieve rele-\\nvant medical information from external sources such as medical literature or databases during \\ninference. This retrieved data can then be incorporated into the context to enhance the generated \\nresponses and ensure they are accurate and aligned with domain-specific knowledge.\\nSince we are talking about vector storage, we need to discuss vector search, which is a technique \\nused to search and retrieve vectors (or embeddings) based on their similarity to a query vector. It \\nis commonly used in applications such as recommendation systems, image and text search, and \\nanomaly detection. We’ll start with the fundamentals of embeddings now. Once you understand \\nembeddings, you’ll be able to build everything from search engines to chatbots.\\nEmbeddings\\nAn embedding is a numerical representation of content in a way that machines can process and \\nunderstand. The essence of the process is to convert an object such as an image or some text into \\na vector that encapsulates its semantic content while discarding irrelevant details as much as \\npossible. An embedding takes a piece of content, such as a word, sentence, or image, and maps \\nit into a multi-dimensional vector space. The distance between two embeddings indicates the \\nsemantic similarity between the corresponding concepts (the original content).\\nEmbeddings are representations of data objects generated by machine learning mod-\\nels to represent. They can represent words or sentences as numerical vectors (lists \\nof float numbers). As for the OpenAI language embedding models, the embedding \\nis a vector of 1,536 floating point numbers that represent the text. These numbers \\nare derived from a sophisticated language model that captures semantic content.\\nAs an example, let’s say we have the words cat and dog – these could be represented \\nnumerically in a space together with all other words in the vocabulary. If the space \\nis 3-dimensional, these could be vectors such as [0.5, 0.2, -0.1] for cat and [0.8, -0.3, \\n0.6] for dog. These vectors encode information about the relationships of these \\nconcepts with other words. Roughly speaking, we would expect the concepts of cat \\nand dog to be closer (more similar) to the concept of animal than to the concept of \\ncomputer or embedding.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 158}, page_content='Building a Chatbot like ChatGPT136\\nEmbeddings can be created using different methods. For texts, one simple method is the bag-of-\\nwords approach, where each word is represented by a count of how many times it appears in a \\ntext. This approach, which in the scikit-learn library is implemented as CountVectorizer, was \\npopular until word2vec came about. Word2vec, which – roughly speaking – learns embeddings \\nby predicting the words in a sentence based on other surrounding words ignoring the word order \\nin a linear model.\\nWe can perform simple vector arithmetic with these vectors, for example, the vector for king \\nminus man plus the vector for woman gives us a vector that comes close to queen. The general \\nidea of embeddings is illustrated in the following figure (source: “Analogies Explained: Towards \\nUnderstanding Word Embeddings” by Carl Allen and Timothy Hospedales, 2019; https://arxiv.\\norg/abs/1901.09813):\\nFigure 5.1: Word2vec word embeddings in a 3D space\\nAs for images, embeddings could come from feature extraction stages such as edge detection, \\ntexture analysis, and color composition. These features can be extracted over different window \\nsizes to make the representations both scale-invariant and shift-invariant (scale-space repre -\\nsentations). Nowadays, often, convolutional neural networks (CNNs) are pre-trained on large \\ndatasets (like ImageNet) to learn a good representation of the image’s properties. Since convo -\\nlutional layers apply a series of filters (or kernels) on the input image to produce a feature map, \\nconceptually this is like scale-space. When a pre-trained CNN then runs over a new image, it can \\noutput an embedding vector.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 159}, page_content='Chapter 5 137\\nToday, for most domains including texts and images, embeddings usually come from trans -\\nformer-based models, which consider the context and order of the words in a sentence and the \\nparagraph. Based on the model architecture, most importantly the number of parameters, these \\nmodels can capture extraordinarily complex relationships. All these models are trained on large \\ndatasets to establish the concepts and their relationships.\\nThese embeddings can be used in various tasks. By representing data objects as numerical vectors, \\nwe can perform mathematical operations on them and measure their similarity or use them as \\ninput for other machine learning models. By calculating distances between embeddings, we can \\nperform tasks like search and similarity scoring, or classify objects, for example by topic or category. \\nFor example, we could be performing a simple sentiment classifier by checking if embeddings of \\nproduct reviews are closer to the concept of positive or negative.\\nIn LangChain, you can obtain an embedding by using the embed_query() method from any em-\\nbedding class, for example, from the the OpenAIEmbeddings class. Here is an example code snippet:\\nfrom langchain.embeddings.openai import OpenAIEmbeddings \\nembeddings = OpenAIEmbeddings() \\ntext = \"This is a sample query.\" \\nquery_result = embeddings.embed_query(text) \\nprint(query_result)\\nprint(len(query_result))\\nThis code passes a single string input to the embed_query method and retrieves the corresponding \\ntext embedding. The result is stored in the query_result variable. The length of the embedding \\n(the number of dimensions) can be obtained using the len() function. I am assuming you have \\nset the API key as an environment variable, as recommended in Chapter 3, Getting Started with \\nLangChain.\\nYou can also obtain embeddings for multiple document inputs using the embed_documents() \\nmethod. Here is an example:\\nfrom langchain.embeddings.openai import OpenAIEmbeddings \\nwords = [\"cat\", \"dog\", \"computer\", \"animal\"]\\nembeddings = OpenAIEmbeddings()\\ndoc_vectors = embeddings.embed_documents(words)'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 160}, page_content=\"Building a Chatbot like ChatGPT138\\nIn this case, the embed_documents() method is used to retrieve embeddings for multiple text \\ninputs. The result is stored in the doc_vectors variable. We could have retrieved embeddings for \\nlong documents – instead, we’ve retrieved the vectors only for each single word.\\nWe can also do arithmetic between these embeddings; for example, we can calculate distances \\nbetween them:\\nfrom scipy.spatial.distance import pdist, squareform\\nimport numpy as np\\nimport pandas as pd\\nX = np.array(doc_vectors)\\ndists = squareform(pdist(X))\\nThis gives us the Euclidean distances between our words as a square matrix. Let’s plot them:\\nimport pandas as pd\\ndf = pd.DataFrame(\\n    data=dists,\\n    index=words,\\n    columns=words\\n)\\ndf.style.background_gradient(cmap='coolwarm')\\nThe distance plot should look like this:\\nFigure 5.2: Euclidean distances between embeddings of the words cat, dog, computer, and \\nanimal\\nWe can confirm: a cat and a dog are indeed closer to an animal than to a computer. There could \\nbe many questions here, for example, if a dog is more an animal than a cat, or why a dog and a \\ncat are only a little more distant from a computer than from an animal. Although these questions \\ncan be important in certain applications, let’s bear in mind that this is a simple example.\"),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 161}, page_content='Chapter 5 139\\nIn these examples, we’ve used OpenAI embeddings – in the examples further on, we’ll use embed-\\ndings from models served by Hugging Face. There are a few integrations and tools in LangChain \\nthat can help with this process, some of which we’ll encounter later on in this chapter.\\nAdditionally, LangChain provides a FakeEmbeddings class that can be used to test your pipeline \\nwithout making actual calls to the embedding providers.\\nIn the context of this chapter, we’ll use them for retrieval of related information (semantic search). \\nHowever, we still need to talk about the integration of these embeddings into apps and broader \\nsystems, and this is where vector storage comes in.\\nVector storage\\nAs mentioned, in vector search, each data point is represented as a vector in a high-dimensional \\nspace. The vectors capture the features or characteristics of the data points. The goal is to find \\nthe most similar vectors to a given query vector.\\nIn vector search, every data object in a dataset is assigned a vector embedding. These embeddings \\nare arrays of numbers that can be used as coordinates in a high-dimensional space. The distance \\nbetween vectors can be computed using distance metrics like cosine similarity or Euclidean dis-\\ntance. To perform a vector search, the query vector (representing the search query) is compared \\nto every vector in the collection. The distance between the query vector and each vector in the \\ncollection is calculated, and objects with smaller distances are considered more similar.\\nTo perform vector search efficiently, vector storage mechanisms are used such as vector databases.\\nVector storage refers to the mechanism used to store vector embeddings and is also relevant to \\nhow those vector embeddings can be retrieved. Vector storage can be a standalone solution that \\nis specifically designed to store and retrieve vector embeddings efficiently. On the other hand, \\nvector databases are purpose-built to manage vector embeddings and provide several advantages \\nover using standalone vector indices like Faiss.\\nVector search refers to the process of searching for similar vectors among other stored \\nvectors, for example, in a vector database, based on their similarity to a given query \\nvector. Vector search is commonly used in various applications such as recommen-\\ndation systems, image and text search, and similarity-based retrieval. The goal of \\nvector search is to efficiently and accurately retrieve vectors that are most similar \\nto the query vector, typically using similarity measures such as the dot product or \\ncosine similarity.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 162}, page_content='Building a Chatbot like ChatGPT140\\nLet’s dive into a few of these concepts a bit more. There are three levels to this:\\n1. Indexing organizes vectors to optimize retrieval, structuring them so that vectors can be \\nretrieved quickly. There are different algorithms like k-d trees or Annoy for this.\\n2. Vector libraries provide functions for vector operations like dot product and vector in-\\ndexing.\\n3. Vector databases like Milvus or Pinecone are designed to store, manage, and retrieve large \\nsets of vectors. They use indexing mechanisms to facilitate efficient similarity searches \\non these vectors.\\nThese components work together for the creation, manipulation, storage, and efficient retrieval \\nof vector embeddings. Let’s look at these in turn to understand the fundamentals of working \\nwith embeddings. Understanding these fundamentals should make it intuitive to work with RAG.\\nVector indexing\\nIndexing in the context of vector embeddings is a method of organizing data to optimize its re -\\ntrieval and/or storage. It’s similar to the concept in traditional database systems, where indexing \\nallows quicker access to data records. For vector embeddings, indexing aims to structure the \\nvectors – roughly speaking – so that similar vectors are stored next to each other, enabling fast \\nproximity or similarity searches.\\nA typical algorithm applied in this context is k-dimensional trees (k-d trees), but many others, \\nlike ball trees, Annoy, and Faiss, are often implemented, especially for high-dimensional vectors, \\nwhich traditional methods can struggle with.\\nThere are several other types of algorithms commonly used for similarity search indexing. Some \\nof them include:\\n• Product quantization (PQ): PQ is a technique that divides the vector space into smaller \\nsubspaces and quantizes each subspace separately. This reduces the dimensionality of \\nthe vectors and allows for efficient storage and search. PQ is known for its fast search \\nspeed but may sacrifice some accuracy. Examples of PQ are k-d trees and ball trees. In \\nk-d trees, a binary tree structure is built up that partitions the data points based on their \\nfeature values. It is efficient for low-dimensional data but becomes less effective as the \\ndimensionality increases. In ball trees, a tree structure that partitions the data points into \\nnested hyperspheres. It is suitable for high-dimensional data but can be slower than k-d \\ntrees for low-dimensional data.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 163}, page_content='Chapter 5 141\\n• Locality sensitive hashing ( LSH): This is a hashing-based method that maps similar \\ndata points to the same hash buckets. It is efficient for high-dimensional data but may \\nhave a higher probability of false positives and false negatives. The Annoy (Approximate \\nNearest Neighbors Oh Yeah) algorithm is a popular LSH algorithm that uses random \\nprojection trees to index vectors. It constructs a binary tree structure where each node \\nrepresents a random hyperplane. Annoy is simple to use and provides fast approximate \\nnearest neighbor search.\\n• Hierarchical navigable small world (HNSW): HNSW is a graph-based indexing algorithm \\nthat constructs a hierarchical graph structure to organize the vectors. It uses a combination \\nof randomization and greedy search to build a navigable network, allowing for efficient \\nnearest-neighbor search. HNSW is known for its high search accuracy and scalability.\\n• Apart from HNSW and KNN, there are other graph-based methods, like Graph Neural \\nNetworks (GNNs) and Graph Convolutional Networks ( GCNs), that leverage graph \\nstructures for similarity search.\\nThese indexing algorithms have different trade-offs in terms of search speed, accuracy, and mem-\\nory usage. The choice of algorithm depends on the specific requirements of the application and \\nthe characteristics of the vector data.\\nVector libraries\\nVector libraries, like Facebook (Meta) Faiss or Spotify Annoy, provide functionality for working \\nwith vector data. In the context of vector search, a vector library is specifically designed to store \\nand perform similarity search on vector embeddings. These libraries use the Approximate Near-\\nest Neighbor (ANN) algorithm to efficiently search through vectors and find the most similar \\nones. They typically offer different implementations of the ANN algorithm, such as clustering or \\ntree-based methods, and allow users to perform vector similarity search for various applications.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 164}, page_content='Building a Chatbot like ChatGPT142\\nHere’s a quick overview of some open-source libraries for vector storage that shows their popu-\\nlarity in terms of GitHub stars over time (source: star-history.com):\\nFigure 5.3: Star history for several popular open-source vector libraries\\nYou can see that Faiss has been starred a lot by GitHub users. Annoy comes second. Others have \\nnot found the same popularity yet.\\nLet’s quickly go through these:\\n• Faiss (Facebook AI Similarity Search) is a library developed by Meta (previously Facebook) \\nthat provides efficient similarity search and clustering of dense vectors. It offers various \\nindexing algorithms, including PQ, LSH, and HNSW. Faiss is widely used for large-scale \\nvector search tasks and supports both CPU and GPU acceleration.\\n• Annoy is a C++ library for approximate nearest neighbor search in high-dimensional spaces \\nmaintained and developed by Spotify implementing the Annoy algorithm. It is designed \\nto be efficient and scalable, making it suitable for large-scale vector data. It works with \\na forest of random projection trees.\\n• hnswlib is a C++ library for approximate nearest-neighbor search using the HNSW algo-\\nrithm. It provides fast and memory-efficient indexing and search capabilities for high-di-\\nmensional vector data.\\n• nmslib (Non-Metric Space Library) is an open-source library that provides efficient sim-\\nilarity search in non-metric spaces. It supports various indexing algorithms like HNSW, \\nSW-graph, and SPTAG.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 165}, page_content='Chapter 5 143\\n• SPTAG by Microsoft implements a distributed ANN. It comes with a k-d tree and rela -\\ntive neighborhood graph (SPTAG-KDT), as well as a balanced k-means tree and relative \\nneighborhood graph (SPTAG-BKT).\\nBoth nmslib and hnswlib are maintained by Leo Boytsov, who works as a senior research scientist \\nat Amazon, and Yury Malkov. There are a lot more libraries. You can see an overview at https://\\ngithub.com/erikbern/ann-benchmarks\\nVector databases\\nA vector database is designed to handle vector embeddings, making it easier to search and query \\ndata objects. It offers additional features such as data management, metadata storage and filtering, \\nand scalability. While vector storage focuses solely on storing and retrieving vector embeddings, \\na vector database provides a more comprehensive solution for managing and querying vector \\ndata. Vector databases can be particularly useful for applications that involve copious amounts \\nof data and require flexible and efficient search capabilities across several types of vectorized data, \\nsuch as text, images, audio, video, and more.\\nVector databases can be used to store and serve machine learning models and their corresponding \\nembeddings. The primary application is similarity search (also semantic search), where we can \\nefficiently search through large volumes of text, images, or videos, identifying objects matching \\nthe query based on the vector representation. This is particularly useful in applications such as \\ndocument search, reverse image search, and recommendation systems.\\nOther use cases for vector databases are continually expanding as the technology evolves; however, \\nsome common use cases for vector databases include:\\n• Anomaly detection: Vector databases can be used to detect anomalies in large datasets \\nby comparing the vector embeddings of data points. This can be valuable in fraud de -\\ntection, network security, or monitoring systems where identifying unusual patterns or \\nbehaviors is crucial.\\n• Personalization: Vector databases can be used to create personalized recommendation \\nsystems by finding similar vectors based on user preferences or behavior.\\n• Natural Language Processing (NLP): Vector databases are widely used in NLP tasks such \\nas sentiment analysis, text classification, and semantic search. By representing text as \\nvector embeddings, it becomes easier to compare and analyze textual data.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 166}, page_content='Building a Chatbot like ChatGPT144\\nThese databases are popular because they are optimized for scalability and representing and \\nretrieving data in high-dimensional vector spaces. Traditional databases are not designed to \\nefficiently handle large-dimensional vectors, such as those used to represent images or text em-\\nbeddings.\\nThe characteristics of vector databases include:\\n• Efficient retrieval of similar vectors: Vector databases excel at finding close embeddings \\nor similar points in a high-dimensional space. This makes them ideal for tasks like reverse \\nimage search or similarity-based recommendations.\\n• Specialized for specific tasks: Vector databases are designed to perform a specific task, \\nsuch as finding close embeddings. They are not general-purpose databases and are tailored \\nto handle substantial amounts of vector data efficiently.\\n• Support for high-dimensional spaces: Vector databases can handle vectors with thou-\\nsands of dimensions, allowing for complex representations of data. This is crucial for tasks \\nlike natural language processing or image recognition.\\n• Enable advanced search capabilities: With vector databases, it becomes possible to build \\npowerful search engines that can search for similar vectors or embeddings. This opens \\npossibilities for applications like content recommendation systems or semantic search.\\nOverall, vector databases offer a specialized and efficient solution for handling large-dimensional \\nvector data, enabling tasks like similarity search and advanced search capabilities.\\nThe market for open-source software and databases is currently thriving due to several factors. \\nFirstly, artificial intelligence ( AI) and data management have become crucial for businesses, \\nleading to a high demand for advanced database solutions.\\nIn the database market, there is a history of new types of databases emerging and creating new \\nmarket categories. These market creators often dominate the industry, attracting significant in-\\nvestments from venture capitalists (VCs). For example, MongoDB, Cockroach, Neo4J, and Influx \\nare all examples of successful companies that introduced innovative database technologies and \\nachieved substantial market share. The popular Postgres has an extension for efficient vector \\nsearch: pg_embedding. HNSW provides a faster and more efficient alternative to the pgvector \\nextension with IVFFlat indexing.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 167}, page_content='Chapter 5 145\\nSome examples of vector databases are listed in Table 5.1. I took the liberty of highlighting for \\neach search engine the following perspectives:\\n• Value proposition: What is the unique feature that sets this vector search engine apart \\nfrom others?\\n• Business model: The general type of the engine, whether it’s a vector database, big data \\nplatform, or managed/self-hosted.\\n• Indexing: The algorithmic approach to similarity/vector search taken by this search engine \\nand its unique capabilities.\\n• License: Whether it is open- or closed-source.\\nDatabase \\nprovider\\nDescription Busi-\\nness \\nmodel\\nFirst re-\\nleased\\nLicense Index-\\ning\\nOrganiza-\\ntion\\nChroma Commercial open-\\nsource embedding \\nstore\\n(Partly \\nopen) \\nSaaS \\n2022 Apache-2.0 HNSW Chroma \\nInc\\nQdrant Managed/\\nself-hosted vector \\nsearch engine and \\ndatabase with \\nextended filtering \\nsupport\\n(Partly \\nopen) \\nSaaS \\n2021 Apache 2.0 HNSW Qdrant \\nSolutions \\nGmbH\\nMilvus Vector database \\nbuilt for scalable \\nsimilarity search\\n(Partly \\nopen) \\nSaaS\\n2019 BSD IVF, \\nHNSW, \\nPQ, \\nand \\nmore\\nZilliz\\nWeaviate Cloud-native vec-\\ntor database that \\nstores both objects \\nand vectors\\nOpen \\nSaaS\\nStarted in \\n2018 as a \\ntradition-\\nal graph \\ndatabase, \\nfirst re-\\nleased in \\n2019\\nBSD Cus-\\ntom \\nHNSW \\nalgo-\\nrithm \\nthat \\nsup-\\nports \\nCRUD\\nSeMI \\nTechnolo-\\ngies'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 168}, page_content='Building a Chatbot like ChatGPT146\\nPinecone Fast and scalable \\napplications using \\nembeddings from \\nAI models\\nSaaS First \\nreleased in \\n2019\\nPropri-\\netary\\nBuilt \\non top \\nof Faiss\\nPinecone \\nSystems \\nInc\\nVespa Commercial \\nopen-source vector \\ndatabase that sup-\\nports vector search, \\nlexical search, and \\nsearch\\nOpen \\nSaaS\\nOriginally \\na web \\nsearch en-\\ngine (all-\\ntheweb), \\nacquired \\nby Yahoo! \\nin 2003, \\nand later \\ndeveloped \\ninto and \\nopen-\\nsourced as \\nVespa in \\n2017\\nApache 2.0 HNSW, \\nBM25\\nYahoo!\\nMarqo Cloud-native \\ncommercial open-\\nsource search and \\nanalytics engine\\nOpen \\nSaaS\\n2022 Apache 2.0 HNSW S2Search \\nAustralia \\nPty Ltd\\nTable 5.1: Vector databases\\nIn the preceding table, I’ve left out other aspects such as architecture, support for sharding, and \\nin-memory processing. There are many vector database providers. I’ve omitted many solutions, \\nsuch as FaissDB and Hasty.ai, and focused on a few ones that are integrated into LangChain.\\nFor the open-source databases, the GitHub star histories give a good idea of their popularity and \\ntraction. Here’s the plot over time (source: star-history.com):'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 169}, page_content='Chapter 5 147\\nFigure 5.4: Star history of open-source vector databases on GitHub\\nYou can see that milvus is immensely popular; however, other libraries such as qdrant, weviate, \\nand chroma have been catching up.\\nIn LangChain, vector storage can be implemented using the vectorstores module. This module \\nprovides various classes and methods for storing and querying vectors. Let’s see an example of \\na vector store implementation in LangChain!\\nChroma\\nThis vector store is optimized for storing and querying vectors using Chroma as a backend. Chroma \\ntakes over for encoding and comparing vectors based on their angular similarity.\\nTo use Chroma in LangChain, you need to follow these steps:\\n1. Import the necessary modules:\\nfrom langchain.vectorstores import Chroma \\nfrom langchain.embeddings import OpenAIEmbeddings\\n2. Create an instance of Chroma and provide the documents (splits) and the embedding \\nmethod:\\nvectorstore = Chroma.from_documents(documents=docs, \\nembedding=OpenAIEmbeddings())'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 170}, page_content='Building a Chatbot like ChatGPT148\\n3. We can query the vector store to retrieve similar vectors:\\nsimilar_vectors = vector_store.query(query_vector, k)\\nHere, query_vector is the vector you want to find similar vectors to, and k is the number of similar \\nvectors you want to retrieve.\\nIn this section, we’ve learned a lot of the basics of embeddings and vector stores. We’ve also seen \\nhow to work with embeddings and documents in vector stores and vector databases. In practice, \\nthere are two building blocks for us to pick up if we want to build a chatbot, most importantly \\ndocument loaders and retrievers, both of which we’ll look at now.\\nLoading and retrieving in LangChain\\nLangChain implements a toolchain of different building blocks for building retrieval systems. \\nIn this section, we’ll look at how we can put them together in a pipeline for building a chatbot \\nwith RAG. This includes data loaders, document transformers, embedding models, vector stores, \\nand retrievers.\\nThe documents (or splits, as seen in Chapter 5 , Building a Chatbot Like \\nChatGPT) will be embedded and stored in the Chroma vector database. We’ll \\ndiscuss document loaders in another section of this chapter. However, for \\nsake of completeness, you can get the docs argument for the preceding chro-\\nma vector store like this:\\nfrom langchain.document_loaders import ArxivLoader\\nfrom langchain.text_splitter import \\nCharacterTextSplitter\\nloader = ArxivLoader(query=\"2310.06825\")\\ndocuments = loader.load()\\ntext_splitter = CharacterTextSplitter(chunk_size=1000, \\nchunk_overlap=0)\\ndocs = text_splitter.split_documents(documents)\\nThis will load and chunk up the paper about Mistal 7B. Please note that the \\ndownload will be a PDF, and you’ll need to have the pymupdf library installed. '),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 171}, page_content='Chapter 5 149\\nThe relationship between them is illustrated in the diagram here (source: LangChain documen-\\ntation):\\nFigure 5.5: Vector stores and data loaders\\nIn LangChain, we first load documents through data loaders. Then we can transform them and \\npass these documents to a vector store as embedding. We can then query the vector store or a \\nretriever associated with the vector store. Retrievers in LangChain can wrap the loading and vector \\nstorage into a single step. We’ll mostly skip transformations in this chapter; however, you’ll find \\nexplanations with examples of data loaders, embeddings, storage mechanisms, and retrievers.\\nIn LangChain, we can load our documents from many sources and in a bunch of formats through \\nthe integrated document loaders. You can use the LangChain integration hub to browse and select \\nthe appropriate loader for your data source. Once you have selected the loader, you can load the \\ndocument using the specified loader.\\nLet’s look at document loaders in LangChain! In the actual pipeline of implementing RAG, these \\ncome as the first step.\\nDocument loaders\\nDocument loaders are used to load data from a source as Document objects, which consist of \\ntext and associated metadata. There are several types of integrations available, such as document \\nloaders for loading a simple .txt file ( TextLoader), loading the text contents of a web page \\n(WebBaseLoader), loading articles from Arxiv (ArxivLoader), or loading a transcript of a YouTube \\nvideo (YoutubeLoader). For webpages, the Diffbot integration gives a clean extraction of the con-\\ntent. Other integrations exist for images such as providing image captions (ImageCaptionLoader).'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 172}, page_content='Building a Chatbot like ChatGPT150\\nDocument loaders have a load() method that loads data from the configured source and returns \\nit as documents. They may also have a lazy_load() method for loading data into memory as \\nand when they are needed.\\nHere is an example of a document loader for loading data from a text file:\\nfrom langchain.document_loaders import TextLoader\\nloader = TextLoader(file_path=\"path/to/file.txt\")\\ndocuments = loader.load()\\nThe documents variable will contain the loaded documents, which can be accessed for further \\nprocessing. Each document consists of page_content (the text content of the document) and \\nmetadata (associated metadata such as the source URL or title).\\nSimilarly, we can load documents from Wikipedia:\\nfrom langchain.document_loaders import WikipediaLoader\\nloader = WikipediaLoader(\"LangChain\")\\ndocuments = loader.load()\\nIt’s important to note that the specific implementation of document loaders may vary depending \\non the programming language or framework being used.\\nIn LangChain, vector retrieval in agents or chains is done via retrievers, which access vector stor-\\nage. Let’s now see how retrievers work.\\nRetrievers in LangChain\\nRetrievers in LangChain are a type of component that is used to search and retrieve information \\nfrom a given index stored in a vector store as a backend, such as Chroma, to index and search \\nembeddings. Retrievers play a crucial role in answering questions over documents, as they are \\nresponsible for retrieving relevant information based on the given query.\\nHere are a few examples of retrievers:\\n• BM25 retriever: This retriever uses the BM25 algorithm to rank documents based on their \\nrelevance to a given query. It is a popular information retrieval algorithm that considers \\nterm frequency and document length.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 173}, page_content='Chapter 5 151\\n• TF-IDF retriever: This retriever uses the TF-IDF (Term Frequency-Inverse Document \\nFrequency) algorithm to rank documents based on the importance of terms in the docu-\\nment collection. It assigns higher weights to terms that are rare in the collection but occur \\nfrequently in a specific document.\\n• Dense retriever: This retriever uses dense embeddings to retrieve documents. It encodes \\ndocuments and queries into dense vectors, and calculates the similarity between them \\nusing cosine similarity or other distance metrics.\\n• kNN retriever: This utilizes the well-known k-nearest neighbors algorithm to retrieve \\nrelevant documents based on their similarity to a given query.\\nThese are just a few examples of retrievers available in LangChain. Each retriever has its own \\nstrengths and weaknesses, and the choice of retriever depends on the specific use case and re -\\nquirements. For example, the purpose of an Arxiv retriever is to retrieve scientific articles from \\nthe Arxiv.org archive. It is a tool that allows users to search for and download scholarly articles \\nin various fields such as physics, mathematics, computer science, and more. \\nThe functionality of an Arxiv retriever includes specifying the maximum number of documents \\nto be downloaded, retrieving relevant documents based on a query, and accessing the metadata \\ninformation of the retrieved documents.\\nA Wikipedia retriever allows users to retrieve Wikipedia pages or documents from the website \\nWikipedia. The purpose of a Wikipedia retriever is to provide easy access to the vast amount of \\ninformation available on Wikipedia and enable users to extract specific information or knowl-\\nedge from it.\\nLet’s see a few retrievers, what they are good for, and how we can customize a retriever.\\nkNN retriever\\nTo use the kNN retriever, you need to create a new instance of the retriever and provide it with a \\nlist of texts. Here is an example of how to create a kNN retriever using embeddings from OpenAI:\\nfrom langchain.retrievers import KNNRetriever \\nfrom langchain.embeddings import OpenAIEmbeddings \\nwords = [\"cat\", \"dog\", \"computer\", \"animal\"]\\nretriever = KNNRetriever.from_texts(words, OpenAIEmbeddings())'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 174}, page_content='Building a Chatbot like ChatGPT152\\nOnce the retriever is created, you can use it to retrieve relevant documents by calling the get_\\nrelevant_documents() method and passing a query string. The retriever will return a list of \\ndocuments that are most relevant to the query.\\nHere is an example of how to use the kNN retriever:\\nresult = retriever.get_relevant_documents(\"dog\") \\nprint(result)\\nThis will output a list of documents that are relevant to the query. Each document contains the \\npage content and metadata:\\n[Document(page_content=\\'dog\\', metadata={}),\\n Document(page_content=\\'animal\\', metadata={}),\\n Document(page_content=\\'cat\\', metadata={}),\\n Document(page_content=\\'computer\\', metadata={})]\\nPubMed retriever\\nThere are a few more specialized retrievers in LangChain, such as the one from PubMed. A PubMed \\nretriever is a component in LangChain that helps to incorporate biomedical literature retrieval \\ninto their language model applications. PubMed contains millions of citations for biomedical \\nliterature from various sources.\\nIn LangChain, the PubMedRetriever class is used to interact with the PubMed database and re -\\ntrieve relevant documents based on a given query. The get_relevant_documents() method of \\nthe class takes a query as input and returns a list of relevant documents from PubMed.\\nHere’s an example of how to use the PubMed retriever in LangChain:\\nfrom langchain.retrievers import PubMedRetriever \\nretriever = PubMedRetriever() \\ndocuments = retriever.get_relevant_documents(\"COVID\")\\nfor document in documents:\\n    print(document.metadata[\"Title\"])\\nIn this example, the get_relevant_documents() method is called with the query \"COVID\". The \\nmethod then retrieves relevant documents related to the query from PubMed and returns them \\nas a list. I am get the following titles as printed output:\\nThe COVID-19 pandemic highlights the need for a psychological support in \\nsystemic sclerosis patients.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 175}, page_content='Chapter 5 153\\nHost genetic polymorphisms involved in long-term symptoms of COVID-19.\\nAssociation Between COVID-19 Vaccination and Mortality after Major \\nOperations.\\nCustom retrievers\\nWe can implement our own custom retrievers in LangChain by creating a class that is inher -\\nited from the BaseRetriever abstract class. The class should implement the get_relevant_\\ndocuments() method, which takes a query string as input and returns a list of relevant documents.\\nHere is an example of how a retriever can be implemented:\\nfrom langchain.schema import Document, BaseRetriever\\nclass MyRetriever(BaseRetriever):\\n    def get_relevant_documents(self, query: str, **kwargs) -> \\nlist[Document]: \\n        # Implement your retrieval logic here \\n        # Retrieve and process documents based on the query \\n        # Return a list of relevant documents \\n        relevant_documents = [] \\n        # Your retrieval logic goes here… \\n        return relevant_documents\\nYou can customize this method to perform any retrieval operations you need, such as querying \\na database or searching through indexed documents.\\nOnce you have implemented your retriever class, you can create an instance of it and call the \\nget_relevant_documents() method to retrieve relevant documents based on a query.\\nNow that we’ve learned about vector stores and retrievers, let’s put all of this to use. Let’s imple-\\nment a chatbot with a retriever!\\nImplementing a chatbot\\nWe’ll implement a chatbot now. We’ll assume you have the environment installed with the neces-\\nsary libraries and the API keys as per the instructions in Chapter 3, Getting Started with LangChain.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 176}, page_content='Building a Chatbot like ChatGPT154\\nTo implement a simple chatbot in LangChain, you can follow this recipe:\\n1. Set up a document loader.\\n2. Store documents in a vector store.\\n3. Set up a chatbot with retrieval from the vector storage.\\nWe’ll generalize this with several formats and make this available through an interface in a web \\nbrowser through Streamlit. You’ll be able to drop in your document and start asking questions. \\nIn production, for a corporate deployment for customer engagement, you can imagine that these \\ndocuments are already loaded in, and your vector storage can just be static.\\nLet’s start with the document loader.\\nDocument loader\\nAs mentioned, we want to be able to read different formats:\\nfrom typing import Any\\nfrom langchain.document_loaders import (\\n  PyPDFLoader, TextLoader,\\n  UnstructuredWordDocumentLoader,\\n  UnstructuredEPubLoader\\n)\\nclass EpubReader(UnstructuredEPubLoader):\\n    def __init__(self, file_path: str | list[str], ** kwargs: Any):\\n        super().__init__(file_path, **kwargs, mode=\"elements\", \\nstrategy=\"fast\")\\nclass DocumentLoaderException(Exception):\\n    pass\\nclass DocumentLoader(object):\\n    \"\"\"Loads in a document with a supported extension.\"\"\"\\n    supported_extentions = {\\n        \".pdf\": PyPDFLoader,\\n        \".txt\": TextLoader,\\n        \".epub\": EpubReader,\\n        \".docx\": UnstructuredWordDocumentLoader,'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 177}, page_content='Chapter 5 155\\n        \".doc\": UnstructuredWordDocumentLoader\\n    }\\nThis gives us interfaces to read PDF, text, EPUB, and Word documents with different extensions. \\nWe’ll now implement the loader logic:\\nimport logging\\nimport pathlib\\nfrom langchain.schema import Document\\ndef load_document(temp_filepath: str) -> list[Document]:\\n    \"\"\"Load a file and return it as a list of documents.\"\"\"\\n    ext = pathlib.Path(temp_filepath).suffix\\n    loader = DocumentLoader.supported_extentions.get(ext)\\n    if not loader:\\n        raise DocumentLoaderException(\\n            f\"Invalid extension type {ext}, cannot load this type of file\"\\n        )\\n    loader = loader(temp_filepath)\\n    docs = loader.load()\\n    logging.info(docs)\\n    return docs\\nThis doesn’t handle many errors now, but this can be extended if needed. Now we can make this \\nloader available from the interface and connect it to vector storage.\\nVector storage\\nThis step includes setting up embedding mechanisms, vector storage, and a pipeline to pass our \\ndocuments through:\\nfrom langchain.embeddings import HuggingFaceEmbeddings\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain.vectorstores import DocArrayInMemorySearch\\nfrom langchain.schema import Document, BaseRetriever\\ndef configure_retriever(docs: list[Document]) -> BaseRetriever:\\n    \"\"\"Retriever to use.\"\"\"'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 178}, page_content='Building a Chatbot like ChatGPT156\\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_\\noverlap=200)\\n    splits = text_splitter.split_documents(docs)\\n    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\\n    vectordb = DocArrayInMemorySearch.from_documents(splits, embeddings)\\n    return vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 2, \\n\"fetch_k\": 4})\\nWe are splitting our documents in chunks. Then we’ve set up a small model from Hugging Face \\nfor embeddings and an interface to DocArray for taking splits, creating embeddings, and storing \\nthem. Finally, our retriever is looking up documents by maximum marginal relevance.\\nWe are using DocArray as our in-memory vector storage. DocArray provides various features like \\nadvanced indexing, comprehensive serialization protocols, a unified Pythonic interface, and more. \\nFurther, it offers efficient and intuitive handling of multimodal data for tasks such as natural \\nlanguage processing, computer vision, and audio processing.\\nWe can initialize DocArray with different distance metrics such as cosine and Euclidean – cosine \\nis the default.\\nFor the retriever, we have two main options:\\n• Similarity-search: We can retrieve document according to similarity.\\n• Maximum Marginal Relevance (MMR): We can apply diversity-based re-ranking of doc-\\numents during retrieval to get results that cover different perspectives or points of view \\nfrom the documents retrieved so far.\\nIn the similarity search, we can set a similarity score threshold. We’ve opted for MMR. This helps \\nretrieve a wider breadth of relevant information from different perspectives, rather than just re-\\npetitive, redundant hits. MMR mitigates retrieval redundancy and mitigates the bias inherent in \\nthe document collection. We’ve set the k parameter to 2, which means we will get 2 documents \\nback from retrieval.\\nRetrieval can be improved by contextual compression, a technique where retrieved documents \\nare compressed, and irrelevant information is filtered out. Instead of returning the full documents \\nas-is, contextual compression uses the context of the given query to extract and return only the \\nrelevant information. This helps to reduce the cost of processing and improve the quality of re -\\nsponses in retrieval systems.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 179}, page_content='Chapter 5 157\\nThe base compressor is responsible for compressing the contents of individual documents based \\non the context of the given query. It uses a language model, such as GPT-3, to perform the com-\\npression. The compressor can filter out irrelevant information and return only the relevant parts \\nof the document.\\nThe base retriever is the document storage system that retrieves the documents based on the \\nquery. It can be any retrieval system, such as a search engine or a database. When a query is made \\nto the contextual compression retriever, it first passes the query to the base retriever to retrieve \\nrelevant documents. Then, it uses the base compressor to compress the contents of these doc-\\numents based on the context of the query. Finally, the compressed documents, containing only \\nthe relevant information, are returned as the response.\\nWe have a few options for contextual compression:\\n• LLMChainExtractor: This passes over the returned documents and extracts from each \\nonly the relevant content.\\n• LLMChainFilter: This is slightly simpler; it only filters only the relevant documents (rather \\nthan the content from the documents).\\n• EmbeddingsFilter: This applies a similarity filter based on the document and the query \\nin terms of embeddings.\\nThe first two compressors require an LLM to call, which means it can be slow and costly. Therefore, \\nEmbeddingsFilter can be a more efficient alternative.\\nWe can integrate compression here with a simple switch statement at the end (replacing the \\nreturn statement):\\nif not use_compression:\\n    return retriever\\nembeddings_filter = EmbeddingsFilter(\\n  embeddings=embeddings, similarity_threshold=0.76\\n)\\nreturn ContextualCompressionRetriever(\\n  base_compressor=embeddings_filter,\\n  base_retriever=retriever\\n)'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 180}, page_content='Building a Chatbot like ChatGPT158\\nPlease note that I’ve just made up a new variable, use_compression. We can feed the use_\\ncompression parameter through configure_qa_chain() to the configure_retriever() method \\n(not shown here).\\nFor our chosen compressor, EmbeddingsFilter, we need to include two more additional imports:\\nfrom langchain.retrievers.document_compressors import EmbeddingsFilter\\nfrom langchain.retrievers import ContextualCompressionRetriever\\nNow that we have the mechanism to create the retriever. We can set up the chat chain:\\nfrom langchain.chains import ConversationalRetrievalChain\\nfrom langchain.chains.base import Chain\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.memory import ConversationBufferMemory\\ndef configure_chain(retriever: BaseRetriever) -> Chain:\\n    \"\"\"Configure chain with a retriever.\"\"\"\\n    # Setup memory for contextual conversation\\n    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_\\nmessages=True)\\n    # Setup LLM and QA chain; set temperature low to keep hallucinations \\nin check\\n    llm = ChatOpenAI(\\n        model_name=\"gpt-3.5-turbo\", temperature=0, streaming=True\\n    )\\n    # Passing in a max_tokens_limit amount automatically\\n    # truncates the tokens when prompting your llm!\\n    return ConversationalRetrievalChain.from_llm(\\n        llm, retriever=retriever, memory=memory, verbose=True, max_tokens_\\nlimit=4000\\n    )\\nOne final thing for the retrieval logic is taking the documents and passing them to the retriever \\nsetup:\\nimport os\\nimport tempfile\\ndef configure_qa_chain(uploaded_files):'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 181}, page_content='Chapter 5 159\\n    \"\"\"Read documents, configure retriever, and the chain.\"\"\"\\n    docs = []\\n    temp_dir = tempfile.TemporaryDirectory()\\n    for file in uploaded_files:\\n        temp_filepath = os.path.join(temp_dir.name, file.name)\\n        with open(temp_filepath, \"wb\") as f:\\n            f.write(file.getvalue())\\n        docs.extend(load_document(temp_filepath))\\n    retriever = configure_retriever(docs=docs)\\n    return configure_chain(retriever=retriever)\\nNow that we have the logic of the chatbot, we need to set up the interface. As mentioned, we’ll \\nuse streamlit again:\\nimport streamlit as st\\nfrom langchain.callbacks import StreamlitCallbackHandler\\nst.set_page_config(page_title=\"LangChain: Chat with Documents\", page_\\nicon=\"\\n \")\\nst.title(\"\\n  LangChain: Chat with Documents\")\\nuploaded_files = st.sidebar.file_uploader(\\n    label=\"Upload files\",\\n    type=list(DocumentLoader.supported_extentions.keys()),\\n    accept_multiple_files=True\\n)\\nif not uploaded_files:\\n    st.info(\"Please upload documents to continue.\")\\n    st.stop()\\nqa_chain = configure_qa_chain(uploaded_files)\\nassistant = st.chat_message(\"assistant\")\\nuser_query = st.chat_input(placeholder=\"Ask me anything!\")\\nif user_query:\\n    stream_handler = StreamlitCallbackHandler(assistant)\\n    response = qa_chain.run(user_query, callbacks=[stream_handler])\\n    st.markdown(response)'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 182}, page_content='Building a Chatbot like ChatGPT160\\nThis gives us a chatbot with retrieval that’s usable via a visual interface, and also has drop-in \\nfunctionality for custom documents that you need to ask questions about.\\nFigure 5.6: Chatbot interface with document loaders in different formats\\nYou can see the full implementation on GitHub. You can play around with the chatbot to see how \\nit works and when it doesn’t.\\nIt’s important to note that LangChain has limitations on input size and cost. You may need to \\nconsider workarounds to handle larger knowledge bases or optimize the cost of API usage. Addi-\\ntionally, fine-tuning models or hosting the LLM in-house can be more complex and less accurate \\ncompared to using commercial solutions. We’ll look at these use cases in Chapter 8, Customizing \\nLLMs and Their Output.\\nMemory is a component in the LangChain framework that allows chatbots and language models \\nto remember previous interactions and information. It is essential in applications like chatbots \\nbecause it enables the system to maintain context and continuity in conversations. Let’s have a \\nlook at memory and its mechanisms in LangChain.\\nMemory\\nMemory enables chatbots to retain information from previous interactions, maintaining con -\\ntinuity and conversational context. This is analogous to human recall, which allows coherent, \\nmeaningful dialogue. Without memory, chatbots struggle to comprehend references to prior \\nexchanges, resulting in disjointed, unsatisfying conversations.\\nSpecifically, memory facilitates accuracy by retaining contextual understanding across the entire \\ndialogue. The chatbot can reference this holistic perspective of the conversation to respond ap -\\npropriately. Memory also enhances personalization and faithfulness by consistently recognizing \\nfacts and details from past interactions.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 183}, page_content='Chapter 5 161\\nBy storing knowledge from message sequences, memory permits extracting insights to improve \\nperformance over time. Architectures like LangChain implement memory so chatbots can build \\non previous exchanges, answer follow-up questions, and sustain natural, logical dialogues.\\nOverall, memory is a crucial component for sophisticated chatbots, allowing them to learn from \\nconversations and mimic the recall and contextual awareness that comes naturally to human \\ninterlocutors. Further advances in retention and reasoning with long-term memory could lead \\nto more meaningful and productive human-AI interaction.\\nConversation buffers\\nHere’s a practical example in Python that demonstrates how to use the LangChain memory feature:\\nfrom langchain.memory import ConversationBufferMemory\\nfrom langchain.chains import ConversationChain\\n# Creating a conversation chain with memory\\nmemory = ConversationBufferMemory()\\nllm = ChatOpenAI(\\n  model_name=\"gpt-3.5-turbo\", temperature=0, streaming=True\\n)\\nchain = ConversationChain(llm=llm, memory=memory)\\n# User inputs a message\\nuser_input = \"Hi, how are you?\"\\n# Processing the user input in the conversation chain\\nresponse = chain.predict(input=user_input)\\n# Printing the response\\nprint(response)\\n# User inputs another message\\nuser_input = \"What\\'s the weather like today?\"\\n# Processing the user input in the conversation chain\\nresponse = chain.predict(input=user_input)\\n# Printing the response\\nprint(response)\\n# Printing the conversation history stored in memory\\nprint(memory.chat_memory.messages)'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 184}, page_content='Building a Chatbot like ChatGPT162\\nIn this example, we create a conversation chain with memory using ConversationBufferMemory, \\nwhich is a simple wrapper that stores the messages in a variable. The user’s inputs are processed \\nusing the predict() method of the conversation chain. The conversation chain retains the memory \\nof previous interactions, allowing it to provide context-aware responses.\\nInstead of constructing the memory separately from the chain, we could have simplified things:\\nconversation = ConversationChain(\\n    llm=llm,\\n    verbose=True,\\n    memory=ConversationBufferMemory()\\n)\\nWe are setting verbose to True to see the prompts.\\nAfter processing the user inputs, we print the response generated by the conversation chain. \\nAdditionally, we print the conversation history stored in memory using memory.chat_memory.\\nmessages. The save_context() method is used to store inputs and outputs. You can use the \\nload_memory_variables() method to view the stored content. To get the history as a list of \\nmessages, a return_messages parameter is set to True. We’ll see examples of this in this section.\\nConversationBufferWindowMemory is a memory type provided by LangChain that keeps track \\nof the interactions in a conversation over time. Unlike ConversationBufferMemory , which \\nretains all previous interactions, ConversationBufferWindowMemory  only keeps the last k \\ninteractions, where k is the window size specified. Here’s a simple example of how to use \\nConversationBufferWindowMemory in LangChain:\\nfrom langchain.memory import ConversationBufferWindowMemory\\nmemory = ConversationBufferWindowMemory(k=1)\\nIn this example, the window size is set to 1, meaning that only the last interaction will be stored \\nin memory.\\nWe can use the save_context() method to save the context of each interaction. It takes two ar-\\nguments: user_input and model_output. These represent the user’s input and the corresponding \\nmodel’s output for a given interaction.\\nmemory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})\\nmemory.save_context({\"input\": \"not much you\"}, {\"output\": \"not much\"})'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 185}, page_content='Chapter 5 163\\nWe can see the message with memory.load_memory_variables({}).\\nWe can also customize the conversational memory in LangChain, which involves modifying the \\nprefixes used for the AI and human messages, as well as updating the prompt template to reflect \\nthese changes.\\nTo customize the conversational memory, you can follow these steps:\\n1. Import the necessary classes and modules from LangChain:\\nfrom langchain.llms import OpenAI\\nfrom langchain.chains import ConversationChain\\nfrom langchain.memory import ConversationBufferMemory\\nfrom langchain.prompts.prompt import PromptTemplate\\nllm = OpenAI(temperature=0)\\n2. Define a new prompt template that includes the customized prefixes. You can do this by \\ncreating a PromptTemplate object with the desired template string:\\ntemplate = \"\"\"The following is a friendly conversation between a \\nhuman and an AI. The AI is talkative and provides lots of specific \\ndetails from its context. If the AI does not know the answer to a \\nquestion, it truthfully says it does not know.\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI Assistant:\"\"\"\\nPROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], \\ntemplate=template)\\nconversation = ConversationChain(\\n    prompt=PROMPT,\\n    llm=llm,\\n    verbose=True,\\n    memory=ConversationBufferMemory(ai_prefix=\"AI Assistant\"),\\n)\\nIn this example, the AI prefix is set to AI Assistant instead of the default AI.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 186}, page_content='Building a Chatbot like ChatGPT164\\nRemembering conversation summaries\\nConversationSummaryMemory is a type of memory in LangChain that generates a summary of the \\nconversation as it progresses. Instead of storing all messages verbatim, it condenses the informa-\\ntion, providing a summarized version of the conversation. This is particularly useful for extended \\nconversations, where including all previous messages might exceed token limits.\\nTo use ConversationSummaryMemory, first create an instance of it, passing the language model \\n(llm) as an argument. Then, use the save_context() method to save the interaction context, \\nwhich includes the user input and AI output. To retrieve the summarized conversation history, \\nuse the load_memory_variables() method.\\nHere’s an example:\\nfrom langchain.memory import ConversationSummaryMemory\\nfrom langchain.llms import OpenAI\\n# Initialize the summary memory and the language model\\nmemory = ConversationSummaryMemory(llm=OpenAI(temperature=0))\\n# Save the context of an interaction\\nmemory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})\\n# Load the summarized memory\\nmemory.load_memory_variables({})\\nStoring knowledge graphs\\nIn LangChain, we can also extract information from the conversation as facts and store these by \\nintegrating a knowledge graph as the memory. This can enhance the capabilities of language \\nmodels and enable them to leverage structured knowledge during text generation and inference.\\nA knowledge graph is a structured knowledge representation model that organizes information \\nin the form of entities, attributes, and relationships. It represents knowledge as a graph, where \\nentities are represented as nodes and relationships between entities are represented as edges. In \\na knowledge graph, entities can be any concept, object, or thing in the world, and attributes de-\\nscribe the properties or characteristics of these entities. Relationships capture the connections and \\nassociations between entities, providing contextual information and enabling semantic reasoning.\\nThere’s functionality in LangChain for knowledge graphs for retrieval; however, LangChain also \\nprovides memory components to automatically create a knowledge graph based on our conver-\\nsation messages.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 187}, page_content='Chapter 5 165\\nWe’ll instantiate the ConversationKGMemory class and pass your LLM instance as the llm pa-\\nrameter:\\nfrom langchain.memory import ConversationKGMemory\\nfrom langchain.llms import OpenAI\\nllm = OpenAI(temperature=0)\\nmemory = ConversationKGMemory(llm=llm)\\nAs the conversation progresses, we can save relevant information from the knowledge graph into \\nthe memory using the save_context() function of ConversationKGMemory.\\nCombining several memory mechanisms\\nLangChain also allows combining multiple memory strategies using the CombinedMemory class. \\nThis is useful when you want to maintain various aspects of the conversation history. For instance, \\none memory could be used to store the complete conversation log:\\nfrom langchain.llms import OpenAI\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.chains import ConversationChain\\nfrom langchain.memory import ConversationBufferMemory, CombinedMemory, \\nConversationSummaryMemory\\n# Initialize language model (with desired temperature parameter)\\nllm = OpenAI(temperature=0)\\n# Define Conversation Buffer Memory (for retaining all past messages)\\nconv_memory = ConversationBufferMemory(memory_key=\"chat_history_lines\", \\ninput_key=\"input\")\\n# Define Conversation Summary Memory (for summarizing conversation)\\nsummary_memory = ConversationSummaryMemory(llm=llm, input_key=\"input\")\\n# Combine both memory types\\nmemory = CombinedMemory(memories=[conv_memory, summary_memory])\\n# Define Prompt Template\\n_DEFAULT_TEMPLATE = \"\"\"The following is a friendly conversation between a \\nhuman and an AI. The AI is talkative and provides lots of specific details \\nfrom its context. If the AI does not know the answer to a question, it \\ntruthfully says it does not know.\\nSummary of conversation:\\n{history}'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 188}, page_content='Building a Chatbot like ChatGPT166\\nCurrent conversation:\\n{chat_history_lines}\\nHuman: {input}\\nAI:\"\"\"\\nPROMPT = PromptTemplate(input_variables=[\"history\", \"input\", \"chat_\\nhistory_lines\"], template=_DEFAULT_TEMPLATE)\\n# Initialize the Conversation Chain\\nconversation = ConversationChain(llm=llm, verbose=True, memory=memory, \\nprompt=PROMPT)\\n# Start the conversation\\nconversation.run(\"Hi!\")\\nIn this  example, we first instantiate the language model and the several types of memories \\nwe’re using – ConversationBufferMemory  for retaining the full conversation history and \\nConversationSummaryMemory for creating a summary of the conversation. We then combine \\nthese memories using CombinedMemory. We also define a prompt template that accommodates \\nour memory usage and, finally, we create and run ConversationChain by providing our language \\nmodel, memory, and prompt to it.\\nConversationSummaryBufferMemory is used to keep a buffer of recent interactions in memory and \\ncompiles old interactions into a summary instead of completely flushing them out. The threshold \\nfor flushing interactions is determined by token length and not by the number of interactions.\\nTo use this, the memory buffer needs to be instantiated with the LLM, and max_token_limit. \\nConversationSummaryBufferMemory offers a method called predict_new_summary(), which can \\nbe used directly to generate a conversation summary.\\nLong-term persistence\\nThere are also different ways of storing conversations in dedicated backends. Zep, being one such \\nexample, provides a persistent backend to store, summarize, and search chat histories using vec-\\ntor embeddings and auto-token counting. This long-term memory with fast vector search and \\nconfigurable summarization enables more capable conversational AI with context awareness.\\nA practical example of using Zep is to integrate it as the long-term memory for a chatbot or AI \\napp. By using the ZepMemory class, developers can initialize a ZepMemory instance with the Zep \\nserver URL, API key, and a unique session identifier for the user. This allows the chatbot or AI app \\nto store and retrieve chat history or other relevant information.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 189}, page_content='Chapter 5 167\\nFor example, in Python, you can initialize a ZepMemory instance as follows:\\nfrom langchain.memory import ZepMemory  \\nZEP_API_URL = \"http://localhost:8000\" \\nZEP_API_KEY = \"<your JWT token>\"\\nsession_id = str(uuid4()) \\nmemory = ZepMemory( \\n    session_id=session_id, \\n    url=ZEP_API_URL, \\n    api_key=ZEP_API_KEY, \\n    memory_key=\"chat_history\", \\n)\\nThis sets up a ZepMemory instance that you can use in your chains. Please note that the URL and \\nAPI key need to be set according to your setup. As mentioned, once the memory is set up, you \\ncan use it in your chatbot’s chain or with your AI agent to store and retrieve chat history or other \\nrelevant information. Overall, Zep simplifies the process of persisting, searching, and enriching \\nchatbot or AI app histories, allowing developers to focus on developing their AI applications \\nrather than building memory infrastructure.\\nIn the next section, we’ll look at using moderation to make sure responses are adequate. Mod-\\neration is crucial for creating a safe, respectful, and inclusive environment for users, protecting \\nbrand reputation, and complying with legal obligations.\\nModerating responses\\nThe role of moderation in chatbots is to ensure that the bot’s responses and conversations are \\nappropriate, ethical, and respectful. It involves implementing mechanisms to filter out offensive \\nor inappropriate content and discouraging abusive behavior from users. This is an important part \\nof any application that we’d want to deploy for customers.\\nIn the context of moderation, a constitution refers to a set of guidelines or rules that govern the \\nbehavior and responses of the chatbot. It outlines the standards and principles that the chatbot \\nshould adhere to, such as avoiding offensive language, promoting respectful interactions, and \\nmaintaining ethical standards. The constitution serves as a framework for ensuring that the \\nchatbot operates within the desired boundaries and provides a positive user experience.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 190}, page_content='Building a Chatbot like ChatGPT168\\nModeration and having a constitution are important in chatbots for several reasons:\\n• Ensuring ethical behavior: Chatbots can interact with a wide range of users, including \\nvulnerable individuals. Moderation helps ensure that the bot’s responses are ethical, re-\\nspectful, and do not promote harmful or offensive content.\\n• Protecting users from inappropriate content : Moderation helps prevent the dissemi -\\nnation of inappropriate or offensive language, hate speech, or any content that may be \\nharmful or offensive to users. It creates a safe and inclusive environment for users to \\ninteract with the chatbot.\\n• Maintaining brand reputation: Chatbots often represent a brand or organization. By \\nimplementing moderation, the developer can ensure that the bot’s responses align with \\nthe brand’s values and maintain a positive reputation.\\n• Preventing abusive behavior: Moderation can discourage users from engaging in abusive \\nor improper behavior. By implementing rules and consequences, such as the “two strikes” \\nrule mentioned in the example, the developer can discourage users from using provocative \\nlanguage or engaging in abusive behavior.\\n• Legal compliance: Depending on the jurisdiction, there may be legal requirements for \\nmoderating content and ensuring that it complies with laws and regulations. Having a \\nconstitution or set of guidelines helps the developer adhere to these legal requirements.\\nYou can add a moderation chain to an LLMChain instance or a Runnable instance to ensure that \\nthe generated output from the language model is not harmful.\\nIf the content passed into the moderation chain is deemed harmful, there are a few ways to han-\\ndle it. You can choose to throw an error in the chain and handle it in your application, or you can \\nreturn a message to the user explaining that the text was harmful. The specific handling method \\ndepends on your application’s requirements.\\nIn LangChain, first, you would create an instance of the OpenAIModerationChain class, which is \\na pre-built moderation chain provided by LangChain. This chain is specifically designed to detect \\nand filter out harmful content:\\nfrom langchain.chains import OpenAIModerationChain \\nmoderation_chain = OpenAIModerationChain()'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 191}, page_content='Chapter 5 169\\nNext, you would create an instance of the LLMChain class or of a Runnable instance, which rep-\\nresents your language model chain. This is where you define your prompt and interact with the \\nlanguage model. We can do this using the LCEL syntax, which we’ve introduced in Chapter 4, \\nBuilding Capable Assistants:\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.schema import StrOutputParser\\ncot_prompt = PromptTemplate.from_template(\\n    \"{question} \\\\nLet\\'s think step by step!\"\\n)\\nllm_chain = cot_prompt | ChatOpenAI() | StrOutputParser()\\nThis is a chain with a Chain of Thought (CoT) prompt, which includes the instruction to think \\nstep by step.\\nTo append the moderation chain to the language model chain, you can use the SequentialChain \\nclass or the LCEL (which is recommended). This allows you to chain multiple chains together in \\na sequential manner:\\nchain = llm_chain | moderation_chain\\nNow, when you want to generate text using the language model, you would pass your input text \\nthrough the moderation chain first, and then through the language model chain.\\nresponse = chain.invoke({\"question\": \"What is the future of \\nprogramming?\"}) \\nThe first chain will come up with a preliminary answer. Then, the moderation chain will evaluate \\nthis answer and filter out any harmful content. If the input text is deemed harmful, the moder -\\nation chain can either throw an error or return a message indicating that the text is not allowed. \\nI’ve added an example for moderation to the chatbot app on GitHub.\\nFurther, guardrails can be used to define the behavior of the language model on specific topics, \\nprevent it from engaging in discussions on unwanted topics, guide the conversation along a \\npredefined path, enforce a particular language style, extract structured data, and more.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 192}, page_content='Building a Chatbot like ChatGPT170\\nIn the context of LLMs, guardrails (rails for short) refer to specific ways of controlling the model’s \\noutput. They provide a means to add programmable constraints and guidelines to ensure the \\noutput of the language model aligns with desired criteria.\\nHere are a few ways guardrails can be used:\\n• Controlling topics: Guardrails allow you to define the behavior of your language model or \\nchatbot on specific topics. You can prevent it from engaging in discussions on unwanted \\nor sensitive topics like politics.\\n• Predefined dialogue paths: Guardrails enable you to define a predefined path for the \\nconversation. This ensures that the language model or chatbot follows a specific flow \\nand provides consistent responses.\\n• Language style: Guardrails allow you to specify the language style that the language \\nmodel or chatbot should use. This ensures that the output is in line with your desired \\ntone, formality, or specific language requirements.\\n• Structured data extraction: Guardrails can be used to extract structured data from the \\nconversation. This can be useful for capturing specific information or performing actions \\nbased on user inputs.\\nOverall, guardrails provide a way to add programmable rules and constraints to LLMs and chatbots, \\nmaking them more trustworthy, safe, and secure in their interactions with users. By appending \\nthe moderation chain to your language model chain, you can ensure that the generated text is \\nmoderated and safe for use in your application.\\nSummary\\nIn the previous chapter, we discussed tool-augmented LLMs, which involve the utilization of \\nexternal tools or knowledge resources such as document corpora. In this chapter, we focused \\non retrieving relevant data from sources through vector search and injecting it into the context. \\nThis retrieved data serves as additional information to augment the prompts given to LLMs. I \\nalso introduced retrieval and vector mechanisms, and we discussed implementing a chatbot, the \\nimportance of memory mechanisms, and the importance of appropriate responses.\\nThe chapter started with an overview of chatbots, their evolution, and the current state of chat-\\nbots, highlighting the practical implications and enhancements of the capabilities of the current \\ntechnology. We discussed the importance of proactive communication. We explored retrieval \\nmechanisms, including vector storage, with the goal of improving the accuracy of chatbot re -\\nsponses. We went into detail on methods for loading documents and information, including \\nvector storage and embedding. '),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 193}, page_content='Chapter 5 171\\nAdditionally, we discussed memory mechanisms for maintaining knowledge and the state of \\nongoing conversations. The chapter concluded with a discussion on moderation, emphasizing \\nthe importance of ensuring that responses are respectful and aligned with organizational values.\\nThe features discussed in this chapter serve as a starting point to investigate issues like memory, \\ncontext, and the moderation of speech, but they can also be interesting for issues like halluci -\\nnations.\\nQuestions\\nPlease see if you can produce the answers to these questions from memory. I’d recommend you \\ngo back to the corresponding sections of this chapter if you are unsure about any of them:\\n1. Please name 5 different chatbots!\\n2. What are some important aspects of developing a chatbot?\\n3. What does RAG stand for?\\n4. What is an embedding?\\n5. What is vector search?\\n6. What is a vector database?\\n7. Please name 5 different vector databases!\\n8. What is a retriever in LangChain?\\n9. What is memory and what are the memory options in LangChain?\\n10. What is moderation, what’s a constitution, and how do they work?\\nJoin our community on Discord\\nJoin our community’s Discord space for discussions with the authors and other readers:\\nhttps://packt.link/lang'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 194}, page_content=''),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 195}, page_content='6\\nDeveloping Software with \\nGenerative AI\\nWhile this book is about integrating generative AI particularly LLMs into software applications, in \\nthis chapter, we’ll talk about how we can leverage LLMs to help in software development. This is \\na big topic; software development has been highlighted in reports by several consultancies, such \\nas KPMG and McKinsey, as one of the domains impacted most by generative AI.\\nWe’ll first discuss how LLMs could help in coding tasks, and I’ll provide an overview to see how far \\nwe have come in automating software development. Then, we’ll play around with a few models, \\nevaluating the generated code qualitatively. Next, we’ll implement a fully automated agent for \\nsoftware development tasks. We’ll go through the design choices and show some of the results \\nthat we got in an agent implementation of only a few lines of Python with LangChain. We’ll \\nmention many possible extensions to this approach.\\nThroughout the chapter, we’ll work on different practical approaches to automatic software de-\\nvelopment, which you can find in the software_development directory in the GitHub repository \\nfor the book at https://github.com/benman1/generative_ai_with_langchain.\\nIn short, the main sections in this chapter are:\\n• Software development and AI\\n• Writing code with LLMs\\n• Automated software development\\nWe’ll begin the chapter by giving a broad overview of the current state of using AI for software \\ndevelopment.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 196}, page_content='Developing Software with Generative AI174\\nSoftware development and AI\\nThe emergence of powerful AI systems like ChatGPT has sparked great interest in using AI as \\na tool to assist software developers. A June 2023 report by KPMG estimated that about 25% of \\nsoftware development tasks could be automated away. A McKinsey report from the same month \\nhighlighted software development as a function, where generative AI can have a significant impact \\nin terms of cost reduction and efficiency gain.\\nThe history of software development has been marked by efforts to increase abstraction from \\nmachine code to focus more on problem-solving. Early procedural languages like FORTRAN and \\nCOBOL in the 1950s enabled this by introducing control structures, variables, and other high-lev-\\nel constructs. As programs grew larger, structured programming concepts emerged to improve \\ncode organization through modularity, encapsulation, and stepwise refinement. Object-oriented \\nlanguages like Simula and Smalltalk in the 1960s-70s introduced new paradigms for modularity \\nthrough objects and classes.\\nAs codebases expanded, maintaining quality became more challenging, leading to methodologies \\nlike agile development with iterative cycles and continuous integration. Integrated development \\nenvironments evolved to provide intelligent assistance for coding, testing, and debugging. Static \\nand dynamic program analysis tools helped identify issues in code. With neural networks and deep \\nlearning advancing in the 1990s and 2000s, machine learning techniques began to be applied to \\nimprove analysis capabilities for program synthesis, bug detection, vulnerability discovery, and \\nautomating other programming tasks.\\nToday’s AI assistants integrate predictive typing, syntax checking, code generation, and other \\nfeatures to directly support software development workflows, realizing early aspirations to au-\\ntomate programming itself.\\nNew code LLMs such as ChatGPT and Microsoft’s Copilot are highly popular generative AI models, \\nwith millions of users and significant productivity-boosting capabilities. There are different tasks \\nrelated to programming that LLMs can tackle, such as these:\\n• Code completion: This task involves predicting the next code element based on the sur-\\nrounding code. It is commonly used in Integrated Development Environments (IDEs) \\nto assist developers in writing code.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 197}, page_content='Chapter 6 175\\n• Code summarization/documentation: This task aims to generate a natural language sum-\\nmary or documentation for a given block of source code. This summary helps developers \\nunderstand the purpose and function of the code without having to read the actual code.\\n• Code search: The objective of code search is to find the most relevant code snippets based \\non a given natural language query. This task involves learning the joint embeddings of the \\nquery and code snippets to return the expected ranking order of code snippets.\\n• Bug finding/fixing: AI systems can reduce manual debugging efforts and enhance software \\nreliability and security. Many bugs and vulnerabilities are hard to find for programmers, \\nalthough there are typical patterns for which code validation tools exist. As an alternative, \\nLLMs can spot problems within code and (when prompted) correct them. Thus, these \\nsystems can reduce manual debugging efforts and help improve software reliability and \\nsecurity.\\n• Test generation: Similar to code completion, LLMs can generate unit tests (Codet: Code \\nGeneration with Generated Tests; Bei Chen and others, 2022) and other types of tests en-\\nhancing the maintainability of a codebase.\\nAI programming assistants combine the interactivity of earlier systems with innovative natural \\nlanguage processing. Developers can query programming problems in plain English or describe \\ndesired functions, receiving generated code or debugging tips. However, risks remain around code \\nquality, security, and excessive dependence. Striking the right balance of computer augmentation \\nwhile maintaining human oversight is an ongoing challenge.\\nLet’s look at the current performance of AI systems for coding, particularly code LLMs.\\nCode LLMs\\nQuite a few AI models have emerged, each with their own strengths and weaknesses, which are \\ncontinuously competing to improve and deliver better results. Performance continues to improve \\nwith models like StarCoder, though data quality can also play a key role. Studies show LLMs aid \\nworkflow efficiency but need more robustness, integration, and communication abilities.\\nPowerful pre-trained models like GPT-3 and GPT-4 enable context-aware, conversational support. \\nThese approaches also empower bug detection, repair recommendations, automated testing \\ntools, and code search.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 198}, page_content='Developing Software with Generative AI176\\nMicrosoft’s GitHub Copilot, which is based on OpenAI’s Codex, draws on open-source code to \\nsuggest full code blocks in real time. According to a GitHub report in June 2023, developers ac-\\ncepted the AI assistant’s suggestions about 30 percent of the time, which suggests that the tool \\ncan provide useful suggestions, with less experienced developers profiting the most.\\nTo illustrate the progress made in creating software, let’s look at quantitative results in a bench-\\nmark: the HumanEval dataset, introduced in the Codex paper (Evaluating Large Language Models \\nTrained on Code, 2021), is designed to test the ability of LLMs to complete functions based on \\ntheir signature and docstring. It evaluates the functional correctness of synthesizing programs \\nfrom docstrings. The dataset includes 164 programming problems that cover various aspects, \\nsuch as language comprehension, algorithms, and simple mathematics. Some of the problems \\nare comparable to simple software interview questions. A common metric on HumanEval is \\npass@k (pass@1) – this refers to the fraction of correct samples when generating k code samples \\nper problem.\\nThis chart summarizes the AI models on the HumanEval task (number of parameters against the \\npass@1 performance on HumanEval). A few performance metrics are self-reported:\\nRecent milestones:\\n• OpenAI’s Codex model in 2021 could generate code snippets from natural \\nlanguage descriptions, showing promise for assisting programmers.\\n• GitHub’s Copilot, launched in 2021, was an early integration of LLMs into \\nIDEs for autocompletion, achieving rapid adoption.\\n• DeepMind’s AlphaCode in 2022 matched human programming speed, show-\\ning the ability to generate full programs.\\n• OpenAI’s ChatGPT in 2022 demonstrated exceptionally coherent natural \\nlanguage conversations about coding.\\n• DeepMind’s AlphaTensor and AlphaDev in 2022 demonstrated AI’s ability \\nto discover novel, human-competitive algorithms, unlocking performance \\noptimizations.\\nCodex is a model developed by OpenAI. It can parse natural language and generate \\ncode, and it powers GitHub Copilot. A descendant of the GPT-3 model, it has been \\nfine-tuned on publicly available code from GitHub, 159 gigabytes of Python code \\nfrom 54 million GitHub repositories, for programming applications.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 199}, page_content='Chapter 6 177\\nFigure 6.1: Model comparison on HumanEval coding task benchmark\\nYou can see lines marking the performance of closed-source models such as GPT-4, GPT-4 with \\nreflection, PaLM-Coder 540B, GPT-3.5, and Claude 2. This is mainly based on the Big Code Models \\nLeaderboard, which is hosted on Hugging Face, but I’ve added a few more models for comparison, \\nand I’ve omitted models with more than 70 billion parameters. Some models have self-reported \\nperformance, so you should take this with a grain of salt.\\nAll models can do coding at some level, since the data used in training most LLMs includes some \\nsource code. For example, at least about 11% of the code in The Pile, a dataset that was curated \\nby EleutherAI’s GPT-Neo for training open-source alternatives of the GPT models, is from GitHub \\n(102.18 GB). The Pile was used in the training of Meta’s Llama, Yandex’s YaLM 100B, and many \\nothers.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 200}, page_content='Developing Software with Generative AI178\\nAlthough HumanEval has been broadly used as a benchmark for code LLMs, there are a multitude \\nof benchmarks for programming. Here’s an example question and the response from an advanced \\ncomputer science test given to Codex (source: My AI Wants to Know if This Will Be on the Exam: \\nTesting OpenAI’s Codex on CS2 Programming Exercises by James Finnie-Ansley and others, 2023):\\nFigure 6.2: A question given in a CS2 exam (left) and the Codex response\\nMost recently, the paper Textbooks Are All You Need by Suriya Gunasekar and others at Microsoft \\nResearch (2023) introduced phi-1, a 1.3B-parameter Transformer-based language model for code. \\nThe paper demonstrates how high-quality data can enable smaller models to match larger models \\nfor code tasks. The authors start with a 3 TB corpus of code from The Stack and Stack Overflow. \\nAn LLM filters this to select 6B high-quality tokens. Separately, GPT-3.5 generates 1B tokens mim-\\nicking a textbook style. The small 1.3B-parameter phi-1 model is trained on this filtered data. Phi-1 \\nis then fine-tuned on exercises synthesized by GPT-3.5. Results show phi-1 matches or exceeds \\nthe performance of models over 10x its size on benchmarks like HumanEval and MBPP.\\nThe core conclusion is that high-quality data significantly impacts model performance, potentially \\naltering scaling laws. Instead of brute-force scaling, data quality should take precedence. The \\nauthors reduce costs by using a smaller LLM to select data, rather than an expensive full evalu-\\nation. Recursively filtering and retraining on selected data could enable further improvements.\\nGenerating complete programs that demonstrate a deep understanding of the problem and plan-\\nning involved requires fundamentally different capabilities than producing short code snippets \\nthat mainly translate specifications directly into API calls. While recent models can achieve im -\\npressive performance on snippet generation, there remains a massive step up in difficulty in \\ncreating full programs.\\nHowever, novel reasoning-focused strategies like the Reflexion framework (Reflexion: Language \\nAgents with Verbal Reinforcement Learning by Noah Shinn and others; 2023) can lead to enormous \\nimprovements even for short code snippets. Reflexion enables trial-and-error-based learning, \\nwith language agents verbally reflecting on task feedback and storing this experience in an ep -\\nisodic memory buffer. '),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 201}, page_content='Chapter 6 179\\nThis reflection and memory of past outcomes guides better future decisions. On coding tasks, \\nReflexion significantly outperformed previous state-of-the-art models, achieving 91% pass@1 \\naccuracy on the HumanEval benchmark compared to just 67% for GPT-4 as reported originally \\nby OpenAI, although that metric was surpassed later, as the graph shows.\\nThis demonstrates the substantial potential of reasoning-driven approaches to overcome limita-\\ntions and boost the performance of language models like GPT-4 for programming. Rather than \\njust relying on pattern recognition, integrating symbolic reasoning into model architectures and \\ntraining could provide a path toward more human-like semantic understanding and planning \\nabilities for generating complete programs in the future.\\nThe rapid progress in applying large language models to automate programming tasks is encour-\\naging, but limitations persist, especially in robustness, generalization, and true semantic under-\\nstanding. As more capable models emerge, thoughtfully integrating AI assistance into developer \\nworkflows raises important considerations around human-AI collaboration, establishing trust, \\nand ethical usage. Ongoing research is actively exploring approaches to make these models more \\naccurate, safe, and beneficial for both programmers and society at large. With careful oversight \\nand further technical development to ensure reliability and transparency, AI programming as -\\nsistants have immense potential to increase productivity by automating tedious tasks, while \\nempowering human developers to focus their creativity on solving complex problems. However, \\nfully realizing this potential requires continued progress on the technical challenges, further \\ndeveloping standards and best practices, and proactive engagement with the legal and ethical \\nissues surrounding these emerging technologies.\\nIn the next section, we’ll see how we can generate software code with LLMs and how we can \\nexecute this from within LangChain.\\nWriting code with LLMs\\nLet’s start off by applying a model to write code for us. We can use one of the publicly available \\nmodels for generating code. I’ve listed a few examples before, such as ChatGPT or Bard. From \\nLangChain, we can call OpenAI’s LLMs, PaLM’s code-bison, or a variety of open-source models, \\nfor example, through Replicate, Hugging Face Hub, or – for local models – Llama.cpp, GPT4All, \\nor Hugging Face pipeline integrations.\\nStarCoder\\nLet’s have a look at StarCoder, which is a small model for code generation and quite capable of \\ndoing that. It is available at Hugging Face Spaces at this URL: https://huggingface.co/spaces/\\nbigcode/bigcode-playground'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 202}, page_content='Developing Software with Generative AI180\\nThis screenshot shows the model in a playground on Hugging Face Spaces:\\nFigure 6.3: StarCoder Models Playground\\nWe can choose between different models: StarCoder, StarCoderPlus, and StarCoderBase. Please \\nnote that, as the description says, the StarCoder model is not instruction-tuned, which means \\nthat we can’t give it tasks to do. We can’t say “write a class that…” but we can ask it to complete a \\ntext, where we prompt the model with # dataclass of customer including an alphanumeric \\nid, a name, and a birthday – let’s try this!\\nWe can toggle settings for temperature, max new tokens, top-n, and a repetition penalty. For \\nanything non-trivial, we need to increase the max new tokens setting.\\nWe should be getting a code snippet as a response on the webpage. I got the following code, which \\ngives us a useful data model for our hypothetical customer:\\n@dataclass(frozen=True)  # frozen means that the class cannot be modified \\nafter creation. This is useful for storing data in databases or other \\npersistent storages (e.g., files).\\nclass Customer:\\n    cust_id : str = field()   # this defines how to create instances from \\nthe class; it\\'s called \"field\" because we\\'re creating fields within our \\nobject/instance\\n    firstname : str = \"\"      # these are default values if no value was \\nprovided when instantiating objects using this class\\n    lastname : str = \"\"       # note that there can only ever exist one \\ninstance variable per line - you\\'ll get errors otherwise!'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 203}, page_content='Chapter 6 181\\n    def __post_init__(self):\\n        \"\"\"This method runs automatically once upon instantiation\"\"\"\\n        self._validate_cust_id()    # validate input before saving \\nanything into memory\\n        self.__set_fullname__()     # set full name based on given inputs\\n    @property        # getter function used by Python interpreter whenever \\ntrying to access property directly instead of through. notation e.g.: \\nmyobj.firstname vs just myobj.firstname()\\n    def fullname(self)->str:\\n        return f\"{self.lastname}, {self.firstname}\"\\n    ## private methods below here\\n    ### validation functions go here\\n    def _validate_cust_id(self):\\n        assert len(self.cust_id)==8, \\\\\\n            ValueError(\"Customer ID must have exactly eight characters\")\\n        try:\\n            int(self.cust_id[0]) == True\\n        except Exception as err:\\n            raise TypeError(\\'First character of CustID should be numeric\\') \\nfrom None\\n    #### setter functions go here\\n    def __set_fullname__(self):\\n        \\'\\'\\'sets the customers\\' full name\\'\\'\\'\\n        names = [x.strip().title() for x in re.split(\"[\\\\W]+\", self.\\nfirstname)] + [\\'\\'] * max((1,-len([y.strip().title() for y in \\nre.split(\"[\\\\W]+\", self.lastname)])) )\\n        print(names)\\n        self.firstname=\\' \\'.join(filter(None,[n[:3]for n in \\nreversed(names)])+[\\'\\'])'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 204}, page_content='Developing Software with Generative AI182\\n        self.lastname =\\'\\'.join([\\' \\'*(4-len(l))+\\' \\'+ l[-2:]if not \\nl==\\'I\\' else \\'\\' for l in filter(None,[n[:-2].upper()+\\',\\'*bool(re.\\nsearch(\\'[aeiou]\\',n))+n[-2:].upper() for n in reversed(names)])]).\\nrstrip(\\',\\')\\nThe preceding code is missing the imports, which would usually come before the comment prompt, \\nso I can’t fault the model for it. We need to add these two lines to the top:\\nimport re\\nfrom dataclasses import dataclass, field\\nThis doesn’t throw an error when we run it as is, which is impressive. However, there are a few \\nthings not ideal or even wrong with it when we look at it in a bit more detail.\\nI don’t like the commenting style and lack of docstrings. There are some print() statements left \\nin the code; often this is done for debugging purposes. This shouldn’t be in production code. It’s \\nnot ideal to have empty strings as defaults for firstname and lastname. I would expect the cus-\\ntomer ID to be assigned based on a hashing function – I can’t see why the ID should have exactly \\neight characters as enforced in the validation function for the property.\\nThere are more problems with this code that affect its correctness and readability, as well as a \\nfew other, more crucial problems, including attempting to write read-only attributes. firstname \\nand lastname in the data class can’t be changed after creation because they are frozen. The logic \\nabout creating a variable name from firstname and lastname using regular expressions in a \\nnon-standard syntax is strange to say the least – and incorrect. The last name gets dropped in \\nthe process. The filter on the reversed name is also highly suspect.\\nI’ll leave it at that. This has given us some promising ideas and a structure to start with, but it’s \\nnot production-level code. It doesn’t even work. You can see this code in the customer.py file in \\nthe book’s GitHub repo under the software_development folder.\\nLet’s try again to get something more useful out of the model. We started off on the wrong foot. \\nWe started a code snippet with bad syntax and expected code that works. That’s not realistic. \\nLet’s try again, and start with a prompt that is more up to standard:\\n\"\"\"Customer data model.\\nHere we implement our data model for customers, a dataclass with\\nfields firstname, lastname, customer_id (a hash).\\n\"\"\"'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 205}, page_content='Chapter 6 183\\nWe get the following result:\\n@dataclass(frozen=True)  # frozen means that the object cannot be modified \\nafter creation!\\nclass Customer:\\n    \"\"\"A class representing one single customer.\"\"\"\\n    firstname: str = \"\"\\n    lastname: str = \"\"\\n    id_: int = field(default=-1024, init=False)\\n   \\n    def __post_init__(self):\\n        self.__hash__()\\n       \\n    @property   \\n    def fullname(self)->str:\\n        return f\"{self.firstname} {self.lastname}\"   \\n           \\n    def __eq__(self, other:\"Customer\")->bool:       \\n        if isinstance(other, type(self)):\\n            return all([\\n                getattr(self, attr)==getattr(other,attr)\\n                for attr in [\"firstname\", \"lastname\"]])           \\n        else:\\n            raise TypeError(\"Cannot compare different types\")       \\n               \\n    def __lt__(self, other:\"Customer\"): \\n        try:           \\n            assert len({type(x) for x in [self, other]})==1            \\n           \\n            if not hasattr(self,\"fullname\"):               \\n                setattr(self, \\'fullname\\',f\\'{self.firstname}{self.\\nlastname}\\')\\n               \\n            if not hasattr(other,\\'fullname\\'):              \\n                setattr(other, \\'fullname\\',f\\'{other.firstname}{other.\\nlastname}\\')'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 206}, page_content='Developing Software with Generative AI184\\n            return self.fullname < other.fullname\\n           \\n        except AssertionError as e: \\n            print(\\'Error:\\',e )\\n     \\n    def __repr__(self):   \\n        return f\"Customer(\\'{self.firstname}\\', \\'{self.\\nlastname}\\',\\'{hex(abs(self._customer_id))[-6:]})\"  \\n   \\n    def __hash__(self):      \\n        hsh = abs(hash((self.firstname+self.lastname))) % ((sys.maxsize + \\n1)*2)+ sys.maxsize*3     \\n        self.id_=hsh        \\n       \\n        return hsh\\nIt’s good to see the customer ID is created using a hash as expected. There is some boilerplate code \\nfor comparing two different customer objects. However, again, there are problems like the ones \\nbefore. First, it’s missing the imports, something I don’t understand given our prompt, which \\nshould be a module docstring found at the start of a file. The imports would come right after this. \\nSecond, it’s again attempting to set an attribute after initialization of the class that’s supposed \\nto be frozen, showing a lack of understanding of frozen attributes.\\nAfter fixing these two problems, we get our first Customer(). But then there’s another problem, \\nwhere the customer ID is referenced with the wrong name, demonstrating a lack of consistency. \\nAfter fixing this, we can initialize our customer, look at the attributes, and compare one customer \\nto another. I can see how this approach is starting to become useful for writing boilerplate code.\\nYou can see this code in the customer2.py file in the book’s GitHub repo, again in the software \\ndevelopment folder.\\nStarChat\\nLet’s try an instruction-tuned model so we can give it tasks! StarChat, which is based on StarCoder, \\nis available on Hugging Face at https://huggingface.co/spaces/HuggingFaceH4/starchat-\\nplayground.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 207}, page_content='Chapter 6 185\\nThis screenshot shows an example in StarChat, but please note that not all the code is visible:\\nFigure 6.4: StarChat implementing a function in Python for calculating prime numbers\\nPeople who own playgrounds on HuggingFace can pause or take down their play -\\nground whenever they wish. If you can’t access the HuggingFace StarChat playground \\nfor whatever reason, there are lots of other playgrounds that you could try, first of all, \\nthe BigCode playground, which enables access to StarCoderPlus, StarCoderBase, and \\nStarCoder: https://huggingface.co/spaces/bigcode/bigcode-playground\\nYou can also find quite a few playgrounds that are made available by other people, \\nfor example:\\n• A StarCoder playground by Sanjay Wankhede: https://huggingface.co/\\nspaces/sanjayw/starcoder-playground\\n• A playground for Code Llama models: https://huggingface.co/spaces/\\ncodellama/codellama-playground\\n• Joshua Lochner’s AI Code playground that allows switching between three \\nmodels including CodeGen-Mono 350M: https://huggingface.co/\\nspaces/Xenova/ai-code-playground'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 208}, page_content='Developing Software with Generative AI186\\nYou can find the complete code listing on GitHub.\\nFor this example, which is usually covered in first-year Computer Science courses, no imports \\nare needed. The algorithm’s implementation is straightforward. It executes right away and gives \\nthe expected result. Within LangChain, we can use the HuggingFaceHub integration like this:\\nfrom langchain import HuggingFaceHub\\nllm = HuggingFaceHub(\\n    task=\"text-generation\",\\n    repo_id=\"HuggingFaceH4/starchat-alpha\",\\n    model_kwargs={\\n        \"temperature\": 0.5,\\n        \"max_length\": 1000\\n    }\\n)\\nprint(llm(text))\\nIn this case, text is any prompt you want to give the model.\\nAs of late 2023, this LangChain integration has had some issues with timeouts – hopefully, this \\nwill be fixed soon. We are not going to use it here.\\nLlama 2\\nLlama 2 is not one of the best models for coding, with a pass@1 of about 29%; however, we can \\ntry it out on Hugging Face chat:'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 209}, page_content='Chapter 6 187\\nFigure 6.5: Hugging Face chat with Llama 2 at https://huggingface.co/chat/\\nPlease note that this is only the beginning of the output. Llama 2 implements this well and the \\nexplanations are spot on. Well done, StarCoder and Llama 2! Or was this just too easy?\\nSmall local model\\nThere are so many ways to accomplish code completion or  generation. We can even try a small \\nlocal model:\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\\ncheckpoint = \"Salesforce/codegen-350M-mono\"\\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint)\\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\\npipe = pipeline(\\n    task=\"text-generation\",\\n    model=model,\\n    tokenizer=tokenizer,\\n    max_new_tokens=500'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 210}, page_content='Developing Software with Generative AI188\\n)\\ntext = \"\"\"\\ndef calculate_primes(n):\\n    \\\\\"\\\\\"\\\\\"Create a list of consecutive integers from 2 up to N.\\n    For example:\\n    >>> calculate_primes(20)\\n    Output: [2, 3, 5, 7, 11, 13, 17, 19]\\n    \\\\\"\\\\\"\\\\\"\\n\"\"\"\\nThe preceding code is prompting CodeGen, a model by Salesforce (A Conversational Paradigm for \\nProgram Synthesis; Erik Nijkamp and colleagues, 2022). CodeGen 350 Mono received a pass@1 \\nperformance of 12.76% in HumanEval. As of July 2023, new versions of CodeGen have been released \\nwith only 6B parameters, which are very competitive. This clocks in at a performance of 26.13%. \\nThis last model was trained on the BigQuery dataset containing C, C++, Go, Java, JavaScript, and \\nPython, as well as the Big Python dataset, which consists of 5.5 TB of Python code.\\nSince this model was released before the HumanEval benchmark, the performance statistics for \\nthe benchmark were not part of the initial publication.\\nWe can now get the output from the pipeline like this:\\ncompletion = pipe(text)\\nprint(completion[0][\"generated_text\"])\\nAlternatively, we can wrap this pipeline via the LangChain integration:\\nfrom langchain import HuggingFacePipeline\\nllm = HuggingFacePipeline(pipeline=pipe)\\nllm(text)\\nThis is a bit verbose. There’s also the more convenient constructor method, HuggingFacePipeline.\\nfrom_model_id().\\nI am getting something similar to the StarCoder output. I had to add an import math, but the \\nfunction works.\\nWe could use this pipeline in a LangChain agent; however, please note that this model is not \\ninstruction-tuned, so you cannot give it tasks, only completion tasks. You can also use these \\nmodels for code embeddings. '),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 211}, page_content='Chapter 6 189\\nOther models that have been instruction-tuned and are available for chat can act as your techie \\nassistant to help with providing advice, documenting and explaining existing code, or translating \\ncode into other programming languages – for the last task, they need to have been trained on \\nenough samples in these languages.\\nPlease note that the approach taken here is a bit naïve; however, it is a good way to get started, \\nnonetheless. The discussion should serve as an introductory overview of code generation with \\nLLMs, from prompting considerations to execution and real-world viability. Publicly available \\nmodels like GPT-3 can produce initial code from prompts, but the results often require refinement \\nbefore use, as issues like incorrect logic may appear. Fine-tuning specifically for programming \\ntasks significantly improves control, accuracy, and task completion. Models trained on coding \\nprompts like StarCoder reliably generate valid code-matching prompts and conventions. Smaller \\nmodels are also capable options for lightweight code generation.\\nLet’s now try to implement a feedback cycle for code development, where we validate and run \\nthe code and change it based on feedback.\\nAutomating software development\\nIn LangChain, we have several integrations for code execution, like LLMMathChain, which executes \\nPython code to solve math questions, and BashChain, which executes Bash terminal commands, \\nwhich can help with system administration tasks. However, while useful for problem-solving, \\nthese don’t address the larger software development process.\\nThis approach of solving problems with code can, however, work quite well, as we’ll see here:\\nfrom langchain.llms.openai import OpenAI\\nfrom langchain.agents import load_tools, initialize_agent, AgentType\\nllm = OpenAI()\\ntools = load_tools([\"python_repl\"])\\nagent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_\\nDESCRIPTION, verbose=True)\\nresult = agent(\"What are the prime numbers until 20?\")\\nprint(result)\\nWe can see how the prime number calculations get processed quite well under the hood between \\nOpenAI’s LLM and the Python interpreter:\\nEntering new AgentExecutor chain...\\nI need to find a way to check if a number is prime'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 212}, page_content=\"Developing Software with Generative AI190\\nAction: Python_REPL\\nAction Input:\\ndef is_prime(n):\\n    for i in range(2, n):\\n        if n % i == 0:\\n            return False\\n    return True\\nObservation:\\nThought: I need to loop through the numbers to check if they are prime\\nAction: Python_REPL\\nAction Input:\\nprime_numbers = []\\nfor i in range(2, 21):\\n    if is_prime(i):\\n        prime_numbers.append(i)\\nObservation:\\nThought: I now know the prime numbers until 20\\nFinal Answer: 2, 3, 5, 7, 11, 13, 17, 19\\nFinished chain.\\n{'input': 'What are the prime numbers until 20?', 'output': '2, 3, 5, 7, \\n11, 13, 17, 19'}\\nWe get to the right answer about the prime numbers. LLM can produce correct prime number \\ncalculations. The code generation approach can work for simple cases. But real-world software \\ndemands modular, well-structured design with a separation of concerns.\\nTo automate software creation rather than just problem-solving, we need more sophisticated \\napproaches. This could involve an interactive loop where the LLM generates draft code, a human \\nprovides feedback steering it toward readable, maintainable code, and the model refines its output \\naccordingly. The human developer provides high-level strategic guidance while the LLM handles \\nthe grunt work of writing code.\\nThe next frontier is developing frameworks that enable human-LLM collaboration or – more \\ngenerally – feedback loops for efficient, robust software delivery. There are a few interesting \\nimplementations around for this.\"),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 213}, page_content='Chapter 6 191\\nFor example, the MetaGPT library approaches this with an agent simulation, where different \\nagents represent job roles in a company or IT department:\\nfrom metagpt.software_company import SoftwareCompany\\nfrom metagpt.roles import ProjectManager, ProductManager, Architect, \\nEngineer\\nasync def startup(idea: str, investment: float = 3.0, n_round: int = 5):\\n    \"\"\"Run a startup. Be a boss.\"\"\"\\n    company = SoftwareCompany()\\n    company.hire([ProductManager(), Architect(), ProjectManager(), \\nEngineer()])\\n    company.invest(investment)\\n    company.start_project(idea)\\n    await company.run(n_round=n_round)\\nThis is an example from the MetaGPT documentation. You need to have MetaGPT installed for \\nthis to work.\\nThis is an inspiring use case of an agent simulation. Another library for automated software \\ndevelopment is llm-strategy by Andreas Kirsch, which generates code for data classes using \\ndecorator patterns.\\nThis table gives an overview of a few projects:\\nproject maintainer description stars\\nGPT Engineer\\nhttps://github.com/AntonOsika/\\ngpt-engineer Anton Osika\\nGenerates full codebases from \\nprompts. Developer-friendly \\nworkflow. 45600\\nMetaGPT\\nhttps://github.com/geekan/\\nMetaGPT\\nAlexander \\nWu\\nMultiple GPT agents play \\ndevelopment roles based on \\na team Standard Operating \\nProcedure (SOP). 30700\\nChatDev\\nhttps://github.com/OpenBMB/\\nChatDev\\nOpenBMB \\n(Open Lab \\nfor Big \\nModel Base)\\nMulti-agent organization that \\ncollaborates via meetings. 17100'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 214}, page_content='Developing Software with Generative AI192\\nGPT Pilot\\nhttps://github.com/Pythagora-\\nio/gpt-pilot Pythagora\\nHuman oversees step-by-step \\ncoding toward production \\napps. 14800\\nDevOpsGPT\\nhttps://github.com/kuafuai/\\nDevOpsGPT KuafuAI\\nConverts requirements to \\nworking software with LLMs \\nand DevOps. 5100\\nCode Interpreter API\\nhttps://github.com/shroominic/\\ncodeinterpreter-api/\\nDominic \\nBäumer\\nExecutes Python from \\nprompts locally with \\nsandboxing. 3400\\nCodiumAI PR-Agent\\nhttps://github.com/Codium-ai/\\npr-agent Codium\\nAnalyzes pull requests \\nand provides auto-review \\ncommands. 2600\\nGPTeam\\nhttps://github.com/101dotxyz/\\nGPTeam 101dotxyz\\nCollaborative agents with \\nmemory and reflection. 1400\\nCodeT\\nhttps://github.com/microsoft/\\nCodeT/tree/main/CodeT\\nMicrosoft \\nResearch\\nGenerates code and tests. \\nRuns code against tests. 480\\nLangChain Coder\\nhttps://github.com/haseeb-\\nheaven/LangChain-Coder\\nHaseeb \\nHeaven\\nWeb code generation/\\ncompletion with OpenAI and \\nVertex AI. 58\\nCode-it\\nhttps://github.com/ChuloAI/\\ncode-it ChuloAI\\nIteratively refines code by \\nsteering LLM prompts based \\non execution. 46\\nTable 6.6: Overview of different LLM software development projects\\nThe key steps involve the LLM breaking down the software project into subtasks through prompts \\nand then attempting to complete each step. For example, prompts can instruct the model to set \\nup directories, install dependencies, write boilerplate code, and so on.\\nAfter executing each subtask, the LLM then assesses if it has completed successfully. If not, it \\ntries to debug the issue or reformulates the plan. This feedback loop of planning, attempting, and \\nreviewing allows it to iteratively refine its process.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 215}, page_content='Chapter 6 193\\nThe code-It project by Paolo Rechia and GPT Engineer by Anton Osika both follow a pattern as \\nillustrated in this graph for Code-It (source: https://github.com/ChuloAI/code-it):\\nFigure 6.6: Code-It control flow\\nMany of these steps consist of specific prompts that are sent to LLMs with instructions to break \\ndown the project or set up the environment. It’s quite impressive to implement the full feedback \\nloop with all the tools.\\nAutomatic software development with LLMs can also be explored with projects such as Auto-GPT \\nor Baby-GPT. However, these systems often get stuck in failure loops. The agent architecture is \\nkey to the robustness of the system.\\nWe can implement a simple feedback loop in various ways in LangChain, for example, using the \\nPlanAndExecute chain, a ZeroShotAgent, or BabyAGI. '),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 216}, page_content='Developing Software with Generative AI194\\nWe’ve discussed the basics of these two agent architectures in Chapter 5, Building a Chatbot like \\nChatGPT. Let’s go with PlanAndExecute, which is quite common. In the code on GitHub, you can \\nsee different architectures to try out.\\nThe main idea is to set up a chain and execute it with the objective of writing software, like this:\\nfrom langchain import OpenAI\\nfrom langchain_experimental.plan_and_execute import load_chat_planner, \\nload_agent_executor, PlanAndExecute\\nllm = OpenAI()\\nplanner = load_chat_planner(llm)\\nexecutor = load_agent_executor(\\n    llm,\\n    tools,\\n    verbose=True,\\n)\\nagent_executor = PlanAndExecute(\\n    planner=planner,\\n    executor=executor,\\n    verbose=True,\\n    handle_parsing_errors=\"Check your output and make sure it conforms!\",\\n    return_intermediate_steps=True\\n)\\nagent_executor.run(\"Write a tetris game in python!\")\\nSince I just want to show the idea here, I am omitting defining the tools \\nfor now – we\\'ll come to this in a moment. As mentioned already, the code \\non GitHub features many other implementation options; for example, agent \\narchitectures can be found there as well.\\nThere are a few more pieces to this implementation, but simple work like this could already write \\nsome code, depending on the instructions that we give.\\nOne thing we need is clear instructions for a language model to write Python code in a certain \\nform – we can reference syntax guidelines, for example:\\nfrom langchain import PromptTemplate, LLMChain, OpenAI'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 217}, page_content='Chapter 6 195\\nDEV_PROMPT = (\\n    \"You are a software engineer who writes Python code given tasks or \\nobjectives. \"\\n    \"Come up with a python code for this task: {task}\"\\n    \"Please use PEP8 syntax and comments!\"\\n)\\nsoftware_prompt = PromptTemplate.from_template(DEV_PROMPT)\\nsoftware_llm = LLMChain(\\n    llm=OpenAI(\\n        temperature=0,\\n        max_tokens=1000\\n    ),\\n    prompt=software_prompt\\n)\\nWhen using LLMs for code generation, it’s important to choose a model architecture that is opti-\\nmized for producing software code specifically. Models trained on more general textual data may \\nnot reliably generate syntactically correct and logically sound code. I’ve chosen a longer context, \\nso we don’t get cut off in the middle of a function, and a low temperature, so it doesn’t get too wild.\\nWe need an LLM that has seen many code examples during its training and can thus generate \\ncoherent functions, classes, control structures, and so on. Models like Codex, PythonCoder, and \\nAlphaCode are designed for code generation capabilities.\\nHowever, just generating raw code text is not sufficient. We also need to execute the code to test \\nit and provide meaningful feedback to the LLM. This allows us to iteratively refine and improve \\nthe code quality.\\nFor execution and feedback, the LLM itself does not have inherent capabilities to save files, run \\nprograms, or integrate with external environments. That’s where LangChain’s tools come in.\\nThe tools argument to the executor allows specifying Python modules, libraries, and other re -\\nsources that can extend the LLM’s reach. For example, we can use tools to write the code to file, \\nexecute it with different inputs, capture the outputs, check correctness, analyze style, and more.\\nBased on the tool outputs, we can provide feedback to the LLM on which parts of the code worked \\nand which need improvement. The LLM can then generate enhanced code incorporating this \\nfeedback.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 218}, page_content='Developing Software with Generative AI196\\nOver multiple generations, the human-LLM loop allows for the creation of well-structured, robust \\nsoftware that meets the desired specifications. The LLM brings raw coding productivity while \\nthe tools and human oversight ensure quality.\\nLet’s see how we can implement this – let’s define the tools argument as promised:\\nfrom langchain.tools import Tool\\nfrom software_development.python_developer import PythonDeveloper, \\nPythonExecutorInput\\nsoftware_dev = PythonDeveloper(llm_chain=software_llm)\\ncode_tool = Tool.from_function(\\n    func=software_dev.run,\\n    name=\"PythonREPL\",\\n    description=(\\n        \"You are a software engineer who writes Python code given a \\nfunction description or task.\"\\n    ),\\n    args_schema=PythonExecutorInput\\n)\\nThe PythonDeveloper class has all the logic about taking tasks given in any form and translat -\\ning them into code. The main idea is that it provides a pipeline to go from natural language task \\ndescriptions to generated Python code to executing that code safely, capturing the output, and \\nvalidating that it runs. The LLM chain powers the code generation while the execute_code() \\nmethod handles running it.\\nThis environment enables automating the development cycle of coding and testing from language \\nspecifications. The human provides the task and validates the results while the LLM handles \\ntranslating descriptions to code. Here it goes:\\nclass PythonDeveloper():\\n    \"\"\"Execution environment for Python code.\"\"\"\\n    def __init__(\\n            self,\\n            llm_chain: Chain,\\n    ):\\n        self. llm_chain = llm_chain'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 219}, page_content='Chapter 6 197\\n    def write_code(self, task: str) -> str:\\n        return self.llm_chain.run(task)\\n    def run(\\n            self,\\n            task: str,\\n    ) -> str:\\n        \"\"\"Generate and Execute Python code.\"\"\"\\n        code = self.write_code(task)\\n        try:\\n            return self.execute_code(code, \"main.py\")\\n        except Exception as ex:\\n            return str(ex)\\n    def execute_code(self, code: str, filename: str) -> str:\\n        \"\"\"Execute a python code.\"\"\"\\n        try:\\n            with set_directory(Path(self.path)):\\n                ns = dict(__file__=filename, __name__=\"__main__\")\\n                function = compile(code, \"<>\", \"exec\")\\n                with redirect_stdout(io.StringIO()) as f:\\n                    exec(function, ns)\\n                    return f.getvalue()\\nI am again leaving out a few pieces – the error handling in particular is very simplistic here. In the \\nimplementation on GitHub, we can distinguish various kinds of errors we are getting, such as these:\\n• ModuleNotFoundError: This means that the code tries to work with packages that we \\ndon’t have installed. I’ve implemented logic to install these packages.\\n• NameError: Using variable names that don’t exist.\\n• SyntaxError: The parentheses in the code haven’t been closed or it is not even code.\\n• FileNotFoundError: The code relies on files that don’t exist. I’ve found a few times that \\nthe code tried showing images that were made up.\\n• SystemExit: If something more dramatic happens and Python crashes.\\nI’ve implemented logic to install packages for ModuleNotFoundError, and clearer messages for \\nsome of these problems. In the case of missing images, we could add a generative image model to \\ncreate them. Returning all this as enriched feedback to the code generation results in increasingly \\nspecific output such as this:'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 220}, page_content='Developing Software with Generative AI198\\nWrite a basic tetris game in Python with no syntax errors, properly closed \\nstrings, brackets, parentheses, quotes, commas, colons, semi-colons, and \\nbraces, no other potential syntax errors, and including the necessary \\nimports for the game\\nThe Python code itself gets compiled and executed in a subdirectory and we redirect the output \\nof the Python execution to capture it; this is implemented as Python contexts.\\nWhen generating code using large language models, it is important to be careful about running \\nthat code, especially on a production system. There are several security risks involved:\\n• The LLM could produce code with vulnerabilities or backdoors either inadvertently due \\nto its training or maliciously if adversarially manipulated.\\n• The generated code interacts directly with the underlying operating system, allowing \\naccess to files, networks, and so on. It is not sandboxed or containerized.\\n• Bugs in the code could cause crashes or unwanted behavior on the host machine.\\n• Resource usage like CPU, memory, and disk could be unchecked.\\nSo, essentially, any code executed from an LLM has significant power over the local system. This \\nmakes security a major concern compared to running code in isolated environments like note -\\nbooks or sandboxes.\\nThere are tools and frameworks that can sandbox generated code and limit its authority. For \\nPython, options include RestrictedPython, pychroot, setuptools’ DirectorySandbox, and code -\\nbox-api. These allow enclosing the code in virtual environments or restricting access to sensitive \\nOS functions.\\nIdeally, LLM-generated code should first be thoroughly inspected and its resource usage profiled, \\nvulnerabilities scanned, and functionality unit tested before being run on production systems. We \\ncould implement safety and style guardrails similar to what we discussed in Chapter 5, Building \\na Chatbot like ChatGPT.\\nWhile sandboxing tools can provide additional protection, it’s best to be cautious and only execute \\nLLM code in disposable or isolated environments until trust in the model is established. Risks \\nlike crashes, hacks, and data loss from blindly running unverified code could be substantial. Safe \\npractices are crucial as LLMs become part of software pipelines.\\nWith this out of the way, let’s define tools:\\nddg_search = DuckDuckGoSearchResults()\\ntools = [\\n    codetool,'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 221}, page_content='Chapter 6 199\\n    Tool(\\n        name=\"DDGSearch\",\\n        func=ddg_search.run,\\n        description=(\\n            \"Useful for research and understanding background of \\nobjectives. \"\\n            \"Input: an objective. \"\\n            \"Output: background information about the objective. \"\\n        )\\n    )\\n]\\nAn internet search is worth adding to ensure we are implementing something related to our ob-\\njective. When working with this tool, I’ve seen a few implementations of Rock, Paper, Scissors \\ninstead of Tetris, so it’s important to understand the objective.\\nWhen running our agent executor with the objective of implementing Tetris, the results are a \\nbit different every time. We can see the agent activity in the intermittent results. Looking at this, \\nI am observing searches for requirements and game mechanics, and code is repeatedly being \\nproduced and executed.\\nI find here that the pygame library is installed. The following code snippet is not the final product, \\nbut it brings up a window:\\n# This code is written in PEP8 syntax and includes comments to explain the \\ncode\\n# Import the necessary modules\\nimport pygame\\nimport sys\\n# Initialize pygame\\npygame.init()\\n# Set the window size\\nwindow_width = 800\\nwindow_height = 600\\n# Create the window'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 222}, page_content=\"Developing Software with Generative AI200\\nwindow = pygame.display.set_mode((window_width, window_height))\\n# Set the window title\\npygame.display.set_caption('My Game')\\n# Set the background color\\nbackground_color = (255, 255, 255)\\n# Main game loop\\nwhile True:\\n    # Check for events\\n    for event in pygame.event.get():\\n        # Quit if the user closes the window\\n        if event.type == pygame.QUIT:\\n            pygame.quit()\\n            sys.exit()\\n    # Fill the background with the background color\\n    window.fill(background_color)\\n    # Update the display\\n    pygame.display.update()\\nThe code is not too bad in terms of syntax – I guess the prompt must have helped. However, in \\nterms of functionality, it’s very far from Tetris.\\nThis implementation of a fully automated agent for software development is still quite experimen-\\ntal. It’s also amazingly simple and basic, consisting only of about 340 lines of Python, including \\nthe imports, which you can find on GitHub.\\nI think a better approach could be to break down all the functionality into functions and maintain \\na list of functions to call, which can be used in all subsequent generations of code. An advantage to \\nour approach is, however, that it’s easy to debug, since all steps including searches and generated \\ncode are written to a log file in the implementation.\\nWe could also define additional tools such as a planner that breaks down the tasks into functions. \\nYou can see this in the GitHub repo.\\nFinally, we could try a test-driven development approach or have a human give feedback rather \\nthan a fully automated process. \"),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 223}, page_content='Chapter 6 201\\nLLMs can produce reasonable sets of test cases from high-level descriptions. But human oversight \\nis essential to catch subtle mistakes and validate completeness. Generating implementation code \\nfirst and then deriving tests risks baking in incorrect behavior. The right flow is specifying expect-\\ned behavior, vetting test cases, and then creating code that passes. The process works in small \\nsteps – generate a test, review and enhance it, and use the final version’s changes to inform the \\nnext test or code generation. Explicitly providing feedback helps the LLM improve over iterations.\\nSummary\\nIn this chapter, we’ve discussed LLMs for source code and how they can help in developing soft-\\nware. There are quite a few areas where LLMs can benefit software development, mostly as coding \\nassistants. We’ve applied a few models for code generation using naïve approaches and we’ve \\nevaluated them qualitatively. In programming, as we’ve seen, compiler errors and results of code \\nexecution can be used to provide feedback. Alternatively, we could have used human feedback \\nor implemented tests.\\nWe’ve seen how the suggested solutions seem superficially correct but don’t perform the task \\nor are full of bugs. However, we can get a sense that – with the right architectural setup – LLMs \\ncould feasibly learn to automate coding pipelines. This could have significant implications re -\\ngarding safety and reliability. As for now, human guidance on high-level design and rigorous \\nreview seem indispensable to prevent subtle errors, and the future likely involves collaboration \\nbetween humans and AI.\\nWe didn’t implement semantic code search in this chapter since it’s very similar to the chatbot \\nimplementation in the previous chapter. In Chapter 7, LLMs for Data Science, we’ll work with LLMs \\nfor applications in data science and machine learning.\\nQuestions\\nPlease look to see if you can produce the answers to these questions from memory. I’d recommend \\nyou go back to the corresponding sections of this chapter if you are unsure about any of them:\\n1. What can LLMs do to help in software development?\\n2. How do you measure a code LLM’s performance on coding tasks?\\n3. Which code LLMs are available, both open- and closed-source?\\n4. How does the Reflexion strategy work?\\n5. What options do we have available to establish a feedback loop for writing code?\\n6. What do you think is the impact of generative AI on software development?'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 224}, page_content='Developing Software with Generative AI202\\nJoin our community on Discord\\nJoin our community’s Discord space for discussions with the authors and other readers:\\nhttps://packt.link/lang'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 225}, page_content='7\\nLLMs for Data Science\\nThis chapter is about how generative AI can automate data science. Generative AI, in particular \\nLLMs, has the potential to accelerate scientific progress across various domains, especially by \\nproviding efficient analysis of research data and aiding in literature review processes. A lot of the \\ncurrent approaches that fall within the domain of Automated Machine Learning (AutoML) can \\nhelp data scientists increase their productivity and make data science processes more repeatable. \\nIn this chapter, we’ll first discuss how data science is affected by generative AI and then cover an \\noverview of automation in data science.\\nNext, we’ll discuss how we can use code generation and tools in diverse ways to answer questions \\nrelated to data science. This can come in the form of doing a simulation or enriching our dataset \\nwith additional information. Finally, we’ll shift the focus to the exploratory analysis of structured \\ndatasets. We can set up agents to run SQL or tabular data in pandas. We’ll see how we can ask \\nquestions about the dataset, statistical questions about the data, or ask for visualizations.\\nThroughout the chapter, we’ll work on different approaches to doing data science with LLMs, \\nwhich you can find in the data_science directory in the GitHub repository for this book at \\nhttps://github.com/benman1/generative_ai_with_langchain.\\nThe main sections in this chapter are:\\n• The impact of generative models on data science\\n• Automated data science\\n• Using agents to answer data science questions\\n• Data exploration with LLMs'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 226}, page_content='LLMs for Data Science204\\nBefore delving into how data science can be automated, let’s start by discussing how generative \\nAI will impact data science!\\nThe impact of generative models on data science\\nGenerative AI and LLMs like GPT-4 have brought about significant changes in the field of data \\nscience and analysis. These models, particularly LLMs, can revolutionize all the steps involved in \\ndata science in many ways, offering exciting opportunities for researchers and analysts. Generative \\nAI models, such as ChatGPT, can understand and generate human-like responses, making them \\nvaluable tools for enhancing research productivity.\\nGenerative AI plays a crucial role in analyzing and interpreting research data. These models can \\nassist in data exploration, uncover hidden patterns or correlations, and provide insights that \\nmay not be apparent through traditional methods. By automating certain aspects of data anal-\\nysis, generative AI saves time and resources, allowing researchers to focus on higher-level tasks.\\nAnother area where generative AI can benefit researchers is in performing literature reviews and \\nidentifying research gaps. ChatGPT and similar models can summarize vast amounts of informa-\\ntion from academic papers or articles, providing a concise overview of existing knowledge. This \\nhelps researchers identify gaps in the literature and guide their own investigations more efficiently. \\nWe’ve looked at this aspect of using generative AI models in Chapter 4, Building Capable Assistants.\\nOther data science use cases for generative AI are:\\n• Automatically generating  synthetic data: Generative AI can be used to automatically \\ngenerate synthetic data that can be used to train machine learning models. This can be \\nhelpful for businesses that do not have access to enormous amounts of real-world data.\\n• Identifying patterns in data: Generative AI can be used to identify patterns in data that \\nwould not be visible to human analysts. This can be helpful for businesses that are looking \\nto gain new insights from their data.\\n• Creating new features from existing data: Generative AI can be used to create new fea -\\ntures from existing data. This can be helpful for businesses that are looking to improve \\nthe accuracy of their machine learning models.\\nAccording to recent reports by the likes of McKinsey and KPMG, the consequences of AI relate to \\nwhat data scientists will work on, how they will work, and who can work on data science tasks. \\nThe principal areas of key impact include:'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 227}, page_content='Chapter 7 205\\n• Democratization of AI: Generative models allow many more people to leverage AI by \\ngenerating text, code, and data from simple prompts. This expands the use of AI beyond \\ndata scientists.\\n• Increased productivity: By auto-generating code, data, and text, generative AI can accel-\\nerate development and analysis workflows. This allows data scientists and analysts to \\nfocus on higher-value tasks.\\n• Innovation in data science: Generative AI is bringing about the ability to explore data in \\nnew and more creative ways, and generate new hypotheses and insights that would not \\nhave been possible with traditional methods\\n• Disruption of industries: New applications of generative AI could disrupt industries by \\nautomating tasks or enhancing products and services. Data teams will need to identify \\nhigh-impact use cases.\\n• Limitations remain: Current models still have accuracy limitations, bias issues, and lack \\nof controllability. Data experts are needed to oversee responsible development.\\n• Importance of governance: Rigorous governance over development and ethical use of \\ngenerative AI models will be critical to maintaining stakeholder trust.\\n• Changes to data science skills: Demand may shift from coding expertise to abilities in data \\ngovernance, ethics, translating business problems, and overseeing AI systems.\\nRegarding the democratization and innovation of data science, more specifically, generative AI \\nis also having an impact on the way that data is visualized. In the past, data visualizations were \\noften static and two-dimensional. However, generative AI can be used to create interactive and \\nthree-dimensional visualizations that can help to make data more accessible and understandable. \\nThis is making it easier for people to understand and interpret data, which can lead to better \\ndecision-making.\\nAgain, one of the biggest changes that generative AI is bringing about is the democratization of data \\nscience. In the past, data science was a very specialized field that required a deep understanding \\nof statistics and machine learning. However, generative AI is making it possible for people with \\nless technical expertise to create and use data models. This is opening up the field of data science \\nto a much wider range of people.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 228}, page_content='LLMs for Data Science206\\nLLMs and generative AI can play a crucial role in automated data science by offering several \\nbenefits:\\n• Natural language interaction: LLMs allow for natural language interaction, enabling users \\nto communicate with the model using plain English or other languages. This makes it \\neasier for non-technical users to interact with and explore the data using everyday lan-\\nguage, without requiring expertise in coding or data analysis.\\n• Code generation: Generative AI can automatically generate code snippets to perform spe-\\ncific analysis tasks during Exploratory Data Analysis (EDA). For example, it can generate \\ncode such as SQL to retrieve data, clean data, handle missing values, or create visualiza -\\ntions. This feature saves time and reduces the need for manual coding.\\n• Automated report generation: LLMs can generate automated reports summarizing the \\nkey findings of EDA. These reports provide insights into various aspects of the dataset, \\nsuch as statistical summary, correlation analysis, feature importance, and so on, making \\nit easier for users to understand and present their findings.\\n• Data exploration and visualization: Generative AI algorithms can explore large datasets \\ncomprehensively and generate visualizations that reveal underlying patterns, relationships \\nbetween variables, outliers, or anomalies in the data automatically. This helps users gain \\na holistic understanding of the dataset without manually creating each visualization.\\nFurther, we could think that generative AI algorithms should be able to learn from user interac-\\ntions and adapt their recommendations based on individual preferences or past behaviors. They \\nimprove over time through continuous adaptive learning and user feedback, providing more \\npersonalized and useful insights during automated EDA.\\nFinally, generative AI models can identify errors or anomalies in the data during EDA by learning \\npatterns from existing datasets (intelligent error identification). They can detect inconsistencies \\nand highlight potential issues quickly and accurately.\\nOverall, LLMs and generative AI can enhance automated EDA by simplifying user interaction, \\ngenerating code snippets, identifying errors/anomalies efficiently, automating report generation, \\nfacilitating comprehensive data exploration, visualization creation, and adapting to user prefer-\\nences for more effective analysis of large and complex datasets.\\nHowever, while these models offer immense potential to enhance research and aid in literature \\nreview processes, they should not be treated as infallible sources. As we’ve seen earlier, LLMs work \\nby analogy and struggle with reasoning and math. Their strength is creativity, not accuracy, and \\ntherefore, researchers must exercise critical thinking and ensure that the outputs generated by \\nthese models are accurate, unbiased, and aligned with rigorous scientific standards.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 229}, page_content='Chapter 7 207\\nOne notable example is Microsoft’s Fabric, which incorporates a chat interface powered by gen-\\nerative AI. This allows users to ask data-related questions using natural language and receive \\ninstant answers without having to wait in a data request queue. By leveraging LLMs like OpenAI \\nmodels, Fabric enables real-time access to valuable insights.\\nFabric stands out among other analytics products due to its comprehensive approach. It addresses \\nvarious aspects of an organization’s analytics needs and provides role-specific experiences for \\ndifferent teams involved in the analytics process, such as data engineers, warehousing profes -\\nsionals, scientists, analysts, and business users.\\nWith the integration of Azure OpenAI Service at every layer, Fabric harnesses generative AI’s power \\nto unlock the full potential of data. Features like Copilot in Microsoft Fabric provide conversational \\nlanguage experiences, allowing users to create dataflows, generate code or entire functions, build \\nmachine learning models, visualize results, and even develop custom conversational language \\nexperiences.\\nChatGPT (and Fabric in extension) often produces incorrect SQL queries. This is fine when used by \\nanalysts who can check the validity of the output but a total disaster as a self-service analytics tool \\nfor non-technical business users. Therefore, organizations must ensure that they have reliable data \\npipelines in place and employ data quality management practices while using Fabric for analysis.\\nWhile the possibilities of generative AI in data analytics are promising, caution must be exercised. \\nThe reliability and accuracy of LLMs should be verified using first-principles reasoning and rigor-\\nous analysis. While these models have shown their potential in ad hoc analysis, idea generation \\nduring research, and summarizing complex analyses, they may not always be suitable as self-ser-\\nvice analytical tools for non-technical users due to the need for validation by domain experts.\\nAutomated data science\\nData science is a field that combines computer science, statistics, and business analytics to extract \\nknowledge and insights from data. Data scientists use a variety of tools and techniques to collect, \\nclean, analyze, and visualize data. They then use this information to help businesses make better \\ndecisions. The responsibilities of a data scientist are wide-ranging and often involve multiple \\nsteps that vary depending on the specific role and industry. Tasks include data collecting, clean-\\ning, analyzing, and visualizing. Data scientists are also tasked with building predictive models \\nto help in decision-making processes. All the tasks mentioned are crucial to data science but can \\nbe time-consuming and complex.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 230}, page_content='LLMs for Data Science208\\nAutomating various aspects of the data science workflow allows data scientists to focus more on \\ncreative problem-solving while enhancing productivity. Recent tools are making different stages \\nof the process more efficient by enabling faster iterations and less manual coding for common \\nworkflows. Some of the tasks for data science overlap with those of a software developer that we \\ntalked about in Chapter 6, Developing Software with Generative AI, namely, writing and deploying \\nsoftware, although with a narrower focus, on models.\\nData science platforms like KNIME, H2O, and RapidMiner provide unified analytics engines to \\npreprocess data, extract features, and build models. LLMs integrated into these platforms, such \\nas GitHub Copilot or Jupyter AI, can generate code for data processing, analysis, and visualiza -\\ntion based on natural language prompts. Jupyter AI allows conversing with a virtual assistant to \\nexplain code, identify errors, and create notebooks. \\nThis screenshot from the documentation shows the chat feature, the Jupyternaut chat (Jupyter AI):\\nFigure 7.1: Jupyter AI – Jupyternaut chat'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 231}, page_content='Chapter 7 209\\nIt should be plain to see that having a chat like that at your fingertips to ask questions, create \\nsimple functions, or change existing functions can be a boon to data scientists.\\nOverall, automated data science can accelerate analytics and ML application development. It \\nallows data scientists to focus on higher-value and creative aspects of the process. Democratizing \\ndata science for business analysts is also a key motivation behind automating these workflows. In \\nthe following sections, we’ll investigate different tasks in turn, and we’ll highlight how generative \\nAI can contribute to improving the workflow and create efficiency gains in areas such as data \\ncollection, visualization and EDA , preprocessing and feature engineering, and finally, AutoML. \\nLet’s look at each of these areas in more detail.\\nData collection\\nAutomated data collection is the process of collecting data without human intervention. Automatic \\ndata collection can be a valuable tool for businesses. It can help businesses to collect data more \\nquickly and efficiently, and it can free up human resources to focus on other tasks.\\nIn the context of data science or analytics, we refer to ETL (extract, transform, and load) as the \\nprocess that not only takes data from one or more sources (data collection) but also prepares it \\nfor specific use cases.\\nThere are many ETL tools, including commercial ones such as AWS Glue, Google Dataflow, Am-\\nazon Simple Workflow Service (SWF), dbt, Fivetran, Microsoft SSIS, IBM InfoSphere DataStage, \\nTalend Open Studio, or open-source tools such as Airflow, Kafka, and Spark. In Python, there are \\nmany more tools (too many to list them all), such as pandas for data extraction and processing, \\nand even celery and joblib, which can serve as ETL orchestration tools.\\nIn LangChain, there’s an integration with Zapier, which is an automation tool that can be used \\nto connect different applications and services. This can be used to automate the process of data \\ncollection from a variety of sources. LLMs offer an accelerated way to gather and process data, \\nnotably excelling in the organization of unstructured datasets.\\nThe best tool for automatic data collection will depend on the specific needs of the business. \\nBusinesses should consider the type of data they need to collect, the volume of data they need to \\ncollect, and the budget they have available.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 232}, page_content='LLMs for Data Science210\\nVisualization and EDA\\nEDA involves manually exploring and summarizing data to understand its various aspects before \\nperforming machine learning tasks. It helps in identifying patterns, detecting inconsistencies, \\ntesting assumptions, and gaining insights. However, with the advent of large datasets and the \\nneed for efficient analysis, automated EDA has become important.\\nAutomated EDA and visualization refer to the process of using software tools and algorithms to \\nautomatically analyze and visualize data, without significant manual intervention. These tools \\nprovide several benefits. They can speed up the data analysis process, reducing the time spent \\non tasks like data cleaning, handling missing values, outlier detection, and feature engineering. \\nThese tools also enable the more efficient exploration of complex datasets by generating inter -\\nactive visualizations that provide a comprehensive overview of the data.\\nThe use of generative AI in data visualization adds another dimension to automated EDA by gen-\\nerating new visualizations based on user prompts, making the visualization and interpretation \\nof data even more accessible.\\nPreprocessing and feature extraction\\nAutomated data preprocessing can include tasks such as data cleaning, data integration, data \\ntransformation, and feature extraction. It is related to the transform step in ETL, so there’s a lot of \\noverlap in tools and techniques. Automated feature engineering, on the other hand, is becoming \\nessential to leveraging the full power of ML algorithms on complex real-world data. This includes \\nremoving errors and inconsistencies from the data and converting it into a format compatible \\nwith the analytical tools that will be used.\\nDuring preprocessing and feature engineering, LLMs automate the cleaning, integration, and \\ntransformation of data. The adoption of these models promises to streamline processes, thereby \\nimproving privacy management by minimizing human handling of sensitive information during \\nthese stages. While boosting flexibility and performance in preprocessing tasks, there remains a \\nchallenge in ensuring the safety and interpretability of automatically engineered features, which \\nmay not be as transparent as manually created ones. The gains in efficiency must not undermine \\nthe need for checks against introducing inadvertent biases or errors through automation.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 233}, page_content='Chapter 7 211\\nAutoML\\nAutoML frameworks represent a noteworthy leap in the evolution of machine learning. By stream-\\nlining the complete model development cycle, including tasks such as data cleaning, feature selec-\\ntion, model training, and hyperparameter tuning, AutoML frameworks significantly economize on \\nthe time and effort customarily expended by data scientists. These frameworks not only enhance \\nthe pace but also potentially elevate the quality of machine learning models.\\nThe basic idea of AutoML is illustrated in this diagram from the GitHub repo of the mljar AutoML \\nlibrary (source: https://github.com/mljar/mljar-supervised):\\nFigure 7.2: How AutoML works\\nKey to the value offered by AutoML systems is their contributory effect on ease of use and pro -\\nductivity growth. Within typical developer environments, these systems enable the rapid iden -\\ntification and productionizing of machine learning models, simplifying both comprehension \\nand deployment processes. The genesis of these frameworks can be traced back to innovations \\nlike Auto-WEKA. As one of the early broad-framework attempts, developed at the University of \\nWaikato, it was penned in Java to automate the process for tabular data within the Weka machine \\nlearning suite.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 234}, page_content='LLMs for Data Science212\\nSince the release of Auto-Weka, the landscape has vastly diversified with powerful frameworks \\nsuch as auto-sklearn, autokeras, NASLib, Auto-PyTorch, TPOT, Optuna, AutoGluon, and Ray \\n(tune). Spawning across various programming languages, these frameworks lend themselves \\nto an eclectic array of machine learning tasks. More contemporary AutoML advancements have \\nharnessed neural architecture search techniques to encapsulate vast portions of the ML pipeline, \\nincluding unstructured data types like images, video, and audio. Solutions like Google AutoML, \\nAzure AutoML, and H2O’s offering are at the forefront of this revolution, delivering capabilities \\nthat extend ML accessibility to individuals beyond expert data scientists.\\nThese modern solutions are equipped to adeptly deal with structured formats such as tables \\nand time series. By conducting elaborate hyperparameter searches, their performance can meet \\nor even surpass manual interventions. Frameworks such as PyCaret facilitate training multiple \\nmodels concurrently with minimal code while maintaining a focus on time series data through \\nspecialized projects like Nixtla’s StatsForecast and MLForecast.\\nThe attributes characterizing AutoML frameworks are manifold: they provide deployment ca -\\npacities wherein certain solutions enable direct production embedding, especially cloud-based \\nones; others necessitate exportation in formats compatible with platforms like TensorFlow. The \\ndiversity in the data types handled is another facet, with a concentrated focus on tabular data-\\nsets alongside deep learning frameworks catering to assorted data varieties. Several frameworks \\nhighlight explainability as a paramount feature – this is particularly pertinent where regulations \\nor reliability are at stake in industries like healthcare and finance. Monitoring post-deployment \\nis another operational feature to ensure sustained model performance over time.\\nDespite recent advancements, users are confronted with typical drawbacks associated with such \\nautomated systems. A “black-box” scenario emerges quite frequently yielding difficulties in com-\\nprehending the internal workings, which can impede problem debugging within AutoML models. \\nMoreover, while their impact through time savings and democratization of ML practices makes \\nmachine learning more accessible for those without extensive experience, their efficacy in auto-\\nmating ML tasks can face limitations due to inherent task complexities.\\nAutoML has been revitalized with the inclusion of LLMs, as they bring automation to tasks such as \\nfeature selection, model training, and hyperparameter tuning. The impact on privacy is consider-\\nable; AutoML systems that utilize generative models can create synthetic data, reducing reliance \\non personal data repositories. In terms of safety, automated systems must be designed with fail-\\nsafe mechanisms to prevent the propagation of errors across successive layers of ML workflows. \\nThe flexibility offered by AutoML through LLM integration improves competitive performance \\nby making it possible for non-experts to achieve expert-level model tuning. '),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 235}, page_content='Chapter 7 213\\nWith respect to ease of use, while AutoML with integrated LLMs offers simplified interfaces for \\nmodel development pipelines, users must grapple with complex choices regarding model selec-\\ntion and evaluation.\\nAs we’ll see in the next couple of sections, LLMs and tools can significantly accelerate data science \\nworkflows, reduce manual effort, and open up new analysis opportunities. As we’ve seen with \\nJupyter AI (Jupyternaut chat) – and in Chapter 6, Developing Software with Generative AI – there’s a \\nlot of potential to increase efficiency by creating software with generative AI (code LLMs). This is \\na good starting point for the practical part of this chapter as we investigate the use of generative \\nAI in data science. Let’s start to use agents to run code or call other tools to answer questions!\\nUsing agents to answer data science questions\\nTools like LLMMathChain can be utilized to execute Python for answering computational queries. \\nWe’ve already seen different agents with tools before.\\nFor instance, by chaining LLMs and tools, one can calculate mathematical powers and obtain \\nresults effortlessly:\\nfrom langchain import OpenAI, LLMMathChain\\nllm = OpenAI(temperature=0)\\nllm_math = LLMMathChain.from_llm(llm, verbose=True)\\nllm_math.run(\"What is 2 raised to the 10th power?\")\\nWe should see something like this:\\n> Entering new LLMMathChain chain...\\nWhat is 2 raised to the 10th power?\\n2**10\\nnumexpr.evaluate(\"2**10\")\\nAnswer: 1024\\n> Finished chain.\\n[2]:\\'Answer: 1024\\'\\nSuch capabilities, while adept at delivering straightforward numerical answers, are not as straight-\\nforward to integrate into conventional EDA workflows. Other chains, like CPAL ( CPALChain) \\nand PAL ( PALChain), can tackle more complex reasoning challenges, mitigating the risks of \\ngenerative models producing implausible content; yet their practical applications remain elusive \\nin real-world scenarios.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 236}, page_content='LLMs for Data Science214\\nWith PythonREPLTool, we can create simple visualizations of toy data or train with synthetic \\ndata, which can be nice for illustration or bootstrapping a project. This is an example from the \\nLangChain documentation:\\nfrom langchain.agents.agent_toolkits import create_python_agent\\nfrom langchain.tools.python.tool import PythonREPLTool\\nfrom langchain.llms.openai import OpenAI\\nfrom langchain.agents.agent_types import AgentType\\nagent_executor = create_python_agent(\\n    llm=OpenAI(temperature=0, max_tokens=1000),\\n    tool=PythonREPLTool(),\\n    verbose=True,\\n    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\\n)\\nagent_executor.run(\\n    \"\"\"Understand, write a single neuron neural network in PyTorch.\\nTake synthetic data for y=2x. Train for 1000 epochs and print every 100 \\nepochs.\\nReturn prediction for x = 5\"\"\"\\n)\\nThis demonstrates constructing a single-neuron neural network using PyTorch, training it with \\nsynthetic data, and making predictions – all performed directly on the user’s machine. However, \\ncaution is advised as executing Python code without safeguards can pose security risks.\\nWe get this output back, which includes a prediction:\\nEntering new AgentExecutor chain...\\nI need to write a neural network in PyTorch and train it on the given data\\nAction: Python_REPL\\nAction Input:\\nimport torch\\nmodel = torch.nn.Sequential(\\n    torch.nn.Linear(1, 1)\\n)\\nloss_fn = torch.nn.MSELoss()\\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\\n# Define the data\\nx_data = torch.tensor([[1.0], [2.0], [3.0], [4.0]])'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 237}, page_content=\"Chapter 7 215\\ny_data = torch.tensor([[2.0], [4.0], [6.0], [8.0]])\\nfor epoch in range(1000):  # Train the model\\n    y_pred = model(x_data)\\n    loss = loss_fn(y_pred, y_data)\\n    if (epoch+1) % 100 == 0:\\n        print(f'Epoch {epoch+1}: {loss.item():.4f}')\\n    optimizer.zero_grad()\\n    loss.backward()\\n    optimizer.step()\\n# Make a prediction\\nx_pred = torch.tensor([[5.0]])\\ny_pred = model(x_pred)\\nObservation: Epoch 100: 0.0043\\nEpoch 200: 0.0023\\nEpoch 300: 0.0013\\nEpoch 400: 0.0007\\nEpoch 500: 0.0004\\nEpoch 600: 0.0002\\nEpoch 700: 0.0001\\nEpoch 800: 0.0001\\nEpoch 900: 0.0000\\nEpoch 1000: 0.0000\\nThought: I now know the final answer\\nFinal Answer: The prediction for x = 5 is y = 10.00.\\nThrough iterative training displayed in verbose logs, users witness the progressive reduction of \\nloss over epochs until a satisfactory prediction is attained. Despite this showcasing how a neural \\nnetwork learns and predicts over time, scaling this approach in practice would necessitate more \\nsophisticated engineering efforts.\\nLLMs and tools can be useful if we want to enrich our data with category or geographic informa-\\ntion. For example, if our company offers flights from Tokyo, and we want to know the distances \\nof our customers from Tokyo, we can use WolframAlpha as a tool. Here’s a simplistic example:\\nfrom langchain.agents import load_tools, initialize_agent\\nfrom langchain.llms import OpenAI\"),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 238}, page_content='LLMs for Data Science216\\nfrom langchain.chains.conversation.memory import ConversationBufferMemory\\nllm = OpenAI(temperature=0)\\ntools = load_tools([\\'wolfram-alpha\\'])\\nmemory = ConversationBufferMemory(memory_key=\"chat_history\")\\nagent = initialize_agent(tools, llm, agent=\"conversational-react-\\ndescription\", memory=memory, verbose=True)\\nagent.run(\\n    \"\"\"How far are these cities to Tokyo?\\n* New York City\\n* Madrid, Spain\\n* Berlin\\n\"\"\")\\nPlease make sure you’ve set the OPENAI_API_KEY and WOLFRAM_ALPHA_APPID environment variables \\nas discussed in Chapter 3, Getting Started with LangChain. Here’s the output:\\n> Entering new AgentExecutor chain...\\nAI: The distance from New York City to Tokyo is 6760 miles. The distance \\nfrom Madrid, Spain to Tokyo is 8,845 miles. The distance from Berlin, \\nGermany to Tokyo is 6,845 miles.\\n> Finished chain.\\n\\'\\nThe distance from New York City to Tokyo is 6760 miles. The distance from \\nMadrid, Spain to Tokyo is 8,845 miles. The distance from Berlin, Germany \\nto Tokyo is 6,845 miles.\\nBy combining LLMs with external tools like WolframAlpha, it’s possible to perform more challeng-\\ning data enrichment, such as calculating distances between cities, such as from Tokyo to New York \\nCity, Madrid, or Berlin. Such integrations could significantly enhance the utility of datasets used \\nin various business applications. Nonetheless, these examples address relatively straightforward \\nqueries; deploying such implementations on a larger scale demands more extensive engineering \\nstrategies beyond those discussed.\\nHowever, we can give agents datasets to work with, and here is where it can get immensely pow-\\nerful when we connect more tools. Let’s ask and answer questions about structured datasets!'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 239}, page_content='Chapter 7 217\\nData exploration with LLMs\\nData exploration is a crucial and foundational step in data analysis, allowing researchers to gain a \\ncomprehensive understanding of their datasets and uncover significant insights. With the emer-\\ngence of LLMs like ChatGPT, researchers can harness the power of natural language processing \\nto facilitate data exploration.\\nAs we mentioned earlier, generative AI models such as ChatGPT have the ability to understand and \\ngenerate human-like responses, making them valuable tools for enhancing research productivity. \\nAsking our questions in natural language and getting responses in digestible pieces and shapes \\ncan be a great boost to analysis.\\nLLMs can help explore textual data and other forms of data, such as numerical datasets or multi-\\nmedia content. Researchers can leverage ChatGPT’s capabilities to ask questions about statistical \\ntrends in numerical datasets or even query visualizations for image classification tasks.\\nLet’s load up a dataset and work with that. We can quickly get a dataset from scikit-learn:\\nfrom sklearn.datasets import load_iris\\ndf = load_iris(as_frame=True)[\"data\"]\\nThe Iris dataset is well known – it’s a toy dataset, but it will help us illustrate the capabilities of \\nusing generative AI for data exploration. We’ll use a DataFrame in the following example. We \\ncan create a pandas DataFrame agent now and we’ll see how easy it is to get simple stuff done!\\nfrom langchain.agents import create_pandas_dataframe_agent\\nfrom langchain import PromptTemplate\\nfrom langchain.llms.openai import OpenAI\\nPROMPT = (\\n    \"If you do not know the answer, say you don\\'t know.\\\\n\"\\n    \"Think step by step.\\\\n\"\\n    \"\\\\n\"\\n    \"Below is the query.\\\\n\"\\n    \"Query: {query}\\\\n\"\\n)\\nprompt = PromptTemplate(template=PROMPT, input_variables=[\"query\"])\\nllm = OpenAI()\\nagent = create_pandas_dataframe_agent(llm, df, verbose=True)'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 240}, page_content='LLMs for Data Science218\\nI’ve included instructions for the model to indicate uncertainty and to follow a step-by-step \\nthought process, with the aim of reducing hallucinations. Now we can query our agent against \\nthe DataFrame:\\nagent.run(prompt.format(query=\"What\\'s this dataset about?\"))\\nWe get the answer This dataset is about the measurements of some type of flower , \\nwhich is correct.\\nLet’s show how to get a visualization:\\nagent.run(prompt.format(query=\"Plot each column as a barplot!\"))\\nHere is the plot:\\nFigure 7.3: Iris dataset barplots\\nThe plot is not perfect. The output can be finicky and depends on the llm model parameter and \\non the instructions. In this case, I used df.plot.bar(rot=0, subplots=True). We might want \\nto introduce more tweaks, for example, to padding between the panels, the font size, or the \\nplacement of the legend, to make this really nice.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 241}, page_content=\"Chapter 7 219\\nWe can also ask to see the distributions of the columns visually, which will give us this neat plot:\\nFigure 7.4: Iris dataset boxplots\\nWe can request the plot to use other plotting backends, such as Seaborn; however, please note \\nthat these have to be installed.\\nWe can also ask more questions about the dataset, like which row has the biggest difference \\nbetween petal length and petal width. We get the answer with the intermediate steps as follows \\n(shortened):\\ndf['difference'] = df['petal length (cm)'] - df['petal width (cm)']\\ndf.loc[df['difference'].idxmax()]\\nObservation: sepal length (cm)    7.7\\nsepal width (cm)     2.8\\npetal length (cm)    6.7\\npetal width (cm)     2.0\\ndifference           4.7\\nName: 122, dtype: float64\\nThought: I now know the final answer\\nFinal Answer: Row 122 has the biggest difference between petal length and \\npetal width.\\nI think that’s worth a pat on the back, LLM!\"),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 242}, page_content='LLMs for Data Science220\\nWe could extend this example by adding more instructions to the prompt about plotting, such \\nas the sizes of plots.\\nIt’s a bit harder to implement the same plotting logic in a Streamlit app, because we need to use \\nthe plotting functionality in corresponding Streamlit functions, for example, st.bar_chart(). \\nHowever, this can be done as well. You can find explanations for this on the Streamlit blog (Building \\na Streamlit and scikit-learn app with ChatGPT).\\nWhat about statistical tests?\\nagent.run(prompt.format(query=\"Validate the following hypothesis \\nstatistically: petal width and petal length come from the same \\ndistribution.\"))\\nWe get this response:\\nThought: I should use a statistical test to answer this question.\\nAction: python_repl_ast\\nAction Input: from scipy.stats import ks_2samp\\nObservation:\\nThought: I now have the necessary tools to answer this question.\\nAction: python_repl_ast\\nAction Input: ks_2samp(df[\\'petal width (cm)\\'], df[\\'petal length (cm)\\'])\\nObservation: KstestResult(statistic=0.6666666666666666, \\npvalue=6.639808432803654e-32, statistic_location=2.5, statistic_sign=1)\\nThought: I now know the final answer\\nFinal Answer: The p-value of 6.639808432803654e-32 indicates that the two \\nvariables come from different distributions.\\nThat checks off the statistical test! We can ask complex questions about the dataset with simple \\nprompts in plain English.\\nThere’s also the PandasAI library, which uses LangChain under the hood and provides similar \\nfunctionality. Here’s an example from the documentation with an example dataset:\\nimport pandas as pd\\nfrom pandasai.llm import OpenAI\\nfrom pandasai.schemas.df_config import Config\\nfrom pandasai import SmartDataframe\\ndf = pd.DataFrame({'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 243}, page_content='Chapter 7 221\\n    \"country\": [\"United States\", \"United Kingdom\", \"France\", \"Germany\", \\n\"Italy\", \"Spain\", \"Canada\", \"Australia\", \"Japan\", \"China\"],\\n    \"gdp\": [19294482071552, 2891615567872, 2411255037952, 3435817336832, \\n1745433788416, 1181205135360, 1607402389504, 1490967855104, 4380756541440, \\n14631844184064],\\n    \"happiness_index\": [6.94, 7.16, 6.66, 7.07, 6.38, 6.4, 7.23, 7.22, \\n5.87, 5.12]\\n})\\nsmart_df = SmartDataframe(df, config=Config(llm=OpenAI()))\\nprint(smart_df.chat(\"Which are the 5 happiest countries?\"))\\nThis will give us the requested result similar to before when we were using LangChain directly. \\nPlease note that PandasAI is not part of the setup for the book, so you’ll have to install it sepa-\\nrately if you want to use it.\\nFor data in SQL databases, we can connect with a SQLDatabaseChain. The LangChain documen-\\ntation shows this example:\\nfrom langchain.llms import OpenAI\\nfrom langchain.utilities import SQLDatabase\\nfrom langchain_experimental.sql import SQLDatabaseChain\\ndb = SQLDatabase.from_uri(\"sqlite:///../../../../notebooks/Chinook.db\")\\nllm = OpenAI(temperature=0, verbose=True)\\ndb_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True)\\ndb_chain.run(\"How many employees are there?\")\\nWe are connecting to a database first. Then we can ask questions about the data in natural language. \\nThis can also be quite powerful. An LLM will create the queries for us. I would expect this to be \\nparticularly useful when we don’t know about the schema of the database. The SQLDatabaseChain \\ncan also check queries and autocorrect them if the use_query_checker option is set.\\nBy following the outlined steps, we have leveraged the impressive natural language processing \\ncapabilities of LLMs for data exploration. Through loading a dataset, such as the Iris dataset \\nfrom scikit-learn, we can use an LLM-powered agent to query about data specifics in accessible, \\neveryday language. The creation of a pandas DataFrame agent enabled simple analysis tasks and \\nvisualization requests, demonstrating the AI’s capacity to produce plots and specific data insights.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 244}, page_content='LLMs for Data Science222\\nWe can not only inquire about the nature of the dataset verbally but also command the agent to \\ngenerate visual representations such as barplots and boxplots for EDA. Although these visualiza-\\ntions might require additional fine-tuning for aesthetic refinement, they established a groundwork \\nfor analysis. When delving into more nuanced requests, such as identifying disparities between \\ntwo data attributes, the agent adeptly added new columns and located pertinent numerical \\ndifferences, showing its practical utility in drawing actionable conclusions.\\nEfforts extended beyond mere visualization as the application of statistical tests was also explored \\nthrough concise English prompts, resulting in articulate interpretations of statistical operations \\nlike KS-tests performed by the agent.\\nThe capabilities of integrations aren’t limited to static datasets but extend to dynamic SQL data-\\nbases where an LLM can automate query generation, even offering autocorrection for syntactical \\nerrors in SQL statements. This capability particularly shines when schemas are unfamiliar.\\nSummary\\nBeginning with an examination of AutoML frameworks, this chapter highlighted the value these \\nsystems bring to the entirety of the data science pipeline, facilitating each stage from data prepa-\\nration to model deployment. We then considered how the integration of LLMs can further elevate \\nproductivity and make data science more approachable for both technical and non-technical \\nstakeholders.\\nDiving into code generation, we saw parallels with software development, as discussed in Chapter \\n6, Developing Software with Generative AI, observing how tools and functions generated by LLMs \\ncan respond to queries or enhance datasets through augmentation techniques. This included \\nleveraging third-party tools like WolframAlpha to add external data points to existing datasets. \\nOur exploration then shifted toward the use of LLMs for data exploration, building upon the \\ntechniques for ingesting and analyzing voluminous textual data detailed in Chapter 4, Building \\nCapable Assistants, on question answering. Here, our focus turned to structured datasets, examining \\nhow SQL databases or tabular information could be effectively analyzed through LLM-powered \\nexploratory processes.\\nTo sum up our exploration, it is clear that AI technologies, illustrated by platforms such as ChatGPT \\nplugins and Microsoft Fabric, hold transformative potential for data analysis. However, despite \\nthe remarkable strides in enabling and enhancing the work of data scientists through these AI \\ntools, the current state of AI technology isn’t at a point where it can supplant human experts but \\nrather augments their capabilities and broadens their analytical toolset.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 245}, page_content='Chapter 7 223\\nIn the next chapter, we’ll focus on conditioning techniques to improve the performance of LLMs \\nthrough prompting and fine-tuning.\\nQuestions\\nPlease have a look to see if you can come up with the answers to these questions from memory. \\nI recommend you go back to the corresponding sections of this chapter if you are unsure about \\nany of them:\\n1. What steps are involved in data science?\\n2. Why would we want to automate data science/analysis?\\n3. How can generative AI help data scientists?\\n4. What kind of agents and tools can we use to answer simple questions?\\n5. How can we get an LLM to work with data?\\nJoin our community on Discord\\nJoin our community’s Discord space for discussions with the authors and other readers:\\nhttps://packt.link/lang'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 246}, page_content=''),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 247}, page_content='8\\nCustomizing LLMs and Their \\nOutput\\nThis chapter is about techniques and best practices to improve the reliability and performance \\nof LLMs in certain scenarios, such as complex reasoning and problem-solving tasks. This pro -\\ncess of adapting a model for a certain task or making sure that our model output corresponds to \\nwhat we expect is called conditioning. In this chapter, we’ll discuss fine-tuning and prompting \\nas methods for conditioning.\\nFine-tuning involves training the pre-trained base model on specific tasks or datasets relevant \\nto the desired application. This process allows the model to adapt, becoming more accurate and \\ncontextually relevant for the intended use case.\\nOn the other hand, by providing additional input or context at inference time, LLMs can gen -\\nerate text tailored to a particular task or style. Prompt engineering is significant in unlocking \\nLLM reasoning capabilities, and prompt techniques form a valuable toolkit for researchers and \\npractitioners working with LLMs. We’ll discuss and implement advanced prompt engineering \\nstrategies like few-shot learning, tree-of-thought, and self-consistency.\\nThroughout the chapter, we’ll work on fine-tuning and prompting with LLMs. You can find the \\ncorresponding code in the GitHub repository for the book at https://github.com/benman1/\\ngenerative_ai_with_langchain'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 248}, page_content='Customizing LLMs and Their Output226\\nThe main sections in this chapter are:\\n• Conditioning LLMs\\n• Fine-tuning\\n• Prompt engineering\\nLet’s start by discussing conditioning, why it’s important, and how we can achieve it.\\nConditioning LLMs\\nPre-training an LLM on diverse data to learn patterns of language results in a base model that \\nhas a broad understanding of diverse topics. While base models such as GPT-4 can generate \\nimpressive text on a wide range of topics, conditioning them can enhance their capabilities in \\nterms of task relevance, specificity, and coherence, and can guide the model’s behavior to be in \\nline with what is considered ethical and appropriate. In this chapter, we’ll focus on fine-tuning \\nand prompt techniques as two methods of conditioning.\\nConditioning techniques enable LLMs to comprehend and execute complex instructions, deliv -\\nering content that closely matches our expectations. This ranges from off-the-cuff interactions \\nto systematic training that orients a model’s behavior toward reliable performance in specialist \\ndomains, like legal consultation or technical documentation. Furthermore, part of conditioning \\nincludes implementing safeguards to avert the production of malicious or harmful content, such \\nas incorporating filters or training the model to avoid certain types of problematic outputs, thereby \\nbetter aligning it with desired ethical standards.\\nConditioning refers to a collection of methods used to direct the model’s generation \\nof outputs. This includes not only prompt crafting but also more systemic techniques, \\nsuch as fine-tuning the model on specific datasets to adapt its responses to certain \\ntopics or styles persistently.\\nAlignment refers to the process and goal of training and modifying LLMs so that \\ntheir general behavior, decision-making processes, and outputs conform to broader \\nhuman values, ethical principles, and safety considerations.\\nThe two terms are not synonymous; while conditioning can include fine-tuning and \\nis focused on influencing the model through various techniques at different layers of \\ninteraction, alignment is concerned with the fundamental and holistic calibration \\nof the model’s behavior to human ethics and safety standards.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 249}, page_content='Chapter 8 227\\nConditioning can be applied at different points in a model’s life cycle. One strategy involves \\nfine-tuning the model on data that represents the intended use case to help the model specialize \\nin that area. This method depends on the availability of such data and the ability to integrate it \\ninto the training process. Another method involves conditioning the model dynamically at the \\ntime of inference, where the input prompt is tailored with additional context to shape the desired \\noutput. This approach offers flexibility but can add complexity to the model’s operation in live \\nenvironments.\\nIn the next section, I will summarize key methods for conditioning such as fine-tuning and prompt \\nengineering, discuss the rationale, and examine their relative pros and cons.\\nMethods for conditioning\\nWith the advent of large pre-trained language models like GPT-3, there has been growing interest \\nin techniques to adapt these models for downstream tasks. As LLMs continue to develop, they \\nwill become even more effective and useful for a broader range of applications, and we can expect \\nfuture advancements in fine-tuning and prompting techniques to help go even further in complex \\ntasks that involve reasoning and tool use.\\nSeveral approaches have been proposed for conditioning. Here is a table summarizing the dif -\\nferent techniques:\\nStage Technique Examples\\nTraining\\nData curation Training on diverse data\\nObjective function Careful design of training objective\\nArchitecture and training \\nprocess Optimizing model structure and training\\nFine-tuning Task specialization Training on specific datasets/tasks\\nInference-time \\nconditioning Dynamic inputs\\nPrefixes, control codes, and context \\nexamples\\nHuman oversight Human-in-the-loop Human review and feedback\\nTable 8.1: Steering generative AI outputs\\nCombining these techniques provides developers with more control over the behavior and outputs \\nof generative AI systems. The ultimate goal is to ensure that human values are incorporated at all \\nstages, from training to deployment, to create responsible and aligned AI systems.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 250}, page_content='Customizing LLMs and Their Output228\\nIn this chapter, we emphasize fine-tuning and prompting, as they stand out for their effective -\\nness and prevalence in the conditioning of LLMs. Fine-tuning involves adjusting all parameters \\nof a pre-trained model through additional training on specialized tasks. This method is aimed \\nat enhancing model performance for particular objectives and is known to yield robust results. \\nHowever, fine-tuning can be resource-intensive, presenting a trade-off between high performance \\nand computational efficiency. To address these limitations, we explore strategies like adapters \\nand Low-Rank Adaptation (LoRA), which introduce elements of sparsity or implement partial \\nfreezing of parameters to lighten the burden.\\nPrompt-based techniques, on the other hand, offer a way to dynamically condition LLMs at infer-\\nence time. Through careful crafting of input prompts and subsequent optimization and evaluations, \\nthese methods can steer the behavior of LLMs in desired directions without the need for heavy \\nretraining. Prompts can be carefully designed to elicit specific behaviors or to encapsulate partic-\\nular knowledge areas, providing a versatile and resource-savvy approach to model conditioning.\\nMoreover, we delve into the transformative role of Reinforcement Learning with Human Feed-\\nback (RLHF) within fine-tuning processes, where human feedback serves as a critical guide for \\nthe model’s learning trajectory. RLHF has exhibited the potential to profoundly improve the capa-\\nbilities of language models like GPT-3, making fine-tuning an even more impactful technique. By \\nintegrating RLHF, we harness the nuanced understanding of human evaluators to further refine \\nthe model behavior, ensuring outputs that are not only relevant and accurate but also align with \\nuser intent and expectations.\\nAll these different techniques for conditioning facilitate the development of LLMs that are both \\nhigh-performing and aligned with desired outcomes across various applications. Let’s start off \\nby discussing the reasons why InstructGPT, which was trained through RLHF, has had such a \\ntransformative impact.\\nReinforcement learning with human feedback\\nIn their March 2022 paper, Ouyang and others from OpenAI demonstrated using RLHF with \\nProximal Policy Optimization (PPO) to align LLMs, like GPT-3, with human preferences.\\nRLHF is an online approach that fine-tunes LMs using human preferences. It has three main steps:\\n1. Supervised pre-training: The LM is first trained via standard supervised learning on \\nhuman demonstrations.\\n2. Reward model training: A reward model is trained on human ratings of LM outputs to \\nestimate a reward.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 251}, page_content='Chapter 8 229\\n3. RL fine-tuning: The LM is fine-tuned via reinforcement learning to maximize the expected \\nreward from the reward model using an algorithm like PPO.\\nThe main change, RLHF, allows incorporating nuanced human judgments into language model \\ntraining through a learned reward model. As a result, human feedback can steer and improve \\nlanguage model capabilities beyond standard supervised fine-tuning. This new model can be \\nused to follow instructions that are given in natural language, and it can answer questions in \\na way that’s more accurate and relevant than GPT-3. InstructGPT outperformed GPT-3 on user \\npreference, truthfulness, and harm reduction, despite having 100x fewer parameters.\\nStarting in March 2022, OpenAI started releasing the GPT-3.5 series models, upgraded versions \\nof GPT-3, which include fine-tuning with RLHF.\\nInstructGPT opened up new avenues to improve language models by incorporating reinforce -\\nment learning from human feedback methods beyond traditional fine-tuning approaches. RL \\ntraining can be unstable and computationally expensive; notwithstanding, its success inspired \\nfurther research into refining RLHF techniques, reducing data requirements for alignment, and \\ndeveloping more powerful and accessible models for a wide range of applications.\\nLow-rank adaptation\\nAs LLMs become larger, it becomes difficult to train them on consumer hardware, and deploying \\nthem for each specific task becomes expensive. There are a few methods that reduce computa-\\ntional, memory, and storage costs while improving performance in low-data and out-of-domain \\nscenarios.\\nParameter-Efficient Fine-Tuning (PEFT) methods enable the use of small checkpoints for each \\ntask, making the models more portable. This small set of trained weights can be added on top of \\nthe LLM, allowing the same model to be used for multiple tasks without replacing the entire model.\\nLow-Rank Adaptation (LoRA) is a type of PEFT, where the pre-trained model weights are frozen. \\nIt introduces trainable rank decomposition matrices into each layer of the Transformer archi -\\ntecture to reduce the number of trainable parameters. LoRA achieves comparable model quality \\ncompared to fine-tuning while having fewer trainable parameters and higher training throughput.\\nThe QLORA method is an extension of LoRA, which enables efficient fine-tuning of large models \\nby backpropagating gradients through a frozen 4-bit quantized model into learnable low-rank \\nadapters. This allows you to fine-tune a 65B parameter model on a single GPU. QLORA models \\nachieve 99% of ChatGPT performance on Vicuna, using innovations like new data types and \\noptimizers. QLORA reduces the memory requirements to fine-tune a 65B parameter model from \\n>780 GB to <48 GB, without affecting runtime or predictive performance.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 252}, page_content='Customizing LLMs and Their Output230\\nIn the next section, we’ll discuss methods to condition LLMs at inference time, which include \\nprompt engineering.\\nInference-time conditioning\\nOne commonly used approach is conditioning at inference time (an output generation phase), \\nwhere specific inputs or conditions are provided dynamically to guide the output generation \\nprocess. LLM fine-tuning may not always be feasible or beneficial in certain scenarios:\\n• Limited fine-tuning services: When models are only accessible through APIs that lack \\nor have restricted fine-tuning capabilities\\n• Insufficient data : In cases where there is a lack of data for fine-tuning, either for the \\nspecific downstream task or relevant application domain\\n• Dynamic data: In cases of applications with frequently changing data, such as news-re-\\nlated platforms, fine-tuning models frequently becomes challenging, leading to potential \\ndrawbacks\\nQuantization refers to techniques to reduce the numerical precision of weights \\nand activations in neural networks like LLMs. The main purpose of quantization is \\nto reduce the memory footprint and computational requirements of large models.\\nSome key points about the quantization of LLMs:\\n• It involves representing weights and activations using fewer bits than a \\nstandard single-precision floating point (FP32). For example, weights could \\nbe quantized to 8-bit integers.\\n• This allows you to shrink a model size by up to 4x and improve throughput \\non specialized hardware.\\n• Quantization typically has a minor impact on model accuracy, especially \\nwith re-training.\\n• Common quantization methods include scalar, vector, and product quan-\\ntization, which quantize weights separately or in groups.\\n• Activations can also be quantized by estimating their distribution and bin -\\nning appropriately.\\n• Quantization-aware training adjusts weights during training to minimize \\nquantization loss.\\n• LLMs like BERT and GPT-3 have been shown to work well with 4–8-bit quan-\\ntization via fine-tuning.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 253}, page_content='Chapter 8 231\\n• Context-sensitive applications: Dynamic and context-specific applications like person-\\nalized chatbots cannot perform fine-tuning based on individual user data\\nFor conditioning at inference time, most commonly, we provide a textual prompt or instruction \\nat the beginning of the text generation process. This prompt can be a few sentences or even a \\nsingle word, acting as an explicit indication of the desired output.\\nSome common techniques for inference-time conditioning include:\\n• Prompt tuning: Providing natural language guidance for intended behavior. Sensitive \\nto prompt design.\\n• Prefix tuning: Prepending trainable vectors to LLM layers.\\n• Constraining tokens: Forcing inclusion/exclusion of certain words\\n• Metadata: Providing high-level info like genre, target audience, and so on\\nPrompts can facilitate generating text that adheres to specific themes, styles, or even mimics a \\nparticular author’s writing style. These techniques involve providing contextual information \\nduring inference time, such as for in-context learning or retrieval augmentation.\\nAn example of prompt tuning is prefixing prompts, where instructions like “Write a child-friendly \\nstory about...” are prepended to the prompt. For example, in chatbot applications, conditioning \\nthe model with user messages helps it generate responses that are personalized and pertinent \\nto the ongoing conversation.\\nFurther examples include prepending relevant documents to prompts to assist LLMs with writing \\ntasks (for example, news reports, Wikipedia pages, and company documents), or retrieving and \\nprepending user-specific data (financial records, health data, and emails) before prompting an \\nLLM to ensure personalized answers. By conditioning LLM outputs on contextual information at \\nruntime, these methods can guide models without relying on traditional fine-tuning processes.\\nOften, demonstrations are part of the instructions for reasoning tasks, where few-shot examples \\nare provided to induce the desired behavior. Powerful LLMs, such as GPT-3, can solve tasks with-\\nout further training through prompting techniques. In this approach, the problem to be solved \\nis presented to the model as a text prompt, with some text examples of similar problems and \\ntheir solutions. The model must provide a completion of the prompt via inference. Zero-shot \\nprompting involves no solved examples, while few-shot prompting includes a small number of \\nexamples of similar (problem and solution) pairs.\\nIt has been shown that prompting provides easy control over large frozen models like GPT-4 and \\nallows steering model behavior without extensive fine-tuning. '),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 254}, page_content='Customizing LLMs and Their Output232\\nPrompting enables conditioning models on new knowledge with low overhead, but careful prompt \\nengineering is needed for the best results. This is what we’ll discuss as part of this chapter.\\nIn prefix tuning, continuous task-specific vectors are trained and supplied to models at inference \\ntime. Similar ideas have been proposed for adapter approaches, such as parameter Efficient \\nTransfer Learning (PELT) or Ladder Side-Tuning (LST).\\nConditioning at inference time can also happen during sampling, such as grammar-based sam -\\npling, where the output can be constrained to be compatible with certain well-defined patterns, \\nsuch as a programming language syntax.\\nIn the next section, we’ll fine-tune a small open-source LLM (OpenLLaMa) for Question Answer-\\ning (QA) with PEFT and quantization, and we’ll deploy it on Hugging Face.\\nFine-tuning\\nAs we discussed in the first section of this chapter, the goal of model fine-tuning for LLMs is to \\noptimize a model to generate outputs that are more specific to a task and context than the orig -\\ninal foundation model.\\nThe need for fine-tuning arises because pre-trained LMs are designed to model general linguistic \\nknowledge, not specific downstream tasks. Their capabilities manifest only when adapted to ap-\\nplications. Fine-tuning allows pre-trained weights to be updated for target datasets and objectives. \\nThis enables knowledge transfer from the general model while customizing it for specialized tasks.\\nIn general, there are three advantages of fine-tuning that are immediately obvious to users of \\nthese models:\\n• Steerability: The capability of models to follow instructions (instruction-tuning)\\n• Reliable output-formatting: This is important, for example, for API calls/function calling)\\n• Custom tone: This makes it possible to adapt the output style as appropriate to the task \\nand audience.\\n• Alignment: The output of models should correspond to core values, for example, con -\\ncerning safety, security, and privacy considerations.\\nThe idea of fine-tuning pre-trained neural networks originated in computer vision research in \\nthe early 2010s. Howard and Ruder (2018) demonstrated the effectiveness of fine-tuning models \\nlike ELMo and ULMFit on downstream tasks. The seminal BERT model (Devlin and others., 2019) \\nestablished fine-tuning of pre-trained transformers as the de facto approach in NLP.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 255}, page_content='Chapter 8 233\\nIn this section, we’ll fine-tune a model for question answering. This recipe is not specific to Lang-\\nChain, but we’ll point out a few customizations, where LangChain could be applicable. You can \\nfind the code in the notebooks directory in the GitHub repository for the book.\\nAs a first step, we’ll set up fine-tuning with libraries and environment variables.\\nSetup for fine-tuning\\nFine-tuning consistently achieves strong results across tasks but requires extensive computa -\\ntional resources. Therefore, it’s a good idea to do fine-tuning in an environment where we can \\naccess powerful GPUs and memory resources. We’ll run this on Google Colab instead of the local \\nenvironment, where we can run fine-tuning of LLMs free of charge (with only a few restrictions).\\nPlease make sure you set your Google Colab machine settings in the top menu to TPU or GPU to \\nmake sure you have sufficient resources to run the following code and that the training doesn’t \\ntake too long. We’ll install all required libraries in the Google Colab environment – I am adding \\nthe versions of these libraries that I’ve used to make our fine-tuning repeatable:\\n• peft: PEFT (version 0.5.0)\\n• trl: Proximal Policy Optimization (0.6.0)\\n• bitsandbytes: k-bit optimizers and matrix multiplication routines, needed for quanti-\\nzation (0.41.1)\\n• accelerate: train and use PyTorch models with multi-GPU, TPU, and mixed-precision \\n(0.22.0)\\n• transformers: Hugging Face transformers library with backends in JAX, PyTorch, and \\nTensorFlow (4.32.0)\\n• datasets: community-driven open-source library of datasets (2.14.4)\\n• sentencepiece: Python wrapper for fast tokenization (0.1.99)\\n• wandb: for monitoring the training progress on Weights and Biases (0.15.8)\\n• langchain for loading the model back as a LangChain LLM after training (0.0.273)\\nGoogle Colab is a computation environment that provides different means for hard-\\nware acceleration of computation tasks such as Tensor Processing Units (TPUs) and \\nGraphical Processing Units (GPUs). These are available both in free and professional \\ntiers. For the task in this section, the free tier is sufficient. You can sign into a Colab \\nenvironment at this URL: https://colab.research.google.com/'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 256}, page_content=\"Customizing LLMs and Their Output234\\nWe can install these libraries from the Colab notebook as follows:\\n!pip install -U accelerate bitsandbytes datasets transformers peft trl \\nsentencepiece wandb langchain huggingface_hub\\nTo download and train models from Hugging Face, we need to authenticate with the platform. \\nPlease note that if you want to push your model to Hugging Face later, you need to generate a new \\nAPI token with write permissions on Hugging Face: https://huggingface.co/settings/tokens\\nFigure 8.1: Creating a new API token on Hugging Face write permissions\\nWe can authenticate from the notebook like this:\\nfrom huggingface_hub import notebook_login\\nnotebook_login()\\nWhen prompted, paste your Hugging Face access token.\\nWeights and Biases (W&B) is an MLOps platform that can help developers monitor and docu -\\nment ML training workflows from end to end. As mentioned earlier, we will use W&B to get an \\nidea of how well the training is working and if the model is improving over time. For W&B, we \\nneed to name the project; alternatively, we can use wandb's init() method:\\nA note of caution before we start: when executing the code, you need to log in to \\ndifferent services, so make sure you pay attention when running the notebook!\"),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 257}, page_content='Chapter 8 235\\nimport os\\nos.environ[\"WANDB_PROJECT\"] = \"finetuning\"\\nTo authenticate with W&B, you need to create a free account with them at https://www.wandb.\\nai. You can find your API key on the Authorize page: https://wandb.ai/authorize.\\nAgain, we need to paste in our API token.\\nIf the previous training run is still active – this could be from a previous execution of the notebook \\nif you are running it a second time – let’s make sure we start a new one! This will ensure that we \\nget new reports and a dashboard on W&B:\\nimport wandb\\nif wandb.run is not None:\\n    wandb.finish()\\nNext, we’ll need to choose a dataset against which we want to optimize. We can use lots of different \\ndatasets here that are appropriate for coding, storytelling, tool use, SQL generation, grade-school \\nmath questions (GSM8k), or many other tasks. Hugging Face provides a wealth of datasets, which \\ncan be viewed at this URL: https://huggingface.co/datasets. These cover a lot of different \\nand even the most niche tasks.\\nWe can also customize our own dataset. For example, we can use LangChain to set up training \\ndata. There are quite a few methods available for filtering that could help reduce redundancy in \\nthe dataset. It would have been appealing to show data collection as a practical recipe in this \\nchapter. However, because of the complexity, it is out of the scope of the book.\\nIt might be harder to filter for quality from web data, but there are a lot of possibilities. For code \\nmodels, we could apply code validation techniques to score segments as a quality filter. If the \\ncode comes from GitHub, we can filter by stars or by stars by repo owner.\\nFor texts in natural language, quality filtering is not trivial. Search engine placement could serve \\nas a popularity filter, since it’s often based on user engagement with the content. Further, knowl-\\nedge distillation techniques could be tweaked as a filter by fact density and accuracy.\\nIn this recipe, we are fine-tuning for question-answering performance with the Squad V2 dataset. \\nYou can see a detailed dataset description on Hugging Face: https://huggingface.co/spaces/\\nevaluate-metric/squad_v2:\\nfrom datasets import load_dataset\\ndataset_name = \"squad_v2\"\\ndataset = load_dataset(dataset_name, split=\"train\")\\neval_dataset = load_dataset(dataset_name, split=\"validation\")'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 258}, page_content=\"Customizing LLMs and Their Output236\\nWe are taking both training and validation splits. The Squad V2 dataset has a part that’s sup -\\nposed to be used in training and another one in validation, as we can see in the output of load_\\ndataset(dataset_name):\\nDatasetDict({\\n    train: Dataset({\\n        features: ['id', 'title', 'context', 'question', 'answers'],\\n        num_rows: 130319\\n    })\\n    validation: Dataset({\\n        features: ['id', 'title', 'context', 'question', 'answers'],\\n        num_rows: 11873\\n    })\\n})\\nWe’ll use the validation splits for early stopping. Early stopping will allow us to stop training \\nwhen the validation error begins to degrade.\\nThe Squad V2 dataset is composed of various features, which we can see here:\\n{'id': Value(dtype='string', id=None),\\n 'title': Value(dtype='string', id=None),\\n 'context': Value(dtype='string', id=None),\\n 'question': Value(dtype='string', id=None),\\n 'answers': Sequence(feature={'text': Value(dtype='string', id=None),\\n 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)}\\nThe basic idea in training is prompting the model with a question and comparing the answer to \\nthe dataset. In the next section, we’ll use this setup to fine-tune an open-source LLM.\\nOpen-source models\\nWe want a small model that we can run locally at a decent token rate. LLaMa-2 models require \\nsigning a license agreement with your email address and getting confirmed (which, to be fair, \\ncan be very fast), as it comes with restrictions for commercial use. LLaMa derivatives such as \\nOpenLLaMa have performed quite well, as can be evidenced on the HF leaderboard: https://\\nhuggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\\nOpenLLaMa version 1 cannot be used for coding tasks, because of the tokenizer. Therefore, let’s \\nuse v2! We’ll use a 3B parameter model, which we’ll be able to use even on older hardware:\"),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 259}, page_content='Chapter 8 237\\nmodel_id = \"openlm-research/open_llama_3b_v2\"\\nnew_model_name = f\"openllama-3b-peft-{dataset_name}\"\\nWe can use even smaller models such as EleutherAI/gpt-neo-125m, which can also give a par -\\nticularly good compromise between resource use and performance.\\nLet’s load the model:\\nimport torch\\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\\nbnb_config = BitsAndBytesConfig(\\n    load_in_4bit=True,\\n    bnb_4bit_quant_type=\"nf4\",\\n    bnb_4bit_compute_dtype=torch.float16,\\n)\\ndevice_map=\"auto\"\\nbase_model = AutoModelForCausalLM.from_pretrained(\\n    model_id,\\n    quantization_config=bnb_config,\\n    device_map=\"auto\",\\n    trust_remote_code=True,\\n)\\nbase_model.config.use_cache = False\\nThe Bits and Bytes configuration makes it possible to quantize our model in 8, 4, 3, or even 2 bits \\nwith a much-accelerated inference and lower memory footprint, without incurring a big cost in \\nterms of performance.\\nWe are going to store model checkpoints on Google Drive; you need to confirm your login to your \\nGoogle account:\\nfrom google.colab import drive\\ndrive.mount(\\'/content/gdrive\\')\\nWe’ll need to authenticate with Google for this to work.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 260}, page_content='Customizing LLMs and Their Output238\\nWe can set our output directory for model checkpoints and logs to our Google Drive:\\noutput_dir = \"/content/gdrive/My Drive/results\"\\nIf you don’t want to use Google Drive, just set this to a directory on your computer.\\nFor training, we need to set up a tokenizer:\\nfrom transformers import AutoTokenizer\\ntokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_\\ncode=True)\\ntokenizer.pad_token = tokenizer.eos_token\\ntokenizer.padding_side = \"right\"\\nNow, we’ll define our training configuration. We’ll set up LORA and other training arguments:\\nfrom transformers import TrainingArguments, EarlyStoppingCallback\\nfrom peft import LoraConfig\\n# More info: https://github.com/huggingface/transformers/pull/24906\\nbase_model.config.pretraining_tp = 1\\npeft_config = LoraConfig(\\n    lora_alpha=16,\\n    lora_dropout=0.1,\\n    r=64,\\n    bias=\"none\",\\n    task_type=\"CAUSAL_LM\",\\n)\\ntraining_args = TrainingArguments(\\n    output_dir=output_dir, \\n    per_device_train_batch_size=4,\\n    gradient_accumulation_steps=4,\\n    learning_rate=2e-4,\\n    logging_steps=10,\\n    max_steps=2000,\\n    num_train_epochs=100,\\n    evaluation_strategy=\"steps\",\\n    eval_steps=5,\\n    save_total_limit=5,\\n    push_to_hub=False,'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 261}, page_content='Chapter 8 239\\n    load_best_model_at_end=True,\\n    report_to=\"wandb\"\\n)\\nA few comments to explain some of these parameters are in order. The push_to_hub argument \\nmeans that we can push the model checkpoints to the HuggingSpace Hub regularly during train-\\ning. For this to work, you need to set up the HuggingSpace authentication (with write permis-\\nsions, as mentioned). If we opt for this, as output_dir, we can use new_model_name. This will be \\nthe repository name under which the model will be available here on Hugging Face: https://\\nhuggingface.co/models.\\nAlternatively, as I’ve done here, you can save your model locally or in the cloud, for example, in \\nGoogle Drive in a directory. I’ve set max_steps and num_train_epochs very high, because I’ve \\nnoticed that training can still improve after many steps. Early stepping and a high number of \\nmaximum training steps should help to get the model to provide higher performance. For early \\nstopping, we need to set evaluation_strategy as \"steps\" and load_best_model_at_end=True.\\neval_steps is the number of update steps between two evaluations. save_total_limit=5 means \\nthat only the last five models are saved. Finally, report_to=\"wandb\" means that we’ll send training \\nstats, some model metadata, and hardware information to W&B, where we can look at graphs \\nand dashboards for each run.\\nThe training can then use our configuration:\\nfrom trl import SFTTrainer\\ntrainer = SFTTrainer(\\n    model=base_model,\\n    train_dataset=dataset,\\n    eval_dataset=eval_dataset,\\n    peft_config=peft_config,\\n    dataset_text_field=\"question\",  # this depends on the dataset!\\n    max_seq_length=512,\\n    tokenizer=tokenizer,\\n    args=training_args,\\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=200)]\\n)\\ntrainer.train()'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 262}, page_content='Customizing LLMs and Their Output240\\nThe training can take quite a while, even running on a TPU device. Frequent evaluation slows the \\ntraining down by a lot. If you disable the early stopping, you can make this much faster.\\nWe should see some statistics as the training progresses, but it’s nicer to show the graph of per-\\nformance, as we can see on W&B:\\nFigure 8.2: Fine-tuning training loss over time (steps)\\nAfter training is done, we can save the final checkpoint on disk for re-loading:\\ntrainer.model.save_pretrained(\\n    os.path.join(output_dir, \"final_checkpoint\"),\\n)\\nWe can now share our final model with friends to brag about the performance we’ve achieved by \\nmanually pushing to Hugging Face:\\ntrainer.model.push_to_hub(\\n    repo_id=new_model_name\\n)\\nWe can now load the model back using a combination of our Hugging Face username and the \\nrepository name (the new model name). Let’s quickly show how to use this model in LangChain. \\nUsually, the peft model is stored as an adapter, not as a full model; therefore, the loading is a \\nbit different:\\nfrom peft import PeftModel, PeftConfig\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 263}, page_content='Chapter 8 241\\nfrom langchain.llms import HuggingFacePipeline\\nmodel_id = \\'openlm-research/open_llama_3b_v2\\'\\nconfig = PeftConfig.from_pretrained(\"benji1a/openllama-3b-peft-squad_v2\")\\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\\nmodel = PeftModel.from_pretrained(model, \"benji1a/openllama-3b-peft-squad_\\nv2\")\\ntokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_\\ncode=True)\\ntokenizer.pad_token = tokenizer.eos_token\\npipe = pipeline(\\n    \"text-generation\",\\n    model=model,\\n    tokenizer=tokenizer,\\n    max_length=256\\n)\\nllm = HuggingFacePipeline(pipeline=pipe)\\nWe’ve done everything so far on Google Colab, but we could equally execute this locally; just note \\nthat you need to have the huggingface peft library installed!\\nCommercial models\\nSo far, we’ve shown how to fine-tune and deploy an open-source LLM. Some commercial models \\ncan be fine-tuned on custom data as well. For example, both OpenAI’s GPT-3.5 and Google’s PaLM \\nmodel offer this capability. This has been integrated with a few Python libraries.\\nWith the Scikit-LLM library, this is only a few lines of code. We won’t go through a full recipe in \\nthis section, but please look at the Scikit-LLM library or the documentation of different cloud \\nLLM providers to find all the details. The Scikit-LLM library is not part of the setup that we dis-\\ncussed in Chapter 3, Getting Started with LangChain, so you’d have to install it manually. I’ve also \\nnot included the training data, X_train. You’d have to come up with a training dataset yourself.\\nFine-tuning a PaLM model for text classification can be done like this:\\nfrom skllm.models.palm import PaLMClassifier\\nclf = PaLMClassifier(n_update_steps=100)\\nclf.fit(X_train, y_train) # y_train is a list of labels\\nlabels = clf.predict(X_test)'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 264}, page_content='Customizing LLMs and Their Output242\\nSimilarly, you can fine-tune the GPT-3.5 model for text classification like this:\\nfrom skllm.models.gpt import GPTClassifier\\nclf = GPTClassifier(\\n        base_model = \"gpt-3.5-turbo-0613\",\\n        n_epochs = None, # int or None. When None, will be determined \\nautomatically by OpenAI\\n        default_label = \"Random\", # optional\\n)\\nclf.fit(X_train, y_train) # y_train is a list of labels\\nlabels = clf.predict(X_test)\\nInterestingly, in the fine-tuning available on OpenAI, all inputs are passed through a moderation \\nsystem to make sure that the inputs are compatible with safety standards.\\nThis concludes fine-tuning. LLMs can be deployed and queried without any task-specific tuning. \\nBy prompting, we can accomplish few-shot learning or even zero-shot learning, as we’ll discuss \\nin the next section.\\nPrompt engineering\\nPrompts are the instructions and examples we provide to language models to steer their behavior. \\nThey are important for steering the behavior of LLMs because they allow you to align the model \\noutputs to human intentions without expensive retraining. Carefully engineered prompts can \\nmake LLMs suitable for a wide variety of tasks beyond what they were originally trained for. \\nPrompts act as instructions that demonstrate to the LLM what the desired input-output map -\\nping is.\\nPrompts consist of three main components:\\n• Instructions that describe the task requirements, goals, and format of input/output. They \\nexplain the task to the model unambiguously.\\n• Examples that demonstrate the desired input-output pairs. They provide diverse demon-\\nstrations of how different inputs should map to outputs.\\n• Input that the model must act on to generate the output.\\nThe following figure shows a few examples of prompting different language models (source: Pre-\\ntrain, Prompt, and Predict - A Systematic Survey of Prompting Methods in Natural Language Processing \\nby Liu and colleagues, 2021):'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 265}, page_content='Chapter 8 243\\nFigure 8.3: Prompt examples, particularly knowledge probing in close form, and summari -\\nzation\\nPrompt engineering, also known as in-context learning, refers to techniques to steer LLM behavior \\nthrough carefully designed prompts, without changing the model weights. The goal is to align \\nthe model outputs with human intentions for a given task. Prompt tuning, on the other hand, \\nprovides intuitive control over model behavior but is sensitive to the precise wording and design \\nof prompts, suggesting the need for carefully crafted guidelines to achieve desired results. But \\nwhat do good prompts look like?\\nThe most important first step is to start simple and work iteratively. Begin with concise, straight-\\nforward instructions and build up complexity gradually as needed. Break complex tasks down \\ninto simpler sub-tasks. This avoids overwhelming the model initially. Be as specific, descriptive, \\nand detailed as possible about the exact task and desired format of the output. Providing relevant \\nexamples is highly effective in demonstrating the required reasoning chains or output styles.\\nFor complex reasoning tasks, prompting the model to explain its step-by-step thought process \\nleads to increased accuracy. Techniques like chain-of-thought prompting guide the model to \\nreason explicitly. Providing few-shot examples further demonstrates the desired reasoning for-\\nmat. Problem decomposition prompts that break down complex problems into smaller, more \\nmanageable sub-tasks also improve reliability by enabling a more structured reasoning process. \\nSampling multiple candidate responses and picking the most consistent one helps reduce errors \\nand inconsistencies, compared to relying on a single-model output.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 266}, page_content='Customizing LLMs and Their Output244\\nInstead of focusing on what not to do, clearly specify the desired actions and outcomes. Direct, \\nunambiguous instructions work best. Avoid imprecise or vague prompts. Start simple, be specific, \\nprovide examples, prompt for explanations, decompose problems, and sample multiple responses \\n– these are some best practices to steer LLMs effectively using careful prompt engineering. With \\niteration and experimentation, prompts can be optimized to improve reliability, even for complex \\ntasks, and achieve a performance often comparable to fine-tuning.\\nAfter learning about best practices, let’s look at a few prompt techniques, from simple to increas-\\ningly more advanced!\\nPrompt techniques\\nBasic prompting methods include zero-shot prompting with just the input text, and few-shot \\nprompting with a few demonstration examples showing the desired input-output pairs. Research-\\ners have identified biases like majority label bias and recency bias that contribute to variability \\nin few-shot performance. Careful prompt design through example selection, ordering, and for -\\nmatting can help mitigate these issues.\\nMore advanced prompting techniques include instruction prompting, where the task require -\\nments are described explicitly rather than just demonstrated. Self-consistency sampling gener -\\nates multiple outputs and selects the one that aligns best with the examples. Chain-of-Thought \\n(CoT) prompting generates explicit reasoning steps, leading to the final output. This is especially \\nbeneficial for complex reasoning tasks. CoT prompts can be manually written or generated au-\\ntomatically via methods like augment-prune-select.\\nThis table gives a brief overview of a few methods of prompting compared to fine-tuning:\\nTechnique Description Key Idea\\nPerformance \\nConsiderations\\nZero-Shot \\nPrompting\\nNo examples provided; rely \\non the model’s training\\nLeverages the model’s pre-\\ntraining\\nWorks for simple \\ntasks, but struggles \\nwith complex \\nreasoning\\nFew-Shot \\nPrompting\\nProvides a few demos of \\ninput and desired output\\nShows desired reasoning \\nformat\\nTripled accuracy \\non grade-school \\nmath'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 267}, page_content='Chapter 8 245\\nCoT\\nPrefix responses with \\nintermediate reasoning \\nsteps\\nGives the model space to \\nreason before answering\\nQuadrupled \\naccuracy on a math \\ndataset\\nLeast-\\nto-Most \\nPrompting\\nPrompts the model for \\nsimpler subtasks first\\nDecomposes a problem \\ninto smaller pieces\\nBoosted accuracy \\nfrom 16% to 99.7% \\non some tasks\\nSelf-\\nConsistency\\nPicks the most frequent \\nanswer from multiple \\nsamples Increases redundancy\\nGained 1–24 \\npercentage points \\nacross benchmarks\\nChain-of-\\nDensity\\nIteratively creates dense \\nsummaries by adding \\nentities\\nGenerates rich, concise \\nsummaries\\nImproves \\ninformation \\ndensity in \\nsummaries\\nChain-of-\\nVerification \\n(CoV)\\nVerifies an initial response \\nby generating and \\nanswering questions Mimics human verification\\nEnhances \\nrobustness and \\nconfidence\\nActive \\nPrompting\\nPicks uncertain samples for \\nhuman labeling as examples\\nFinds effective few-shot \\nexamples\\nImproves few-shot \\nperformance\\nTree-of-\\nThought\\nGenerates and automatically \\nevaluates multiple \\nresponses\\nAllows backtracking \\nthrough reasoning paths\\nFinds an optimal \\nreasoning route\\nVerifiers\\nTrains a separate model to \\nevaluate responses\\nFilters out incorrect \\nresponses\\nLifted grade-school \\nmath accuracy \\nby ~20 percentage \\npoints\\nFine-\\nTuning\\nFine-tunes on an \\nexplanation dataset \\ngenerated via prompting\\nImproves the model’s \\nreasoning abilities\\n73% accuracy on a \\ncommonsense QA \\ndataset\\nTable 8.2: Prompting techniques for LLMs compared to fine-tuning\\nSome prompting techniques incorporate external information retrieval to provide missing con-\\ntext to the LLM before generating the output. For open-domain QA, relevant paragraphs can be \\nretrieved via search engines and incorporated into the prompt. For closed-book QA, few-shot \\nexamples with an evidence-question-answer format work better than a QA format.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 268}, page_content='Customizing LLMs and Their Output246\\nIn the next few subsections, we’ll go through a few of the aforementioned techniques. LangC -\\nhain provides tools to enable advanced prompt engineering strategies like zero-shot prompting, \\nfew-shot learning, chain-of-thought, self-consistency, and tree-of-thought. All these techniques \\ndescribed here enhance the accuracy, consistency, and reliability of LLMs’ reasoning capabilities \\non complex tasks by providing clearer instructions, fine-tuning with targeted data, employing \\nproblem breakdown strategies, incorporating diverse sampling approaches, integrating verifi -\\ncation mechanisms, and adopting probabilistic modeling frameworks.\\nYou can find all the examples from this section in the prompting directory in the GitHub repository \\nfor the book. Let’s start with the vanilla strategy: we just ask for a solution.\\nZero-shot prompting\\nZero-shot prompting, as opposed to few-shot prompting, involves feeding task instructions \\ndirectly to an LLM without providing any demonstrations or examples. This prompt tests the \\ncapabilities of the pre-trained model to understand and follow the instructions:\\nfrom langchain import PromptTemplate\\nfrom langchain.chat_models import ChatOpenAI\\nmodel = ChatOpenAI()\\nprompt = PromptTemplate(input_variables=[\"text\"], template=\"Classify the \\nsentiment of this text: {text}\")\\nchain = prompt | model\\nprint(chain.invoke({\"text\": \"I hated that movie, it was terrible!\"}))\\nThis outputs the sentiment classification prompt with the input text, without any examples:\\ncontent=\\'The sentiment of this text is negative.\\' additional_kwargs={} \\nexample=False\\nFew-shot learning\\nFew-shot learning presents the LLM with just a few input-output examples relevant to the task, \\nwithout explicit instructions. This allows the model to infer the intentions and goals purely from \\ndemonstrations. Carefully selected, ordered, and formatted examples can improve the model’s \\ninference abilities. However, few-shot learning can be prone to biases and variability across trials. \\nAdding explicit instructions can make the intentions more transparent to the model and improve \\nrobustness. Overall, prompts combine the strengths of instructions and examples to maximize \\nsteering of the LLM for the task at hand.\\nThe FewShotPromptTemplate allows you to show the model just a few demonstration examples \\nof the task to prime it, without explicit instructions.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 269}, page_content='Chapter 8 247\\nLet’s extend the previous example for sentiment classification with few-shot prompting. In this \\nexample, we want an LLM to categorize customer feedback into Positive, Negative, or Neutral. \\nWe provide it with a few examples:\\nexamples = [{\\n    \"input\": \"I absolutely love the new update! Everything works \\nseamlessly.\",\\n    \"output\": \"Positive\",\\n    },{\\n    \"input\": \"It\\'s okay, but I think it could use more features.\",\\n    \"output\": \"Neutral\",\\n    }, {\\n    \"input\": \"I\\'m disappointed with the service, I expected much better \\nperformance.\",\\n    \"output\": \"Negative\"\\n}]\\nWe can use these examples in a prompt like this:\\nfrom langchain.prompts import FewShotPromptTemplate, PromptTemplate\\nfrom langchain.chat_models import ChatOpenAI\\nexample_prompt = PromptTemplate(\\n    template=\"{input} -> {output}\",\\n    input_variables=[\"input\", \"output\"],\\n)\\nprompt = FewShotPromptTemplate(\\n    examples=examples,\\n    example_prompt=example_prompt,\\n    suffix=\"Question: {input}\",\\n    input_variables=[\"input\"]\\n)\\nprint((prompt | ChatOpenAI()).invoke({\"input\": \" This is an excellent book \\nwith high quality explanations.\"}))\\nWe should get the following output:\\ncontent=\\'Positive\\' additional_kwargs={} example=False\\nYou can expect the LLM to use these examples to guide its classification of the new sentence. The \\nfew-shot method primes the model without extensive training, relying instead on its pre-trained \\nknowledge and the context provided by the examples.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 270}, page_content='Customizing LLMs and Their Output248\\nTo choose examples tailored to each input, FewShotPromptTemplate can accept a SemanticSim\\nilarityExampleSelector, based on embeddings rather than hardcoded examples. The Semant\\nicSimilarityExampleSelector automatically finds the most relevant examples for each input.\\nFor many tasks, standard few-shot prompting works well, but there are many other techniques \\nand extensions when dealing with more complex reasoning tasks.\\nChain-of-thought prompting\\nCoT prompting aims to encourage reasoning by getting the model to provide intermediate steps, \\nleading to the definitive answer. This is done by prefixing the prompt with instructions to show \\nits thinking.\\nThere are two variants of CoT, zero-shot and few-shot. In zero-shot CoT, we just add the instruc-\\ntion “Let’s think step by step!” to the prompt.\\nWhen asking an LLM to reason through a problem, it is often more effective to have it explain its \\nreasoning before stating the final answer. This encourages the LLM to logically think through \\nthe problem first, rather than just guessing the answer and trying to justify it afterward. Asking \\nan LLM to explain its thought process aligns well with its core capabilities.\\nFor example:\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.prompts import PromptTemplate\\nreasoning_prompt = \"{question}\\\\nLet\\'s think step by step!\"\\nprompt = PromptTemplate(\\n  template=reasoning_prompt,\\n  input_variables=[\"question\"]\\n)\\nmodel = ChatOpenAI()\\nchain = prompt | model\\nprint(chain.invoke({\\n   \"question\": \"There were 5 apples originally. I ate 2 apples. My friend \\ngave me 3 apples. How many apples do I have now?\",\\n}))\\nAfter running this, we get the reasoning process together with the result:'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 271}, page_content='Chapter 8 249\\ncontent=\\'Step 1: Originally, there were 5 apples.\\\\nStep 2: I ate 2 \\napples.\\\\nStep 3: So, I had 5 - 2 = 3 apples left.\\\\nStep 4: My friend gave \\nme 3 apples.\\\\nStep 5: Adding the apples my friend gave me, I now have 3 + \\n3 = 6 apples.\\' additional_kwargs={} example=False\\nThe preceding approach is also called zero-shot chain-of-thought.\\nFew-shot chain-of-thought prompting is a few-shot prompt, where the reasoning is explained \\nas part of the example solutions, with the idea to encourage an LLM to explain its reasoning \\nbefore deciding.\\nIf we go back to the few-shot examples from earlier, we can extend them as follows:\\nexamples = [{\\n    \"input\": \"I absolutely love the new update! Everything works \\nseamlessly.\",\\n    \"output\": \"Love and absolute works seamlessly are examples of positive \\nsentiment. Therefore, the sentiment is positive\",\\n    },{\\n    \"input\": \"It\\'s okay, but I think it could use more features.\",\\n    \"output\": \"It\\'s okay is not an endorsement. The customer further \\nthinks it should be extended. Therefore, the sentiment is neutral\",\\n    }, {\\n    \"input\": \"I\\'m disappointed with the service, I expected much better \\nperformance.\",\\n    \"output\": \"The customer is disappointed and expected more. This is \\nnegative\"\\n}]\\nIn these examples, the reasons for the decision are explained. This encourages the LLM to give a \\nsimilar result explaining its reasoning.\\nIt has been shown that CoT prompting can lead to more accurate results; however, this perfor-\\nmance boost was found to be proportional to the size of the model, and the improvements were \\nnegligible or even negative in smaller models.\\nSelf-consistency\\nWith self-consistency prompting, the model generates multiple candidate answers to a question. \\nThese are then compared against each other, and the most consistent or frequent answer is selected \\nas the final output. A good example of self-consistency prompting with LLMs is in the context of \\nfact verification or information synthesis, where accuracy is paramount.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 272}, page_content='Customizing LLMs and Their Output250\\nIn the first step, we’ll create multiple solutions to a question or a problem:\\nfrom langchain import PromptTemplate, LLMChain\\nfrom langchain.chat_models import ChatOpenAI\\nsolutions_template = \"\"\"\\nGenerate {num_solutions} distinct answers to this question:\\n{question}\\nSolutions:\\n\"\"\"\\nsolutions_prompt = PromptTemplate(\\n   template=solutions_template,\\n   input_variables=[\"question\", \"num_solutions\"]\\n)\\nsolutions_chain = LLMChain(\\n   llm=ChatOpenAI(),\\n   prompt=solutions_prompt,\\n   output_key=\"solutions\"\\n)\\nFor the second step, we want to count the different answers. We can use an LLM again:\\nconsistency_template = \"\"\"\\nFor each answer in {solutions}, count the number of times it occurs. \\nFinally, choose the answer that occurs most.\\nMost frequent solution: \\n\"\"\"\\nconsistency_prompt = PromptTemplate(\\n   template=consistency_template,\\n   input_variables=[\"solutions\"]\\n)\\nconsistency_chain = LLMChain(\\n   llm=ChatOpenAI(),\\n   prompt=consistency_prompt,\\n   output_key=\"best_solution\"\\n)'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 273}, page_content='Chapter 8 251\\nLet’s put these two chains together with a SequentialChain:\\nfrom langchain.chains import SequentialChain\\nanswer_chain = SequentialChain(\\n   chains=[solutions_chain, consistency_chain],\\n   input_variables=[\"question\", \"num_solutions\"],\\n   output_variables=[\"best_solution\"]\\n)\\nLet’s ask a simple question and check the answer:\\nprint(answer_chain.run(\\n   question=\"Which year was the Declaration of Independence of the United \\nStates signed?\",\\n   num_solutions=\"5\"\\n))\\nWe should get a response like this:\\n1776 is the year in which the Declaration of Independence of the United \\nStates was signed. It occurs twice in the given answers (3 and 4).\\nWe should get the right response based on the vote; however, of the five responses we produced, \\nthree were wrong.\\nThis approach leverages the model’s ability to reason and utilize internal knowledge while reduc-\\ning the risk of outliers or incorrect information, by focusing on the most recurring answer, thus \\nimproving the overall reliability of the response given by the LLM.\\nTree-of-thought\\nIn Tree-of-Thought (ToT) prompting, we generate multiple problem-solving steps or approaches \\nfor a given prompt and then use the AI model to critique them. The critique will be based on the \\nmodel’s judgment of the solution’s suitability to the problem.\\nThere is actually an implementation now of ToT in the LangChain experimental package; however, \\nlet’s walk through an instructive step-by-step example of implementing ToT using LangChain.\\nFirst, we’ll define our four chain components with PromptTemplates. We need a solution template, \\nan evaluation template, a reasoning template, and a ranking template.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 274}, page_content='Customizing LLMs and Their Output252\\nLet’s first generate solutions:\\nsolutions_template = \"\"\"\\nGenerate {num_solutions} distinct solutions for {problem}. Consider \\nfactors like {factors}.\\nSolutions:\\n\"\"\"\\nsolutions_prompt = PromptTemplate(\\n   template=solutions_template,\\n   input_variables=[\"problem\", \"factors\", \"num_solutions\"]\\n)\\nLet’s ask the LLM to evaluate these solutions:\\nevaluation_template = \"\"\"\\nEvaluate each solution in {solutions} by analyzing pros, cons, \\nfeasibility, and probability of success.\\nEvaluations:\\n\"\"\"\\nevaluation_prompt = PromptTemplate(\\n  template=evaluation_template,\\n  input_variables=[\"solutions\"] \\n)\\nAfter this step, we want to reason a bit more about them:\\nreasoning_template = \"\"\"\\nFor the most promising solutions in {evaluations}, explain scenarios, \\nimplementation strategies, partnerships needed, and handling potential \\nobstacles.\\nEnhanced Reasoning:\\n\"\"\"\\nreasoning_prompt = PromptTemplate(\\n  template=reasoning_template,\\n  input_variables=[\"evaluations\"]\\n)'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 275}, page_content='Chapter 8 253\\nFinally, we can rank these solutions given our reasoning so far:\\nranking_template = \"\"\"\\nBased on the evaluations and reasoning, rank the solutions in {enhanced_\\nreasoning} from most to least promising.\\nRanked Solutions:\\n\"\"\"\\nranking_prompt = PromptTemplate(\\n  template=ranking_template,\\n  input_variables=[\"enhanced_reasoning\"]\\n)\\nNext, we create chains from these templates before we put the chains all together:\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.chat_models import ChatOpenAI\\nsolutions_chain = LLMChain(\\n   llm=ChatOpenAI(),\\n   prompt=solutions_prompt,\\n   output_key=\"solutions\"\\n)\\nevalutation_chain = LLMChain(\\n   llm=ChatOpenAI(),\\n   prompt=evaluation_prompt,\\n   output_key=\"evaluations\"\\n)\\nreasoning_chain = LLMChain(\\n   llm=ChatOpenAI(),\\n   prompt=reasoning_prompt,\\n   output_key=\"enhanced_reasoning\"\\n)\\nranking_chain = LLMChain(\\n   llm=ChatOpenAI(),\\n   prompt=ranking_prompt,\\n   output_key=\"ranked_solutions\"\\n)'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 276}, page_content='Customizing LLMs and Their Output254\\nPlease note how each output_key corresponds to an input_key in the prompt of the following \\nchain. Finally, we connect these chains into a SequentialChain:\\nfrom langchain.chains import SequentialChain\\ntot_chain = SequentialChain(\\n   chains=[solutions_chain, evalutation_chain, reasoning_chain, ranking_\\nchain],\\n   input_variables=[\"problem\", \"factors\", \"num_solutions\"],\\n   output_variables=[\"ranked_solutions\"]\\n)\\nprint(tot_chain.run(\\n   problem=\"Prompt engineering\",\\n   factors=\"Requirements for high task performance, low token use, and few \\ncalls to the LLM\",\\n   num_solutions=3\\n))\\nLet’s run our tot_chain and see the printed output:\\n1. Train or fine-tune language models using datasets that are relevant to \\nthe reasoning task at hand.\\n2. Develop or adapt reasoning algorithms and techniques to improve the \\nperformance of language models in specific reasoning tasks.\\n3. Evaluate existing language models and identify their strengths and \\nweaknesses in reasoning.\\n4. Implement evaluation metrics to measure the reasoning performance of \\nthe language models.\\n5. Iteratively refine and optimize the reasoning capabilities of the \\nlanguage models based on evaluation results.\\nIt is important to note that the ranking of solutions may vary depending \\non the specific context and requirements of each scenario.\\nI wholeheartedly agree with the suggestions. They show the strengths of ToT. A lot of these topics \\nare part of this chapter, while some will come up in Chapter 9, Generative AI in Production, where \\nwe’ll discuss evaluating LLMs and their performance.\\nThis allows us to leverage the LLM at each stage of the reasoning process. The ToT approach helps \\navoid dead ends by fostering exploration. If you want to see more examples, in the LangChain \\ncookbook, you can find a ToT for playing sudoku.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 277}, page_content='Chapter 8 255\\nPrompt design is highly significant for unlocking LLM reasoning capabilities, and it offers the \\npotential for future advancements in models and prompting techniques. These principles and \\ntechniques form a valuable toolkit for researchers and practitioners working with LLMs.\\nSummary\\nConditioning allows steering generative AI to improve performance, safety, and quality. In this \\nchapter, the focus is on conditioning through fine-tuning and prompting. In fine-tuning, the lan-\\nguage model is trained on many examples of tasks formulated as natural language instructions, \\nalong with appropriate responses. This is often done through reinforcement learning with human \\nfeedback; however, other techniques have been developed that have been shown to produce com-\\npetitive results with lower resource footprints. In the first recipe of this chapter, we implemented \\nfine-tuning of a small open-source model for question answering.\\nThere are many techniques for prompting that can improve the reliability of LLMs in complex \\nreasoning tasks, including step-by-step prompting, alternate selection, inference prompts, prob-\\nlem decomposition, sampling multiple responses, and employing separate verifier models. These \\nmethods have been shown to enhance accuracy and consistency in reasoning tasks. LangChain \\nprovides building blocks to unlock advanced prompting strategies like few-shot learning, CoT, \\nToT, and others, as we’ve shown in the examples.\\nIn Chapter 9, Generative AI in Production, we’ll talk about the productionization of generative AI \\nand critical issues related to it, such as evaluating LLM apps, deploying them to a server, and \\nmonitoring them.\\nQuestions\\nI’d recommend that you go back to the corresponding sections of this chapter if you are unsure \\nabout any of the answers to these questions:\\n1. What is conditioning, and what is alignment?\\n2. What are the different methods of conditioning, and how can we distinguish them?\\n3. What is instruction tuning, and what is its importance?\\n4. Name a few fine-tuning methods.\\n5. What is quantization?\\n6. What is few-shot learning?\\n7. What is CoT prompting?\\n8. How does ToT work?'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 278}, page_content='Customizing LLMs and Their Output256\\nJoin our community on Discord\\nJoin our community’s Discord space for discussions with the authors and other readers:\\nhttps://packt.link/lang'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 279}, page_content='9\\nGenerative AI in Production\\nAs we’ve discussed in this book, LLMs have gained significant attention in recent years due to \\ntheir ability to generate human-like text. From creative writing to conversational chatbots, these \\ngenerative AI models have diverse applications across industries. However, taking these complex \\nneural network systems from research to real-world deployment comes with significant challenges.\\nSo far, we’ve talked about models, agents, and LLM apps as well as different use cases, but there \\nare many issues that become important when deploying these apps into production to engage \\nwith customers and to make decisions that can have a significant financial impact. This chapter \\nexplores the practical considerations and best practices for productionizing generative AI, spe -\\ncifically LLM apps. Before we deploy an application, performance and regulatory requirements \\nneed to be ensured, it needs to be robust at scale, and finally monitoring has to be in place. Main-\\ntaining rigorous testing, auditing, and ethical safeguards is essential for trustworthy deployment. \\nWe’ll discuss evaluation and observability, and cover a broad range of topics that encompass \\nthe governance and lifecycle management of operationalized AI and decision models, including \\ngenerative AI models.\\nWhile getting an LLM app ready for production, offline evaluation provides a preliminary un -\\nderstanding of a model’s abilities in a controlled setting, and when in production, observability \\noffers continuing insights into its performance in live environments. We’ll discuss a few tools \\nfor either case and I’ll give examples. We’ll also discuss the deployment of LLM applications and \\ngive an overview of available tools and examples for deployment.\\nThroughout the chapter, we’ll work on practical examples with LLM apps, which you can find \\nin the GitHub repository for the book at https://github.com/benman1/generative_ai_with_\\nlangchain.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 280}, page_content='Generative AI in Production258\\nThe main sections of this chapter are:\\n• How to get LLM apps ready for production\\n• How to evaluate LLM apps\\n• How to deploy LLM apps\\n• How to observe LLM apps\\nLet’s start with an overview of what it means and involves to get an LLM app ready for production!\\nHow to get LLM apps ready for production\\nDeploying LLM applications to production is intricate, encompassing robust data management, \\nethical foresight, efficient resource allocation, diligent monitoring, and alignment with behavioral \\nguidelines. Practices to ensure deployment readiness involve:\\n• Data management: Rigorous attention to data quality is critical to avoid biases that can \\nemanate from imbalanced or inappropriate training data. Substantial efforts in data cu-\\nration and ongoing scrutiny of model outputs are required to mitigate emerging biases.\\n• Ethical deployment and compliance: LLM applications are potentially capable of gen -\\nerating harmful content, thus necessitating strict review processes, safety guidelines, \\nand compliance with regulations such as HIPAA, especially in sensitive sectors such as \\nhealthcare.\\n• Resource management: The resource demands of LLMs call for an infrastructure that \\nis both efficient and environmentally sustainable. Innovation in infrastructure helps to \\nreduce costs and address environmental concerns tied to the energy demands of LLMs.\\n• Performance management: Models must be continually monitored for data drift—where \\nchanges in input data patterns can alter model performance—and performance degra-\\ndation over time. Detecting these deviations necessitates prompt retraining or model \\nadjustments.\\n• Interpretability: To build trust and offer insight into the decision-making processes of \\nLLMs, interpretability tools are increasingly important for users who need clarity on how \\nmodel decisions are reached.\\n• Data security: Protecting sensitive information within LLM processes is essential for pri-\\nvacy and compliance. Strong encryption measures and stringent access controls bolster \\nsecurity measures.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 281}, page_content='Chapter 9 259\\n• Model behavior standards: Models must align with ethical guidelines beyond basic func-\\ntional performance—ensuring outputs are constructive (helpful), innocuous (harmless), \\nand trustworthy (honest). This results in stability and societal acceptance.\\n• Hallucination mitigation: Hallucinations refer to instances where LLMs inadvertently \\ngenerate or recall sensitive personal information from their training data corpus in the \\noutputs, despite no prompting for such details in the input source—highlighting critical \\nprivacy concerns and the need for mitigation strategies.\\nEssential recommendations for deploying LLM apps encompass an array of practices aimed at \\nmitigating technical challenges, improving performance, and ensuring ethical integrity. First and \\nforemost, it’s crucial to develop standardized datasets with relevant benchmarks to test and mea-\\nsure model capabilities, including the detection of regressions and alignment with defined goals.\\nMetrics should be task-specific to gauge the model’s proficiency accurately. There also needs to \\nbe a robust framework that includes ethical guidelines, safety protocols, and review processes \\nto prevent the generation and dissemination of harmful content. Human reviewers serve as an \\nessential checkpoint in content validation and bring ethical discernment to AI outputs, ensuring \\nadherence across all contexts.\\nA forward-thinking UX can foster a transparent relationship with users while reinforcing sensible \\nuse. This can include anticipating inaccuracies, such as disclaimers on limitations, attributions, \\nand collecting rich user feedback.\\nTo explain outputs, we should invest in interpretability methods that explain how generative AI \\nmodels arrive at their decisions. Visualizing attention mechanisms or analyzing feature impor-\\ntance can peel back the layers of complexity, which is particularly crucial for high-stakes industries \\nsuch as healthcare or finance.\\nWe discussed hallucinations in Chapter 4, Building Capable Assistants. Mitigation techniques in-\\nclude external retrieval and tool augmentation to provide pertinent context, as we discussed in \\nChapter 5, Building a Chatbot like ChatGPT, and Chapter 6, Developing Software with Generative AI, \\nin particular. There is a danger of models recalling private information, and ongoing advances \\nin methods spanning data filtering, architecture adjustments, and inference techniques show \\npromise in mitigating these problems.\\nFor security, we can strengthen role-based access policies, employ stringent data encryption \\nstandards, adopt anonymization best practices where feasible, and ensure continuous verification \\nthrough compliance audits. Security is a huge topic in the context of LLMs, however, we’ll focus \\non evaluation, observability, and deployment in this chapter.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 282}, page_content='Generative AI in Production260\\nWe need to optimize infrastructure and resource usage by employing distributed techniques \\nsuch as data parallelism or model parallelism to facilitate workload distribution across multiple \\nprocessing units. We can employ techniques such as model compression or other computer ar-\\nchitectural optimizations for more efficient deployment regarding inference speed and latency \\nmanagement. Techniques such as model quantization, discussed in Chapter 8, Customizing LLMs \\nand Their Output, or model distillation can also help reduce the resource footprint of the model. \\nFurther, storing model outputs can reduce latency and costs for repeated queries.\\nWith insightful planning and preparation, generative AI promises to transform industries from \\ncreative writing to customer service. But thoughtfully navigating the complexities of these systems \\nremains critical as they continue to permeate increasingly diverse domains. This chapter aims to \\nprovide a practical guide to some of the pieces that we haven’t covered in this book so far, aiming \\nto help you build impactful and responsible generative AI applications. We cover strategies for \\ndata curation, model development, infrastructure, monitoring, and transparency.\\nBefore we continue our discussion, a few words on terminology are in order. Let’s start by intro-\\nducing MLOps and similar terms, and define what they mean and imply.\\nTerminology\\nMLOps is a paradigm that focuses on deploying and maintaining machine learning models in \\nproduction reliably and efficiently. It combines the practices of DevOps with machine learning to \\ntransition algorithms from experimental systems to production systems. MLOps aims to increase \\nautomation, improve the quality of production models, and address business and regulatory \\nrequirements.\\nLLMOps is a specialized sub-category of MLOps. It refers to the operational capabilities and \\ninfrastructure necessary for fine-tuning and operationalizing LLMs as part of a product. While \\nit may not be drastically different from the concept of MLOps, the distinction lies in the specific \\nrequirements connected to handling, refining, and deploying massive language models such as \\nGPT-3, which houses 175 billion parameters.\\nThe term LMOps is more inclusive than LLMOps as it encompasses various types of language \\nmodels, including both LLMs and smaller generative models. This term acknowledges the ex -\\npanding landscape of language models and their relevance in operational contexts.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 283}, page_content='Chapter 9 261\\nFoundational Model Orchestration (FOMO) specifically addresses the challenges faced when \\nworking with foundation models, that is, models trained on broad data that can be adapted to \\na wide range of downstream tasks. It highlights the need for managing multi-step processes, \\nintegrating with external resources, and coordinating the workflows involving these models.\\nThe term ModelOps focuses on the  governance and lifecycle management of AI and decision \\nmodels as they are deployed. Even more broadly, AgentOps involves the operational management \\nof LLMs and other AI agents, ensuring their appropriate behavior, managing their environment \\nand resource access, and facilitating interactions between agents while addressing concerns \\nrelated to unintended outcomes and incompatible objectives.\\nWhile FOMO emphasizes the unique challenges of working specifically with foundational mod-\\nels, LMOps provides a more inclusive and comprehensive coverage of a wider range of language \\nmodels beyond just the foundational ones. LMOps acknowledges the versatility and increasing \\nimportance of language models in various operational use cases, while still falling under the \\nbroader umbrella of MLOps. Finally, AgentOps explicitly highlights the interactive nature of agents \\nconsisting of generative models operating with certain heuristics and includes tools.\\nThe emergence of all of these very specialized terms underscores the rapid evolution of the field; \\nhowever, their long-term prevalence is unclear. MLOps is widely used and often encompasses the \\nmany more specialized terms we just covered. Therefore, we’ll stick to MLOps for the remainder \\nof this chapter.\\nBefore productionizing any LLM app, we should first evaluate its output, so we should start with \\nthis. We will focus on the evaluation methods provided by LangChain.\\nHow to evaluate LLM apps\\nThe crux of LLM deployment lies in the meticulous curation of training data to preempt biases, \\nimplementing human-led annotation for data enhancement, and establishing automated output \\nmonitoring systems. Evaluating LLMs either as standalone entities or in conjunction with an \\nagent chain is crucial to ensure they function correctly and produce reliable results, and this is \\nan integral part of the ML lifecycle. The evaluation process determines the performance of the \\nmodels in terms of effectiveness, reliability, and efficiency.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 284}, page_content='Generative AI in Production262\\nThe goal of evaluating LLMs is to understand their strengths and weaknesses, enhancing accuracy \\nand efficiency while reducing errors, thereby maximizing their usefulness in solving real-world \\nproblems. This evaluation process typically occurs offline during the development phase. Offline \\nevaluations provide initial insights into model performance under controlled test conditions and \\ninclude aspects such as hyperparameter tuning and benchmarking against peer models or es -\\ntablished standards. They offer a necessary first step toward refining a model before deployment.\\nWhile human assessments are sometimes seen as the gold standard, they are hard to scale and \\nrequire careful design to avoid bias from subjective preferences or authoritative tones. There are \\nmany standardized benchmarks such as MBPP to test basic programming skills, while GSM8K is \\nutilized for multi-step mathematical reasoning. API-Bank evaluates models’ aptitudes for making \\ndecisions about API calls. ARC puts models’ question-answering abilities up against complex \\nintegrations of information, whereas HellaSwag assesses common-sense reasoning in physical \\nsituations using adversarial filtering.\\nHumanEval focuses on code generation’s functional correctness over syntactic similarity. MMLU \\nassesses language understanding across a wide range of subjects at varying depths, indicating \\nproficiency in specialized domains. SuperGLUE takes GLUE a step further with more challeng -\\ning tasks to monitor the fairness and comprehension of language models. TruthfulQA brings a \\nunique angle by benchmarking the truthfulness of LLM responses, foregrounding the significance \\nof veracity.\\nBenchmarks such as MATH demand high-level reasoning capability evaluations. GPT-4’s perfor-\\nmance on this benchmark varies with prompting method sophistication, from few-shot prompts \\nto reinforcement learning with reward modeling approaches. Notably, dialog-based fine-tuning \\ncan sometimes detract from capabilities assessed by measures such as MMLU.\\nEvaluations can provide insights into how well an LLM generates outputs that are relevant, accu-\\nrate, and helpful. Tests such as FLAN and FLASK stress behavioral dimensions, thus prioritizing \\nresponsible AI systems deployment. This chart compares several open and closed source models \\non the FLASK benchmark (source: “FLASK: Fine-grained Language Model Evaluation based on Align-\\nment Skill Sets” by Ye and colleagues, 2023; https://arxiv.org/abs/2307.10928):'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 285}, page_content='Chapter 9 263\\nFigure 9.1: Result of an evaluation with Claude as an evaluating language model\\nIn the result reported in the chart, Claude is the LLM evaluating all outputs. This skews results in \\nfavor of Claude and models similar to it. Often, GPT-3.5 or GPT-4 are used as evaluators, which \\nshows the OpenAI models emerging as winners.\\nIn LangChain, there are various ways to evaluate the outputs of LLMs, including comparing chain \\noutputs, pairwise string comparisons, string distances, and embedding distances. The evaluation \\nresults can be used to determine the preferred model based on the comparison of outputs. Confi-\\ndence intervals and p-values can also be calculated to assess the reliability of the evaluation results.\\nLangChain provides several tools for evaluating the outputs of LLMs. A common approach is \\nto compare the outputs of different models or prompts using PairwiseStringEvaluator. This \\nprompts an evaluator model to choose between two model outputs for the same input and ag -\\ngregates the results to determine an overall preferred model.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 286}, page_content='Generative AI in Production264\\nOther evaluators allow assessing model outputs based on specific criteria such as correctness, \\nrelevance, and conciseness. The CriteriaEvalChain can score outputs on custom or predefined \\nprinciples without needing reference labels. Configuring the evaluation model is also possible \\nby specifying a different chat model such as ChatGPT as the evaluator.\\nYou can follow the code in this section online under the monitoring_and_evaluation  fold -\\ner in the book’s GitHub project. Let’s compare outputs of different prompts or LLMs with the \\nPairwiseStringEvaluator, which prompts an LLM to select the preferred output given a specific \\ninput.\\nComparing two outputs\\nThis evaluation requires an evaluator, a dataset of inputs, and two or more LLMs, chains, or agents \\nto compare. The evaluation aggregates the results to determine the preferred model.\\nThe evaluation process involves several steps:\\n1. Create the evaluator: Load the evaluator using the load_evaluator() function, specifying \\nthe type of evaluator (in this case, pairwise_string).\\n2. Select the dataset: Load a dataset of inputs using the load_dataset() function.\\n3. Define models to compare: Initialize the LLMs, chains, or agents to compare using the \\nnecessary configurations. This involves initializing the language model and any additional \\ntools or agents required.\\n4. Generate responses: Generate outputs for each of the models before evaluating them. \\nThis is typically done in batches to improve efficiency.\\n5. Evaluate pairs: Evaluate the results by comparing the outputs of different models for \\neach input. This is often done using a random selection order to reduce positional bias.\\nHere’s an example from the documentation for pairwise string comparisons:\\nfrom langchain.evaluation import load_evaluator\\nevaluator = load_evaluator(\"labeled_pairwise_string\")\\nevaluator.evaluate_string_pairs(\\n    prediction=\"there are three dogs\",\\n    prediction_b=\"4\",\\n    input=\"how many dogs are in the park?\",\\n    reference=\"four\",\\n)'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 287}, page_content='Chapter 9 265\\nThe output from the evaluator should look as follows:\\n     {\\'reasoning\\': \"Both assistants provided a direct answer to the user\\'s \\nquestion. However, Assistant A\\'s response is incorrect as it stated there \\nare three dogs in the park, while the reference answer indicates there are \\nfour. On the other hand, Assistant B\\'s response is accurate and matches \\nthe reference answer. Therefore, considering the criteria of correctness \\nand accuracy, Assistant B\\'s response is superior. \\\\n\\\\nFinal Verdict: \\n[[B]]\",\\n\\'value\\': \\'B\\',\\n\\'score\\': 0\\n}\\nThe evaluation result includes a score between 0 and 1, indicating the effectiveness of each agent, \\nsometimes along with reasoning that outlines the evaluation process and justifies the score. In \\nthis specific example against the reference, both results are factually incorrect based on the input. \\nWe could remove the reference and let an LLM judge the outputs instead.\\nComparing against criteria\\nLangChain provides several predefined evaluators for different evaluation criteria. These eval -\\nuators can be used to assess outputs based on specific rubrics or criteria sets. Some common \\ncriteria include conciseness, relevance, correctness, coherence, helpfulness, and controversiality.\\nCriteriaEvalChain allows you to evaluate model outputs against custom or predefined criteria. \\nIt provides a way to verify whether an LLM or chain’s output complies with a defined set of cri-\\nteria. You can use this evaluator to assess correctness, relevance, conciseness, and other aspects \\nof the generated outputs.\\nCriteriaEvalChain can be configured to work with or without reference labels. Without reference \\nlabels, the evaluator relies on the LLM’s predicted answer and scores it based on the specified \\ncriteria. With reference labels, the evaluator compares the predicted answer to the reference label \\nand determines its compliance with the criteria.\\nThe evaluation LLM used in LangChain, by default, is GPT-4. However, you can configure the \\nevaluation LLM by specifying other chat models, such as ChatAnthropic or ChatOpenAI, with the \\ndesired settings (for example, temperature). The evaluators can be loaded with a custom LLM by \\npassing the LLM object as a parameter to the load_evaluator() function.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 288}, page_content='Generative AI in Production266\\nLangChain supports both custom criteria and predefined principles for evaluation. Custom criteria \\ncan be defined using a dictionary of criterion_name: criterion_description pairs. These \\ncriteria can be used to assess outputs based on specific requirements or rubrics.\\nHere’s a simple example:\\ncustom_criteria = {\\n    \"simplicity\": \"Is the language straightforward and unpretentious?\",\\n    \"clarity\": \"Are the sentences clear and easy to understand?\",\\n    \"precision\": \"Is the writing precise, with no unnecessary words or \\ndetails?\",\\n    \"truthfulness\": \"Does the writing feel honest and sincere?\",\\n    \"subtext\": \"Does the writing suggest deeper meanings or themes?\",\\n}\\nevaluator = load_evaluator(\"pairwise_string\", criteria=custom_criteria)\\nevaluator.evaluate_string_pairs(\\n    prediction=\"Every cheerful household shares a similar rhythm of joy; \\nbut sorrow, in each household, plays a unique, haunting melody.\",\\n    prediction_b=\"Where one finds a symphony of joy, every domicile of \\nhappiness resounds in harmonious,\"\\n    \" identical notes; yet, every abode of despair conducts a dissonant \\norchestra, each\"\\n    \" playing an elegy of grief that is peculiar and profound to its own \\nexistence.\",\\n    input=\"Write some prose about families.\",\\n)\\nWe can get a very nuanced comparison of the two outputs, as this result shows:\\n{\\'reasoning\\': \\'Response A is simple, clear, and precise. It uses \\nstraightforward language to convey a deep and sincere message about \\nfamilies. The metaphor of music is used effectively to suggest deeper \\nmeanings about the shared joys and unique sorrows of families.\\\\n\\\\nResponse \\nB, on the other hand, is less simple and clear. The language is more \\ncomplex and pretentious, with phrases like \"domicile of happiness\" and \\n\"abode of despair\" instead of the simpler \"household\" used in Response A. \\nThe message is similar to that of Response A, but it is less effectively \\nconveyed due to the unnecessary complexity of the language.\\\\n\\\\nTherefore, \\nbased on the criteria of simplicity, clarity, precision, truthfulness, '),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 289}, page_content='Chapter 9 267\\nand subtext, Response A is the better response.\\\\n\\\\n[[A]]\\', \\'value\\': \\'A\\', \\n\\'score\\': 1}\\nAlternatively, you can use the predefined principles available in LangChain, such as those from \\nConstitutional AI. These principles are designed to evaluate the ethical, harmful, and sensitive \\naspects of the outputs. The use of principles in evaluation allows for a more focused assessment \\nof the generated text.\\nString and semantic comparisons\\nLangChain supports string comparison and distance metrics for evaluating LLM outputs. String \\ndistance metrics such as Levenshtein and Jaro provide a quantitative measure of similarity between \\npredicted and reference strings. Embedding distances using models such as SentenceTransformers \\ncalculates semantic similarity between the generated and expected texts.\\nEmbedding distance evaluators can use embedding models, such as those based on GPT-4 or \\nHugging Face embeddings, to compute vector distances between the predicted and reference \\nstrings. This measures the semantic similarity between the two strings and can provide insights \\ninto the quality of the generated text.\\nHere’s a quick example from the documentation:\\nfrom langchain.evaluation import load_evaluator\\nevaluator = load_evaluator(\"embedding_distance\")\\nevaluator.evaluate_strings(prediction=\"I shall go\", reference=\"I shan\\'t \\ngo\")\\nThe evaluator returns the score 0.0966466944859925. You can change the embeddings used with \\nthe embeddings parameter in the load_evaluator() call.\\nThis often gives better results than older string distance metrics, but these are also available and \\nallow for simple unit testing and assessment of accuracy. String comparison evaluators compare \\npredicted strings against reference strings or inputs.\\nString distance evaluators use distance metrics, such as the Levenshtein or Jaro distance, to \\nmeasure the similarity or dissimilarity between predicted and reference strings. This provides a \\nquantitative measure of how similar the predicted string is to the reference string.\\nFinally, there’s an agent trajectory evaluator, where the evaluate_agent_trajectory() method \\nis used to evaluate the input, prediction, and agent trajectory.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 290}, page_content='Generative AI in Production268\\nWe can also use LangSmith, a companion project for LangChain that aims to facilitate the passage \\nof LLM apps from prototype to production, to compare our performance against a dataset. Let’s \\nstep through an example!\\nRunning evaluations against datasets\\nAs we’ve mentioned, comprehensive benchmarking and evaluation, including testing, are critical \\nfor safety, robustness, and intended behavior. We can run evaluations against benchmark datasets \\nin LangSmith as we’ll see now. First, please make sure you create an account on LangSmith here: \\nhttps://smith.langchain.com/.\\nYou can obtain an API key and set it as LANGCHAIN_API_KEY in your environment. We can also set \\nenvironment variables for project ID and tracing:\\nimport os\\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\\nos.environ[\"LANGCHAIN_PROJECT\"] = \"My Project\"\\nThis configures LangChain to log traces. If we don’t tell LangChain the project ID, it will log \\nagainst the default project. After this setup, when we run our LangChain agent or chain, we’ll \\nbe able to see the traces on LangSmith.\\nLet’s log a run!\\nfrom langchain.chat_models import ChatOpenAI\\nllm = ChatOpenAI()\\nllm.predict(\"Hello, world!\")\\nWe can find all these runs on LangSmith. LangSmith lists all runs so far on the LangSmith project \\npage: https://smith.langchain.com/projects\\nWe can also find all runs via the LangSmith API:\\nfrom langsmith import Client\\nclient = Client()\\nruns = client.list_runs()\\nprint(runs)\\nWe can list runs from a specific project or by run_type, for example, chain. Each run comes with \\ninputs and outputs, as runs[0].inputs and runs[0].outputs, respectively.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 291}, page_content='Chapter 9 269\\nWe can create a dataset from existing agent runs with the create_example_from_run() function \\n– or from anything else. Here’s how to create a dataset with a set of questions:\\nquestions = [\\n    \"A ship\\'s parts are replaced over time until no original parts remain. \\nIs it still the same ship? Why or why not?\",  # The Ship of Theseus \\nParadox\\n    \"If someone lived their whole life chained in a cave seeing only \\nshadows, how would they react if freed and shown the real world?\",  # \\nPlato\\'s Allegory of the Cave\\n    \"Is something good because it is natural, or bad because it is \\nunnatural? Why can this be a faulty argument?\",  # Appeal to Nature \\nFallacy\\n    \"If a coin is flipped 8 times and lands on heads each time, what \\nare the odds it will be tails next flip? Explain your reasoning.\",  # \\nGambler\\'s Fallacy\\n    \"Present two choices as the only options when others exist. Is the \\nstatement \\\\\"You\\'re either with us or against us\\\\\" an example of false \\ndilemma? Why?\",  # False Dilemma\\n    \"Do people tend to develop a preference for things simply because they \\nare familiar with them? Does this impact reasoning?\",  # Mere Exposure \\nEffect\\n    \"Is it surprising that the universe is suitable for intelligent life \\nsince if it weren\\'t, no one would be around to observe it?\",  # Anthropic \\nPrinciple\\n    \"If Theseus\\' ship is restored by replacing each plank, is it still the \\nsame ship? What is identity based on?\",  # Theseus\\' Paradox\\n    \"Does doing one thing really mean that a chain of increasingly \\nnegative events will follow? Why is this a problematic argument?\",  # \\nSlippery Slope Fallacy\\n    \"Is a claim true because it hasn\\'t been proven false? Why could this \\nimpede reasoning?\",  # Appeal to Ignorance\\n]\\nshared_dataset_name = \"Reasoning and Bias\"\\nds = client.create_dataset(\\n    dataset_name=shared_dataset_name, description=\"A few reasoning and \\ncognitive bias questions\",'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 292}, page_content='Generative AI in Production270\\n)\\nfor q in questions:\\n    client.create_example(inputs={\"input\": q}, dataset_id=ds.id)\\nWe can then define an LLM agent or chain on the dataset like this:\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.chains import LLMChain\\nllm = ChatOpenAI(model=\"gpt-4\", temperature=0.0)\\ndef construct_chain():\\n    return LLMChain.from_string(\\n        llm,\\n        template=\"Help out as best you can.\\\\nQuestion: {input}\\\\nResponse: \\n\",\\n    )\\nTo run an evaluation on a dataset, we can either specify an LLM or – for parallelism – use a \\nconstructor function to initialize the model or LLM app for each input. Now, to evaluate the per-\\nformance against our dataset, we need to define an evaluator as we saw in the previous section:\\nfrom langchain.smith import RunEvalConfig\\nevaluation_config = RunEvalConfig(\\n    evaluators=[\\n        RunEvalConfig.Criteria({\"helpfulness\": \"Is the response \\nhelpful?\"}),\\n        RunEvalConfig.Criteria({\"insightful\": \"Is the response carefully \\nthought out?\"})\\n    ]\\n)\\nAs seen, the criteria are defined by a dictionary that includes a criterion as a key and a question \\nto check for as the value.\\nWe’ll pass a dataset together with the evaluation configuration with evaluators to run_on_\\ndataset() to generate metrics and feedback:\\nfrom langchain.smith import run_on_dataset\\nresults = run_on_dataset(\\n  client=client,\\n  dataset_name=shared_dataset_name,\\n  dataset=dataset,'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 293}, page_content='Chapter 9 271\\n  llm_or_chain_factory=construct_chain,\\n  evaluation=evaluation_config\\n)\\nSimilarly, we could pass a dataset and evaluators to run_on_dataset() to generate metrics and \\nfeedback asynchronously.\\nWe can view the evaluator feedback in the LangSmith UI to identify areas for improvement:\\nFigure 9.2: Evaluators in LangSmith\\nWe can click on any of these evaluations to see some detail, for example, for the careful thinking \\nevaluator, we get this prompt that includes the original answer from the LLM:\\nYou are assessing a submitted answer on a given task or input based on a \\nset of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: Is something good because it is natural, or bad because it is \\nunnatural? Why can this be a faulty argument?\\n***\\n[Submission]: The argument that something is good because it is natural, \\nor bad because it is unnatural, is often referred to as the \"appeal to \\nnature\" fallacy. This argument is faulty because it assumes that what is \\nnatural is automatically good or beneficial, and what is unnatural is \\nautomatically bad or harmful. However, this is not always the case. For '),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 294}, page_content='Generative AI in Production272\\nexample, many natural substances can be harmful or deadly, such as certain \\nplants or animals. Conversely, many unnatural things, such as modern \\nmedicine or technology, can greatly benefit our lives. Therefore, whether \\nsomething is natural or unnatural is not a reliable indicator of its value \\nor harm.\\n***\\n[Criteria]: insightful: Is the response carefully thought out?\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step \\nmanner your reasoning about each criterion to be sure that your conclusion \\nis correct. Avoid simply stating the correct answers at the outset. Then \\nprint only the single character \"Y\" or \"N\" (without quotes or punctuation) \\non its own line corresponding to the correct answer of whether the \\nsubmission meets all criteria. At the end, repeat just the letter again by \\nitself on a new line.\\nWe get this evaluation:\\nThe criterion is whether the response is insightful and carefully thought \\nout.\\nThe submission provides a clear and concise explanation of the \"appeal to \\nnature\" fallacy, demonstrating an understanding of the concept. It also \\nprovides examples to illustrate why this argument can be faulty, showing \\nthat the respondent has thought about the question in depth. The response \\nis not just a simple yes or no, but a detailed explanation that shows \\ncareful consideration of the question.\\nTherefore, the submission does meet the criterion of being insightful and \\ncarefully thought out.\\nA way to improve performance for a few types of problems is to use few-shot prompting. LangSmith \\ncan help us with this as well. You can find more examples of this in the LangSmith documentation.\\nWe haven’t discussed data annotation queues, a new feature in LangSmith that addresses a \\ncritical gap that emerges after prototyping. Each log can be filtered by attributes such as errors \\nto focus on problematic cases, or manually reviewed and annotated with labels or feedback and \\nedited as needed. Edited logs can be added to a dataset for uses including fine-tuning the model.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 295}, page_content='Chapter 9 273\\nThis concludes the topic of evaluation here. Now that we’ve evaluated our agent, let’s say we are \\nhappy with the performance and have decided to deploy it! What should we do next?\\nHow to deploy LLM apps\\nGiven the increasing use of LLMs in various sectors, it’s imperative to understand how to effectively \\ndeploy models and apps into production. Deployment services and frameworks can help to scale \\nthe technical hurdles. There are lots of different ways to productionize LLM apps or applications \\nwith generative AI.\\nDeployment for production requires research into, and knowledge of, the generative AI ecosystem, \\nwhich encompasses different aspects including:\\n• Models and LLM-as-a-Service: LLMs and other models either run on-premises or offered \\nas an API on vendor-provided infrastructure.\\n• Reasoning heuristics: Retrieval Augmented Generation (RAG), Tree-of-Thought, and \\nothers.\\n• Vector databases: Aid in retrieving contextually relevant information for prompts.\\n• Prompt engineering tools: These facilitate in-context learning without requiring expen-\\nsive fine-tuning or sensitive data.\\n• Pre-training and fine-tuning: For models specialized for specific tasks or domains.\\n• Prompt logging, testing, and analytics : An emerging sector inspired by the desire to \\nunderstand and improve the performance of LLMs.\\n• Custom LLM stack: A set of tools for shaping and deploying solutions built on LLMs.\\nWe discussed models in Chapter 1, What Is Generative AI? and Chapter 3, Getting Started with Lang-\\nChain, reasoning heuristics in Chapter 4, Building Capable Assistants - Chapter 7, LLMs for Data Sci-\\nence, vector databases in Chapter 5, Building a Chatbot like ChatGPT, and prompts and fine-tuning \\nin Chapter 8, Customizing LLMs and Their Output. In the present chapter, we’ll focus on logging, \\nmonitoring, and custom tools for deployment.\\nLLMs are typically utilized using external LLM providers or self-hosted models. With external \\nproviders, computational burdens are shouldered by companies such as OpenAI or Anthropic, \\nwhile LangChain facilitates business logic implementation. However, self-hosting open-source \\nLLMs can significantly decrease costs, latency, and privacy concerns.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 296}, page_content='Generative AI in Production274\\nSome tools with infrastructure offer the full package. For example, you can deploy LangChain \\nagents with Chainlit, creating ChatGPT-like UIs with Chainlit. Key features include intermediary \\nstep visualization, element management and display (images, text, carousel, and others), and \\ncloud deployment. BentoML is a framework that enables the containerization of machine learning \\napplications to use them as microservices running and scaling independently with automatic \\ngeneration of OpenAPI and gRPC endpoints.\\nYou can also deploy LangChain to different cloud service endpoints, for example, an Azure Machine \\nLearning online endpoint. With Steamship, LangChain developers can rapidly deploy their apps, \\nwith features including production-ready endpoints, horizontal scaling across dependencies, \\npersistent storage of app state, and multi-tenancy support.\\nLangChain AI, the company maintaining LangChain, is developing a new library called LangServe. \\nBuilt on top of FastAPI and Pydantic, it streamlines documentation and deployment. Deployment \\nis further facilitated through integration with platforms including GCP’s Cloud Run and Replit, \\nallowing quick cloning from an existing GitHub repository. Additional deployment instructions \\nfor other platforms will follow shortly based on user input.\\nThe following table summarizes the services and frameworks available for deploying LLM ap -\\nplications:\\nName Description Type\\nStreamlit\\nOpen-source Python framework for building and deploying \\nweb apps Framework\\nGradio\\nLets you wrap models in an interface and host on Hugging \\nFace Framework\\nChainlit Build and deploy conversational ChatGPT-like apps Framework\\nApache Beam\\nTool for defining and orchestrating data processing \\nworkflows Framework\\nVercel Platform for deploying and scaling web apps Cloud service\\nFastAPI Python web framework for building APIs Framework\\nFly.io App hosting platform with autoscaling and global CDN Cloud service\\nDigitalOcean \\nApp Platform Platform to build, deploy, and scale apps Cloud service\\nGoogle Cloud\\nServices such as Cloud Run to host and scale containerized \\napps Cloud service'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 297}, page_content='Chapter 9 275\\nSteamship\\nML infrastructure platform for deploying and scaling \\nmodels Cloud service\\nLangchain-Serve Tool to serve LangChain agents as web APIs Framework\\nBentoML Framework for model serving, packaging, and deployment Framework\\nOpenLLM Provides open APIs to commercial LLMs Cloud service\\nDatabutton No-code platform to build and deploy model workflows Framework\\nAzure ML Managed MLOps service on Azure for models Cloud service\\nLangServe\\nBuilt on top of FastAPI, but specialized for LLM app \\ndeployment Framework\\nTable 9.1: Services and frameworks for deploying LLM applications\\nAll of these are well documented with different use cases, often directly referencing LLMs. We’ve \\nalready shown examples with Streamlit and Gradio, and we’ve discussed how to deploy them to \\nthe Hugging Face Hub as an example.\\nThere are a few main requirements for running LLM applications:\\n• Scalable infrastructure to handle computationally intensive models and potential spikes \\nin traffic\\n• Low latency for real-time serving of model outputs\\n• Persistent storage for managing long conversations and app state\\n• APIs for integration into end-user applications\\n• Monitoring and logging to track metrics and model behavior\\nMaintaining cost efficiency can be challenging with large volumes of user interactions and the \\nhigh costs associated with LLM services. Strategies to manage efficiency include self-hosting \\nmodels, auto-scaling resource allocations based on traffic, using spot instances, independent \\nscaling, and batching requests to better utilize GPU resources.\\nThe choice of tools and the infrastructure determines trade-offs between these requirements. \\nFlexibility and ease is very important, because we want to be able to iterate rapidly, which is \\nvital due to the dynamic nature of ML and LLM landscapes. It’s crucial to avoid getting tied to \\none solution. A flexible, scalable serving layer that accommodates various models is key. Model \\ncomposition and cloud providers’ selection forms part of this flexibility equation.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 298}, page_content='Generative AI in Production276\\nFor the greatest degree of flexibility, Infrastructure as Code (IaC) tools such as Terraform, Cloud-\\nFormation, or Kubernetes YAML files can recreate your infrastructure reliably and quickly. More-\\nover, continuous integration and continuous delivery (CI/CD) pipelines can automate testing \\nand deployment processes to reduce errors and facilitate quicker feedback and iteration.\\nDesigning a robust LLM application service can be a complex task requiring an understanding \\nof the trade-offs and critical considerations when evaluating serving frameworks. Leveraging \\none of these solutions for deployment allows developers to focus on developing impactful AI \\napplications rather than infrastructure.\\nAs mentioned, LangChain plays nicely with several open-source projects and frameworks such \\nas Ray Serve, BentoML, OpenLLM, Modal, and Jina. In the next sections, we’ll deploy apps using \\ndifferent tools. We’ll start with a chat service web server based on FastAPI.\\nFastAPI web server\\nFastAPI is a very popular choice for the deployment of web servers. Designed to be fast, easy to \\nuse, and efficient, it is a modern, high-performance web framework for building APIs with Python. \\nLanarky is a small, open-source library for deploying LLM applications that provides convenient \\nwrappers around Flask API as well as Gradio for the deployment of LLM applications. This means \\nyou can get a REST API endpoint as well as the in-browser visualization at once and you only \\nneed a few lines of code.\\nIn the library documentation, there are several examples, including a Retrieval QA with Sources \\nChain, a Conversational Retrieval app, and a Zero Shot agent. Following another example, we’ll \\nimplement a chatbot web server with Lanarky.\\nWe’ll set up a web server using Lanarky that creates a ConversationChain instance with an LLM \\nmodel and settings, and defines routes for handling HTTP requests. The full code for this recipe \\nis available here: https://github.com/benman1/generative_ai_with_langchain/tree/main/\\nwebserver\\nA Representational State Transfer Application Programming Interface ( REST \\nAPI) is a set of rules  and protocols that allows different software applications to \\ncommunicate with each other over the internet. It follows the principles of REST, \\nwhich is an architectural style for designing networked applications. A REST API \\nuses HTTP methods (such as GET, POST, PUT, or DELETE) to perform operations \\non resources, and it typically sends and receives data in a standardized format, such \\nas JSON or XML.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 299}, page_content='Chapter 9 277\\nFirst, we’ll import the necessary dependencies, including FastAPI for creating the web server and \\nConversationChain and ChatOpenAI from LangChain for handling LLM conversations, along \\nwith some other required modules:\\nfrom fastapi import FastAPI\\nfrom langchain import ConversationChain\\nfrom langchain.chat_models import ChatOpenAI\\nfrom lanarky import LangchainRouter\\nfrom starlette.requests import Request\\nfrom starlette.templating import Jinja2Templates\\nPlease note that you need to set your environment variables as explained in Chapter 3, Getting \\nStarted with LangChain. We can do this by importing the setup_environment() method from the \\nconfig module as we’ve seen in many other examples before: \\nfrom config import set_environment\\nset_environment()\\nNow we create a FastAPI app, which will take care of most of the routing, except for LangChain \\nspecific requests that Lanarky will cover as we’ll see later:\\napp = FastAPI()\\nWe can create an instance of ConversationChain, specifying the LLM model and its settings:\\nchain = ConversationChain(\\n        llm=ChatOpenAI(\\n            temperature=0,\\n            streaming=True,\\n        ),\\n        verbose=True,\\n    ) \\nThe templates variable gets set to a Jinja2Templates class, specifying the directory where tem-\\nplates are located for rendering. This specifies how the webpage will be shown, allowing all kinds \\nof customization:\\ntemplates = Jinja2Templates(directory=\"webserver/templates\")'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 300}, page_content='Generative AI in Production278\\nAn endpoint for handling HTTP GET requests at the root path ( /) is defined using the FastAPI \\ndecorator @app.get. The function associated with this endpoint returns a template response for \\nrendering the index.html template:\\n@app.get(\"/\")\\nasync def get(request: Request):\\n    return templates.TemplateResponse(\"index.html\", {\"request\": request})\\nA router object is created as a LangChainRouter class. This object is responsible for defining and \\nmanaging the routes associated with the ConversationChain instance. We can add additional \\nroutes to the router for handling JSON-based chat that even work with WebSocket requests:\\nlangchain_router = LangchainRouter(\\n    langchain_url=\"/chat\", langchain_object=chain, streaming_mode=1\\n)\\nlangchain_router.add_langchain_api_route(\\n    \"/chat_json\", langchain_object=chain, streaming_mode=2\\n)\\nlangchain_router.add_langchain_api_websocket_route(\"/ws\", langchain_\\nobject=chain)\\napp.include_router(langchain_router)\\nNow our application knows how to handle requests made to the specified routes defined within \\nthe router, directing them to the appropriate functions or handlers for processing.\\nWe will use Uvicorn to run our application. Uvicorn excels in supporting high-performance, asyn-\\nchronous frameworks such as FastAPI and Starlette. It is known for its ability to handle a large \\nnumber of concurrent connections and performs well under heavy loads due to its asynchronous \\nnature.\\nWe can run the web server from the terminal like this:\\nuvicorn webserver.chat:app –reload\\nThis command starts a web server, which you can view in your browser, at this local address: \\nhttp://127.0.0.1:8000\\nThe reload switch (--reload) is particularly handy, because it means the server will be automat-\\nically restarted once you’ve made any changes.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 301}, page_content='Chapter 9 279\\nHere’s a snapshot of the chatbot application we’ve just deployed:\\nFigure 9.3: Chatbot in Flask/Lanarky\\nI think this looks quite nice for what little work we’ve put in. It also comes with a few nice features \\nsuch as a REST API, a web UI, and a WebSocket interface. While Uvicorn itself does not provide \\nbuilt-in load balancing functionality, it can work together with other tools or technologies such \\nas Nginx or HAProxy to achieve load balancing in a deployment setup, which distributes the \\nincoming client requests across multiple worker processes or instances. The use of Uvicorn with \\nload balancers enables horizontal scaling to handle large traffic volumes, improves response times \\nfor clients, and enhances fault tolerance. Finally, Lanarky also plays nicely with Gradio, so with a \\nfew extra lines we have this webserver running as a Gradio app up and running.\\nIn the next section, we’ll see how to build robust and cost-effective generative AI applications \\nwith Ray. We’ll build a simple search engine using LangChain for text processing and then use \\nRay for scaling indexing and serving.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 302}, page_content='Generative AI in Production280\\nRay\\nRay provides a flexible framework to meet the infrastructure challenges of complex neural net -\\nworks in production by scaling out generative AI workloads across clusters. Ray helps with com-\\nmon deployment needs such as low-latency serving, distributed training, and large-scale batch \\ninference. Ray also makes it easy to spin up on-demand fine-tuning or scale existing workloads \\nfrom one machine to many. Its capabilities include:\\n• Scheduling distributed training jobs across GPU clusters using Ray Train\\n• Deploying pre-trained models at scale for low-latency serving with Ray Serve\\n• Running large batch inference in parallel across CPUs and GPUs with Ray Data\\n• Orchestrating end-to-end generative AI workflows combining training, deployment, and \\nbatch processing\\nWe’ll use LangChain and Ray to build a simple search engine for the Ray documentation following \\nan example implemented by Waleed Kadous for the anyscale Blog and on the langchain-ray \\nrepository on GitHub. This can be found here: https://www.anyscale.com/blog/llm-open-\\nsource-search-engine-langchain-ray\\nYou can see this as an extension of the recipe in Chapter 5, Building a Chatbot like ChatGPT. You’ll \\nalso see how to run this as a FastAPI server. The full code for this recipe under semantic search \\nis available here: https://github.com/benman1/generative_ai_with_langchain/tree/main/\\nsearch_engine.\\nFirst, we’ll ingest and index the Ray docs so we can quickly find relevant passages for a search \\nquery:\\n# Load the Ray docs using the LangChain loader\\nloader = RecursiveUrlLoader(\"docs.ray.io/en/master/\")\\ndocs = loader.load()\\n# Split docs into sentences using LangChain splitter\\nchunks = text_splitter.create_documents(\\n    [doc.page_content for doc in docs],\\n    metadatas=[doc.metadata for doc in docs])\\n# Embed sentences into vectors using transformers\\nembeddings = LocalHuggingFaceEmbeddings(\\'multi-qa-mpnet-base-dot-v1\\') '),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 303}, page_content=\"Chapter 9 281\\n# Index vectors using FAISS via LangChain\\ndb = FAISS.from_documents(chunks, embeddings)\\nThis builds our search index by ingesting the docs, splitting them into chunks, embedding the \\nsentences, and indexing the vectors. Alternatively, we can accelerate the indexing by parallelizing \\nthe embedding step:\\n# Define shard processing task\\n@ray.remote(num_gpus=1) \\ndef process_shard(shard):\\n  embeddings = LocalHuggingFaceEmbeddings('multi-qa-mpnet-base-dot-v1')\\n  return FAISS.from_documents(shard, embeddings)\\n# Split chunks into 8 shards\\nshards = np.array_split(chunks, 8) \\n# Process shards in parallel\\nfutures = [process_shard.remote(shard) for shard in shards]\\nresults = ray.get(futures)\\n# Merge index shards\\ndb = results[0]\\nfor result in results[1:]:\\n  db.merge_from(result)\\nBy running embedding on each shard in parallel, we can significantly reduce the indexing time.\\nWe save the database index to disk:\\ndb.save_local(FAISS_INDEX_PATH)\\nFAISS_INDEX_PATH is an arbitrary file name. I’ve set it to faiss_index.db.\\nNext, we’ll see how we can serve search queries with Ray Serve:\\n# Load index and embedding\\ndb = FAISS.load_local(FAISS_INDEX_PATH)\\nembedding = LocalHuggingFaceEmbeddings('multi-qa-mpnet-base-dot-v1')\\n@serve.deployment\"),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 304}, page_content='Generative AI in Production282\\nclass SearchDeployment:\\n  def __init__(self):\\n    self.db = db\\n    self.embedding = embedding\\n \\n  def __call__(self, request):  \\n    query_embed = self.embedding(request.query_params[\"query\"])\\n    results = self.db.max_marginal_relevance_search(query_embed)\\n    return format_results(results)\\ndeployment = SearchDeployment.bind()\\n# Start service\\nserve.run(deployment)\\nThis should load the index we generated and lets us serve search queries as a web endpoint!\\nIf we save this to a file called serve_vector_store.py, we can get the server up and running using \\nthe following command from the search_engine directory:\\nPYTHONPATH=../ python serve_vector_store.py\\nRunning this command in the terminal gives me this output:\\nStarted a local Ray instance.\\nView the dashboard at 127.0.0.1:8265\\nThe message shows us the URL of the dashboard, which we can access in the browser. The search \\nserver, however, is running on localhost on port 8080. We can query it from Python:\\nimport requests\\nquery = \"What are the different components of Ray\"\\n         \" and how can they help with large language models (LLMs)?\"\\nresponse = requests.post(\"http://localhost:8000/\", params={\"query\": \\nquery})\\nprint(response.text)'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 305}, page_content='Chapter 9 283\\nFor me, the server fetches the Ray use cases page at: https://docs.ray.io/en/latest/ray-\\noverview/use-cases.html\\nWhat I really liked was the monitoring with the Ray Dashboard, which looks like this:\\nFigure 9.4: Ray Dashboard\\nThis dashboard is very powerful as it can give you a whole bunch of metrics and other information. \\nCollecting metrics is easy, since all you must do is set up and update variables of type Counter, \\nGauge, Histogram, and others within the deployment object or actor. For time series charts, you \\nshould have either Prometheus or the Grafana server installed.\\nThis practical guide has taken you through the key steps of deploying an LLM application locally \\nusing LangChain and Ray. We first ingested and indexed documents to power a semantic search \\nengine over the Ray documentation. By leveraging Ray’s distributed capabilities, we parallelized \\nthe intensive embedding task to accelerate the indexing time. We then served the search applica-\\ntion via Ray Serve, which provides a flexible framework for low-latency querying. The Ray dash-\\nboard offered helpful monitoring insights into metrics such as request rates, latencies, and errors.\\nAs you can see in the full implementation on GitHub, we can also spin this up as a FastAPI server. \\nThis concludes our simple semantic search engine with LangChain and Ray.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 306}, page_content='Generative AI in Production284\\nAs models and LLM apps grow more sophisticated and highly interwoven into the fabric of busi-\\nness applications, observability and monitoring during production become necessary to ensure \\ntheir accuracy, efficiency, and reliability is ongoing. The next section focuses on the significance \\nof monitoring LLMs and highlights key metrics to track for a comprehensive monitoring strategy.\\nHow to observe LLM apps\\nThe dynamic nature of real-world operations means that the conditions assessed during offline \\nevaluations hardly cover all potential scenarios that LLMs may encounter in production systems. \\nThus comes the need for observability in production – a more continuous, real-time observation \\nto capture anomalies that offline tests could not anticipate.\\nWe need to implement monitoring tools to track vital metrics regularly. This includes user activ-\\nity, response times, traffic volumes, financial expenditures, model behavior patterns, and overall \\nsatisfaction with the app. Ongoing surveillance allows for the early detection of anomalies such \\nas data drift or unexpected lapses in capabilities.\\nObservability allows monitoring behaviors and outcomes as the model interacts with actual in-\\nput data and users in production. It includes logging, tracking, tracing, and alerting mechanisms \\nto ensure healthy system functioning, performance optimization, and catching issues such as \\nmodel drift early.\\nTracking, tracing, and monitoring are three important concepts in the field of soft-\\nware operation and management. While all related to understanding and improving \\na system’s performance, they each have distinct roles. While tracking and tracing \\nare about keeping detailed historical records for analysis and debugging, monitor -\\ning is aimed at real-time observation and immediate awareness of issues to ensure \\noptimal system functionality at all times. All three of these concepts fall within the \\ncategory of observability.\\nMonitoring is the ongoing process of overseeing the performance of a system or \\napplication. This might involve continuously collecting and analyzing metrics re -\\nlated to system health such as memory usage, CPU utilization, network latency, \\nand the overall application/service performance (such as response time). Effective \\nmonitoring includes setting up alert systems for anomalies or unexpected behav -\\niors – sending notifications when certain thresholds are exceeded. While tracking \\nand tracing are about keeping detailed historical records for analysis and debugging, \\nmonitoring is aimed at real-time observation and immediate awareness of issues to \\nensure optimal system functionality at all times.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 307}, page_content='Chapter 9 285\\nThe chief aim for monitoring and observability is to provide insights into LLM app performance \\nand behavior through real-time data. This helps to do the following:\\n• Preventing model drift: LLM performance can degrade over time due to changes in the \\ncharacteristics of input data or user behavior. Regular monitoring can identify such sit -\\nuations early and apply corrective measures.\\n• Performance optimization: By tracking metrics such as inference times, resource usage, \\nand throughput, you can make adjustments to improve the efficiency and effectiveness \\nof LLM apps in production.\\n• A/B testing: Helps compare how slight differences in models may result in different out-\\ncomes, which aids decision-making for model improvements.\\n• Debugging issues: Monitoring helps identify unforeseen problems that can occur during \\nruntime, enabling rapid resolution.\\n• Avoiding hallucinations: We want to ensure the factual accuracy of the response, and – \\nif we are using RAG – retrieved context quality, and sufficient effectiveness in using the \\ncontext.\\n• Ensuring appropriate behavior: Responses should be relevant, complete, helpful, harm-\\nless, conform to the required format, and follow the user’s intent.\\nSince there are so many ways to monitor, it’s important to come up with a monitoring strategy. \\nSome things you should consider when coming up with a strategy are:\\n• Metrics to monitor: Define key metrics of interest such as prediction accuracy, latency, \\nthroughput, and others based on the desired model performance.\\n• Monitoring frequency: Frequency should be determined based on how critical the model \\nis to operations – a highly critical model may require near real-time monitoring.\\n• Logging: Logs should provide comprehensive details regarding every relevant action \\nperformed by the LLM so analysts can track down any anomalies.\\n• Alerting mechanism: The system should raise alerts if it detects anomalous behavior or \\ndrastic performance drops.\\nMonitoring LLMs and LLM apps in production serves multiple purposes, including assessing mod-\\nel performance, detecting abnormalities or issues, optimizing resource utilization, and ensuring \\nconsistent and high-quality outputs. By continuously evaluating the behavior and performance \\nof LLM apps via validation, shadow launches, and interpretation along with dependable offline \\nevaluation, organizations can identify and mitigate potential risks, maintain user trust, and \\nprovide an optimal experience.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 308}, page_content='Generative AI in Production286\\nWhen monitoring LLMs and LLM applications, organizations can rely on a diverse set of metrics to \\ngauge different aspects of performance and user experience. Beyond the crucial metrics of tonality, \\ntoxicity, and harmlessness, here is an expanded list that captures a wider range of evaluation areas:\\n• Inference latency: Measures the time it takes for the LLM app to process a request and \\ngenerate a response. Lower latency ensures a faster and more responsive user experience.\\n• Query per Second (QPS): Calculates the number of queries or requests that the LLM can \\nhandle within a given time frame. Monitoring QPS helps assess scalability and capacity \\nplanning.\\n• Token per Second (TPS) : Tracks the rate at which the LLM app generates tokens. TPS \\nmetrics are useful for estimating computational resource requirements and understanding \\nmodel efficiency.\\n• Token usage: The number of tokens correlates with the resource usage such as hardware \\nutilization, latency, and costs.\\n• Error rate: Monitors the occurrence of errors or failures in LLM app responses, ensuring \\nerror rates are kept within acceptable limits to maintain the quality of outputs.\\n• Resource utilization: Measures the consumption of computational resources, such as \\nthe CPU, memory, and GPU, to reduce costs and avoid bottlenecks.\\n• Model drift: Detects changes in LLM app behavior over time by comparing its outputs \\nto a baseline or ground truth, ensuring the model remains accurate and aligned with \\nexpected outcomes.\\n• Out-of-distribution inputs: Identifies inputs or queries falling outside the intended dis-\\ntribution of the LLM’s training data, which can cause unexpected or unreliable responses.\\n• User feedback metrics: Monitors user feedback channels to gather insights on user sat -\\nisfaction, identify areas for improvement, and validate the effectiveness of the LLM app.\\n• User engagement: We can track how users engage with our app; for example, the frequency \\nand duration of sessions or the usage of specific features.\\n• Tool/retrieval usage: Breakdown of the instances when retrieval and tools are used.\\nThis is just a small selection. This list can easily be extended with many more metrics from Site \\nReliability Engineering (SRE) relating to task performance or the behavior of the LLM app.\\nData scientists and  machine learning engineers should check for staleness, incorrect learning, \\nand bias using model interpretation tools such as LIME and SHAP. The most predictive features \\nchanging suddenly could indicate a data leak.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 309}, page_content='Chapter 9 287\\nOffline metrics such as AUC do not always correlate with online impacts on conversion rate, so \\nit is important to find dependable offline metrics that translate to online gains relevant to the \\nbusiness, ideally direct metrics such as clicks and purchases that the system impacts directly.\\nEffective monitoring enables the successful deployment and utilization of LLMs, boosting con-\\nfidence in their capabilities and fostering user trust. It should be cautioned, however, that you \\nshould study service providers’ privacy and data protection policies when relying on cloud service \\nplatforms.\\nThe full code for the recipes in this section are available on GitHub in the monitoring_and_\\nevaluation directory of the repository corresponding to this book.\\nIn the next section, we’ll start our journey into observability by monitoring the trajectory of an \\nagent.\\nTracking responses\\nTracking in this context refers to recording the full provenance of responses, including the tools, \\nretrievals, the included data, and the LLM used in generating the output. This is key for auditing \\nand reproducibility of responses. We’ll use the terms tracking and tracing interchangeably in \\nthis section.\\nTracking generally refers to the process of recording and managing information \\nabout a particular operation or series of operations within an application or system. \\nFor example, in machine learning applications or projects, tracking can involve keep-\\ning a record of parameters, hyperparameters, metrics, and outcomes across different \\nexperiments or runs. It provides a way to document progress and changes over time.\\nTracing is a more specialized form of tracking. It involves recording the execution \\nflow through software/systems. Particularly in distributed systems where a single \\ntransaction might span multiple services, tracing helps in maintaining an audit or \\nbreadcrumb trail, a detailed source of information about that request path through \\nthe system. This granular view enables developers to understand the interaction \\nbetween various microservices and troubleshoot issues such as latency or failures \\nby identifying exactly where they occurred in the transaction path.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 310}, page_content='Generative AI in Production288\\nTracking the trajectory of agents can be challenging due to their broad range of actions and \\ngenerative capabilities. LangChain comes with functionality for trajectory tracking and eval -\\nuation, so seeing the traces of an agent via LangChain is really easy! You just have to set the \\nreturn_intermediate_steps parameter to True when initializing an agent or an LLM.\\nLet’s define a tool as a function. It’s convenient to re-use the function docstring as a description of \\nthe tool. The tool first sends a ping to a website address and returns information about packages \\ntransmitted and latency or – in the case of an error – the error message:\\nimport subprocess\\nfrom urllib.parse import urlparse\\nfrom pydantic import HttpUrl\\nfrom langchain.tools import StructuredTool\\ndef ping(url: HttpUrl, return_error: bool) -> str:\\n    \"\"\"Ping the fully specified url. Must include https:// in the url.\"\"\"\\n    hostname = urlparse(str(url)).netloc\\n    completed_process = subprocess.run(\\n    [\"ping\", \"-c\", \"1\", hostname], capture_output=True, text=True\\n    )\\n    output = completed_process.stdout\\n    if return_error and completed_process.returncode != 0:\\n        return completed_process.stderr\\n    return output\\nping_tool = StructuredTool.from_function(ping)\\nNow we set up an agent that uses this tool with an LLM to make the calls given a prompt:\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.agents import initialize_agent, AgentType\\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0613\", temperature=0)\\nagent = initialize_agent(\\n    llm=llm,\\n    tools=[ping_tool],\\n    agent=AgentType.OPENAI_MULTI_FUNCTIONS,\\n    return_intermediate_steps=True, # IMPORTANT!'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 311}, page_content='Chapter 9 289\\n)\\nresult = agent(\"What\\'s the latency like for https://langchain.com?\")\\nThe agent reports this:\\nThe latency for https://langchain.com is 13.773 ms\\nIn results[\"intermediate_steps\"], we can see a lot of information about the agent’s actions:\\n[(_FunctionsAgentAction(tool=\\'ping\\', tool_input={\\'url\\': \\'https://\\nlangchain.com\\', \\'return_error\\': False}, log=\"\\\\nInvoking: `ping` with \\n`{\\'url\\': \\'https://langchain.com\\', \\'return_error\\': False}`\\\\n\\\\n\\\\n\", message_\\nlog=[AIMessage(content=\\'\\', additional_kwargs={\\'function_call\\': {\\'name\\': \\n\\'tool_selection\\', \\'arguments\\': \\'{\\\\n  \"actions\": [\\\\n    {\\\\n      \"action_\\nname\": \"ping\",\\\\n      \"action\": {\\\\n        \"url\": \"https://langchain.\\ncom\",\\\\n        \"return_error\": false\\\\n      }\\\\n    }\\\\n  ]\\\\n}\\'}}, \\nexample=False)]), \\'PING langchain.com (35.71.142.77): 56 data bytes\\\\\\nn64 bytes from 35.71.142.77: icmp_seq=0 ttl=249 time=13.773 ms\\\\\\nn\\\\n--- langchain.com ping statistics ---\\\\n1 packets transmitted, 1 \\npackets received, 0.0% packet loss\\\\nround-trip min/avg/max/stddev = \\n13.773/13.773/13.773/0.000 ms\\\\n\\')]\\nBy providing visibility into the system and aiding in problem identification and optimization \\nefforts, this kind of tracking and evaluation can be very helpful.\\nThe LangChain documentation demonstrates how to use a trajectory evaluator to examine the \\nfull sequence of actions and responses they generate and grade an OpenAI functions agent. That’s \\npotentially very powerful stuff!\\nLet’s have a look beyond LangChain and see what else is out there for observability!\\nObservability tools\\nThere are quite a few tools available as integrations in LangChain or through callbacks:\\n• Argilla: Argilla is an open-source data curation platform that can integrate user feed -\\nback (human-in-the-loop workflows) with prompts and responses to curate datasets \\nfor fine-tuning.\\n• Portkey: Portkey adds essential MLOps capabilities like monitoring detailed metrics, trac-\\ning chains, caching, and reliability through automatic retries to LangChain.\\n• Comet�ml: Comet offers robust MLOps capabilities for tracking experiments, comparing \\nmodels and optimizing AI projects.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 312}, page_content='Generative AI in Production290\\n• LLMonitor: Tracks lots of metrics including cost and usage analytics (user tracking), \\ntracing, and evaluation tools (open-source).\\n• DeepEval: Logs default metrics including relevance, bias, and toxicity. Can also help with \\ntesting and monitoring model drift or degradation.\\n• Aim: An open-source visualization and debugging platform for ML models. It logs inputs, \\noutputs, and the serialized state of components, enabling visual inspection of individual \\nLangChain executions and comparing multiple executions side by side.\\n• Argilla: An open-source platform for tracking training data, validation accuracy, param-\\neters, and more across machine learning experiments.\\n• Splunk: Splunk’s Machine Learning Toolkit can provide observability into your machine \\nlearning models in production.\\n• ClearML: An open-source tool for automating training pipelines, seamlessly moving from \\nresearch to production.\\n• IBM Watson OpenScale: A platform providing insights into AI health with fast problem \\nidentification and resolution to help mitigate risks.\\n• DataRobot MLOps: Monitors and manages models to detect issues before they impact \\nperformance.\\n• Datadog APM integration: This integration allows you to capture LangChain requests, \\nparameters, prompt completions, and visualize LangChain operations. You can also cap-\\nture metrics such as request latency, errors, and token/cost usage.\\n• Weights and Biases (W&B) tracing: We’ve already shown an example of using W&B to \\nmonitor fine-training convergence, but it can also fulfill the roles of tracking other metrics \\nand logging and comparing prompts.\\n• Langfuse: With this open-source tool, we can conveniently monitor detailed information \\nalong traces regarding the latency, cost, and scores of our LangChain agents and tools.\\n• LangKit: This extracts signals from prompts and responses to ensure safety and security. \\nIt currently focuses on text quality, relevance metrics, and sentiment analysis.\\nThere are more tools out there at different stages of maturation. For example, the AgentOps SDK \\nis aiming to provide an interface to a toolkit for evaluating and developing robust and reliable AI \\nagents, but is still in closed alpha.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 313}, page_content='Chapter 9 291\\nMost of these integrations are very easy to integrate into LLM pipelines. For example, for W&B, \\nyou can enable tracing by setting the LANGCHAIN_WANDB_TRACING environment variable to True. \\nAlternatively, you can use a context manager with wandb_tracing_enabled() to trace a specific \\nblock of code. With Langfuse, we can hand over langfuse.callback.CallbackHandler() as an \\nargument to the chain.run() call.\\nSome of these tools are open-source, and what’s great about these platforms is that they allow \\nfull customization and on-premises deployment for use cases where privacy is important. For \\nexample, Langfuse is open-source and provides an option of self-hosting. Choose the option that \\nbest suits your needs and follow the instructions provided in the LangChain documentation to \\nenable tracing for your agents. Having been released only recently, I am sure there’s much more to \\ncome for the platform, but it’s already great to see traces of how agents execute, detecting loops \\nand latency issues. It enables sharing traces and stats with collaborators to discuss improvements.\\nLet’s have a look at LangSmith now, which is another companion project of LangChain, developed \\nfor observability!\\nLangSmith\\nLangSmith is a framework for debugging, testing, evaluating, and monitoring LLM applications \\ndeveloped and maintained by LangChain AI, the organization behind LangChain. LangSmith \\nserves as an effective tool for MLOps, specifically for LLMs, by providing features that cover \\nmultiple aspects of the MLOps process. It can help developers take their LLM applications from \\nprototype to production by providing features for debugging, monitoring, and optimizing.\\nLangSmith allows you to:\\n• Log traces of runs from your LangChain agents, chains, and other components\\n• Create datasets to benchmark model performance\\n• Configure AI-assisted evaluators to grade your models\\n• View metrics, visualizations, and feedback to iterate and improve your LLMs'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 314}, page_content='Generative AI in Production292\\nOn the LangSmith web interface, we can get a large set of graphs for a bunch of statistics that can \\nbe useful to optimize latency, hardware efficiency, and cost, as we can see here in the monitoring \\ndashboard:\\nFigure 9.5: Evaluator metrics in LangSmith\\nThe monitoring dashboard includes the following graphs that can be broken down into different \\ntime intervals: \\nStatistics Category\\nTrace Count, LLM Call Count, Trace Success Rates, LLM Call Success Rates Volume\\nTrace Latency (s), LLM Latency (s), LLM Calls per Trace, Tokens / sec Latency\\nTotal Tokens, Tokens per Trace, Tokens per LLM Call Tokens\\n% Traces w/ Streaming, % LLM Calls w/ Streaming, Trace Time-to-First-Token (ms), \\nLLM Time-to-First-Token (ms)\\nStreaming\\nTable 9.2: Statistics in LangSmith'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 315}, page_content='Chapter 9 293\\nHere’s a tracing example in LangSmith for the benchmark dataset run that we saw in the How to \\nevaluate LLM apps section:\\nFigure 9.6: Tracing in LangSmith\\nThe platform itself is not open-source, however, LangChain AI, the company behind LangSmith \\nand LangChain, provides some support for self-hosting for organizations with privacy concerns. \\nThere are, however, a few alternatives to LangSmith such as Langfuse, Weights and Biases, Dat-\\nadog APM, Portkey, and PromptWatch, with some overlap in features. We’ll focus on LangSmith \\nhere because it has a large set of features for evaluation and monitoring, and because it integrates \\nwith LangChain.\\nIn the next section, we’ll demonstrate the utilization of PromptWatch for prompt tracking of \\nLLMs in production environments.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 316}, page_content='Generative AI in Production294\\nPromptWatch\\nPromptWatch records information about response caching, chain execution, prompting and gen-\\nerated output during interactions. The tracing and monitoring can be very useful for debugging \\nand ensuring an audit trail. With PromptWatch.io, you can even track various aspects of LLM \\nchains, actions, retrieved documents, inputs, outputs, execution time, tool details, and more for \\ncomplete visibility in your system.\\nMake sure you sign up with PromptWatch.io online and get your API key – you can find it under \\nthe account settings.\\nLet’s get the inputs out of the way:\\nfrom langchain import LLMChain, OpenAI, PromptTemplate\\nfrom promptwatch import PromptWatch\\nAs discussed in Chapter 3, Getting Started with LangChain, I’ve set all API keys in the environment \\nin the set_environment() function. If you’ve followed my recommendation, you can follow the \\nimports up with this:\\nfrom config import set_environment\\nset_environment()\\nOtherwise, please make sure you set your environment variables in the way you prefer. Next, we \\nneed to set up a prompt and a chain:\\nprompt_template = PromptTemplate.from_template(\"Finish this sentence \\n{input}\")\\nmy_chain = LLMChain(llm=OpenAI(), prompt=prompt_template)\\nUsing the PromptTemplate class, the prompt template is configured with one variable, input, \\nindicating where the user input should be placed within the prompt.\\nWe can create a PromptWatch block, where LLMChain is invoked with an input prompt:\\nwith PromptWatch() as pw:\\n    my_chain(\"The quick brown fox jumped over\")\\nThis is a simple example of the model generating a response based on the provided prompt. We \\ncan see this on PromptWatch.io.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 317}, page_content='Chapter 9 295\\nFigure 9.7: Prompt tracking at PromptWatch.io\\nWe can see the prompt together with the LLM’s response. We also get a dashboard with a time \\nseries of activity, where we can drill down into responses at certain times. This seems quite useful \\nto effectively monitor and analyze prompts, outputs, and costs in real-world scenarios.\\nThe platform allows for in-depth analysis and troubleshooting in the web interface that enables \\nusers to identify the root causes of issues and optimize prompt templates. We could have explored \\nmore, for example around prompt templates and versioning, but there’s only so much we can \\ncover here. promptwatch.io can also help with unit testing and versioning prompt templates.\\nSummary\\nTaking a trained LLM from research into real-world production involves navigating many complex \\nchallenges around aspects such as scalability, monitoring, and unintended behaviors. Responsibly \\ndeploying capable, reliable models involves diligent planning around scalability, interpretability, \\ntesting, and monitoring. Techniques such as fine-tuning, safety interventions, and defensive \\ndesign enable us to develop applications that produce helpful, harmless, and readable outputs. \\nWith care and preparation, generative AI holds immense potential benefit to industries from \\nmedicine to education.\\nWe’ve delved into deployment and the tools used for deployment. Particularly, we deployed \\napplications with FastAPI and Ray. In earlier chapters, we used Streamlit. There are many more \\ntools we could have explored, for example, the recently emerged LangServe, which is developed \\nwith LangChain applications in mind. While it’s still relatively fresh, it’s definitely worth watching \\nout for more developments in the future.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 318}, page_content='Generative AI in Production296\\nThe evaluation of LLMs is important to assess their performance and quality. LangChain sup -\\nports comparative evaluation between models, checking outputs against criteria, simple string \\nmatching, and semantic similarity metrics. These provide different insights into model quality, \\naccuracy, and appropriate generation. Systematic evaluation is key to ensuring LLMs produce \\nuseful, relevant, and sensible outputs.\\nMonitoring LLMs is a vital aspect of deploying and maintaining these complex systems. With the \\nincreasing adoption of LLMs in various applications, ensuring their performance, effectiveness, \\nand reliability is of utmost importance. We’ve discussed the significance of monitoring LLMs, \\nhighlighted key metrics to track for a comprehensive monitoring strategy, and have given exam-\\nples of how to track metrics in practice.\\nWe’ve looked at different tools for observability such as PromptWatch and LangSmith. LangSmith \\nprovides powerful capabilities to track, benchmark, and optimize LLMs built with LangChain. \\nIts automated evaluators, metrics, and visualizations help accelerate LLM development and \\nvalidation.\\nIn the next and final chapter, let’s discuss what the future of Generative AI will look like.\\nQuestions\\nPlease try and see if you can come up with the answers to these questions from memory. If you are \\nunsure about any of them, you might want to refer to the corresponding section in the chapter:\\n1. In your opinion, what is the best term for describing the operationalization of language \\nmodels, LLM apps, or apps that rely on generative models in general?\\n2. What is a token and why should you know about token usage when querying LLMs?\\n3. How can we evaluate LLM apps?\\n4. Which tools can help to evaluate LLM apps?\\n5. What are the considerations for the production deployment of agents?\\n6. Name a few tools used for deployment.\\n7. What are the important metrics for monitoring LLMs in production?\\n8. How can we monitor LLM applications?\\n9. What’s LangSmith?'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 319}, page_content='Chapter 9 297\\nJoin our community on Discord\\nJoin our community’s Discord space for discussions with the authors and other readers:\\nhttps://packt.link/lang'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 320}, page_content=''),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 321}, page_content='10\\nThe Future of Generative Models\\nIn this book, so far, we have discussed generative models for building applications, and we have \\nimplemented a few simple ones – for example, for semantic search, applications for content \\ncreation, customer service agents, and assistants for developers and data scientists. We have \\nexplored techniques such as tool use, agent strategies, semantic search with retrieval augmented \\ngeneration, and the conditioning of models with prompts and fine-tuning.\\nIn this chapter, we’ll deliberate on where this leaves us and where the future leads us. We’ll consid-\\ner weaknesses and socio-technical challenges of generative models, and strategies for mitigation \\nand improvement. We’ll focus on value creation opportunities, where unique customization of \\nfoundation models for specific use cases stands out. It remains uncertain which entities – big \\ntech firms, start-ups, or foundation model developers – will capture the most upsides. We’ll also \\nevaluate and address concerns such as the extinction threat through AI.\\nGiven the massive potential for increased productivity in various industries, venture funding for \\ngenerative AI start-ups skyrocketed in 2022 and 2023, and major players like Salesforce and Ac-\\ncenture among many others have made big commitments to generative AI with multibillion-dollar \\ninvestments. We’ll discuss potential effects on jobs in multiple industries, and disruptive changes \\nin creative industries, education, law, manufacturing, medicine, and the military.\\nWe will evaluate and address concerns such as misinformation, cybersecurity, privacy, and fair-\\nness, and think about how the changes and disruptions brought about by generative AI should \\ninfluence regulations and practical implementation.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 322}, page_content='The Future of Generative Models300\\nThe main sections of this chapter are:\\n• The current state of generative AI\\n• Economic consequences\\n• Societal implications\\nLet’s start with the current state of models and their capabilities.\\nThe current state of generative AI\\nAs discussed in this book, in recent years, generative AI models have attained new milestones in \\nproducing human-like content across modalities including text, images, audio, and video. Leading \\nmodels like OpenAI’s GPT-4 and DALL-E 2, and Anthropic’s Claude display impressive fluency \\nin content generation, be it textual or creative visual artistry.\\nBetween 2022 and 2023, models have progressed in strides. If generative models were previously \\ncapable of producing barely coherent text or grainy images, now we see high-quality 3D images, \\nvideos, and the generation of coherent and contextually relevant prose and dialogue, rivaling or \\neven surpassing the fluency levels of humans. These AI models leverage gargantuan datasets and \\ncomputational scale, enabling them to capture intricate linguistic patterns, display a nuanced \\nunderstanding of knowledge about the world, translate texts, summarize content, answer natural \\nlanguage questions, create appealing visual art, and acquire the capability to describe images. \\nSeemingly by magic, the AI-generated outputs mimic human ingenuity – painting original art, \\nwriting poetry, producing human-level prose, and even engaging in sophisticated aggregation \\nand synthesis of information from diverse sources.\\nBut let’s be a bit more nuanced! Generative models come with weaknesses as well as strengths. \\nDeficiencies persist compared to human cognition, including the frequent generation of plausible \\nyet incorrect or nonsensical statements. Hallucinations show a lack of grounding in reality, given \\nthat they are based on patterns in data rather than an understanding of the real world. Further, \\nmodels exhibit difficulties performing mathematical, logical, or causal reasoning. They are easily \\nconfused by complex inferential questions, which could limit their applicability in certain fields \\nof work. The black box problem of lack of explainability for outputs as well as for the models \\nthemselves hampers troubleshooting efforts, and controlling model behaviors within desired \\nparameters remains challenging. AI can have serious bias issues because of the prejudiced data \\nthey are trained on. This can lead to unfair results and make social inequalities worse.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 323}, page_content='Chapter 10 301\\nHere is a table summarizing the key strengths and deficiencies of current generative AI compared \\nto human cognition:\\nCategory Human Cognition Generative AI Models\\nLanguage Fluency Contextually relevant, draws \\nmeaning from world knowledge\\nHighly eloquent, reflects linguistic \\npatterns\\nKnowledge Conceptual understanding derived \\nfrom learning and experience\\nStatistical synthesis lacking \\ngrounding\\nCreativity Originality reflecting personality \\nand talent\\nImaginative but within training \\ndistribution\\nFactual Accuracy Usually aligns with truth and \\nphysical reality\\nHallucinations reflecting training \\ndata biases\\nReasoning Intuitive yet can apply heuristics \\nafter training\\nLogic is tightly limited to training \\ndistribution\\nBias Sometimes recognizes and can \\noverride inherent biases Propagates systemic biases in data\\nTransparency Partial, subjective insights from \\nthink-aloud techniques\\nPlausible reasoning from chain-of-\\nthought prompts\\nTable 10.1: Strengths and deficiencies of LLMs\\nWhile LLMs such as GPT-4 showcase language fluency on parity with humans, their lack of \\ngrounding, tendency for distortion, opaqueness, and potential for harm underscore deficiencies \\nthat temper the promise of generative AI. Progress in domains like logical reasoning and bias \\nmitigation remains at an early stage. As for transparency, while immense complexity poses an \\nimmense challenge, determined efforts seek to surface the lineage and mechanisms of reasoning \\nfor both humans (advances in the understanding of neurocognition) and AI (interpretability and \\nexplainability). Addressing problematic areas is key to developing reliable and trustworthy sys-\\ntems. Throughout the book, we’ve discussed and implemented potential solutions that address \\nthe weaknesses of generative AI.\\nWe should keep in mind, however, that this gap analysis of human versus AI is for highlighting \\nareas of improvement – as we have seen in domains such as Atari games, chess, and Go, AIs can \\nreach superhuman levels if trained properly, and we haven’t touched the ceiling yet in many ar-\\neas. Let’s look more broadly at some of the socio-technical challenges involved in unlocking the \\ncapabilities of generative AI systems and discuss approaches to overcoming them!'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 324}, page_content='The Future of Generative Models302\\nChallenges\\nThe profound potential of generative AI systems indicates an exciting future if development con-\\ntinues at pace. This table shows a summary of a few of the technical and organizational challenges \\ntogether with approaches to tackle them:\\nChallenge Potential Solutions\\nKnowledge Freshness (+ Concept Drift)\\nContinuous learning methods like elastic weight \\nconsolidation, stream ingestion pipelines, and \\nefficient retraining procedures\\nSpecialized Knowledge\\nTask-specific demonstrations and prompting, \\nknowledge retrieval and grounding, and context \\nexpansion\\nDownstream Adaptability Strategic fine-tuning methods, catastrophic \\nforgetting mitigation, and optimized hardware access\\nBiased Outputs\\nBias mitigation algorithms, balanced training data, \\naudits, inclusivity training, and interdisciplinary \\nresearch\\nHarmful Content Generation Moderation systems, interruption and correction, and \\nconditioning methods such as RLHF\\nLogical Inconsistencies Hybrid architectures, knowledge bases, and retrieval \\naugmentation\\nFactual Inaccuracies Retrieval augmentation, knowledge bases, and \\nconsistent knowledge base updating\\nLack of Explainability Model introspection, concept attribution, and \\ninterpretable model designs\\nPrivacy Risks Differential privacy, federated learning, encryption, \\nand anonymization\\nHigh Latency and Compute Costs Model distillation, optimized hardware, and efficient \\nmodel design\\nLicensing Limitations Open/synthetic data, custom data, and fair licensing \\nagreements\\nSecurity/Vulnerabilities Adversarial robustness and cybersecurity best \\npractices'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 325}, page_content='Chapter 10 303\\nGovernance Compliance frameworks and ethical development \\ngovernance\\nTable 10.2: Challenges of generative AI and potential solutions\\nChallenges of generative AI go beyond just improving content generation—they encompass en-\\nvironmental sustainability, algorithmic equity, and individual privacy. Strategies like employing \\nsimplified model architectures, using knowledge distillation, and developing specialized hardware \\nare critical to reducing the carbon footprint of AI in the face of rapid progress. To ensure fair AI, \\nsteps such as incorporating balanced datasets, applying bias mitigation algorithms, enforcing \\nfairness through constrained optimization, and promoting inclusivity are essential, despite their \\ncomplexity.\\nTo counteract potential harm from AI output, such as toxicity or false information (hallucination), \\ntechniques like reinforcement learning guided by human feedback and grounding responses in \\nverified knowledge can be employed. Additionally, securing sensitive data through privacy-pre-\\nserving methods like differential privacy, federated learning, and real-time content correction is \\nfundamental for upholding user dignity.\\nFinally, staying up to date with the evolving informational landscapes, comprehending specialized \\ndomains, and flexibly adapting to emerging needs represent newly visible obstacles as generative \\nmodels permeate real-world contexts.\\nAddressing these challenges involves a broad spectrum of responses that must consider the entire \\nlife cycle of AI development. Such responses include innovative training objectives focused on \\nconsistency, structural knowledge integrations, and design of models for better controllability, \\nas well as software and hardware optimization for infrastructure efficiency.\\nOne of the most effective developments is flexible user control. With concerted effort in research \\nand development, the aim is to steer generative AI toward alignment with societal values. For \\nreasons of computational efficiency and costs, this implies a shift from pretraining to specialized \\ndownstream conditioning (particularly, fine-tuning and prompt techniques). This, in turn, will \\nlead to a proliferation of start-ups applying core AI technologies.\\nTechnological innovation together with regulation and transparency of AI development will \\nensure that generative AI enhances human capability without compromising ethical standards. \\nLooking ahead, generative AI systems are poised to become more powerful and multifaceted.\\nLet’s have a look at some emerging trends in model development!'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 326}, page_content='The Future of Generative Models304\\nTrends in model development\\nThe current doubling time in training compute of very large models is about 8 months, outstrip-\\nping scaling laws such as Moore’s Law (transistor density at cost increases at a rate of currently \\nabout 18 months) and Rock’s Law (costs of hardware like GPUs and TPUs halve every 4 years). \\nThis graph illustrates this trend in training compute of large models (source: Epoch, Parameter, \\nCompute, and Data Trends in Machine Learning. Retrieved from https://epochai.org/mlinputs/\\nvisualization):\\nFigure 10.1: Training FLOPs of notable AI systems\\nThe main point from this graph is the increase in compute, which is apparent since the 1960s, \\nand the Cambrian explosion of models of the deep learning era at the top right. As discussed \\nin Chapter 1, What Is Generative AI?, parameter sizes for large systems have been increasing at a \\nsimilar rate as the training compute, which means we could see much larger and more expensive \\nsystems if this growth continues.\\nEmpirically derived scaling laws predict the performance of LLMs based on the given training \\nbudget, dataset size, and the number of parameters. This could mean that highly powerful sys-\\ntems will be concentrated in the hands of Big Tech.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 327}, page_content='Chapter 10 305\\nHowever, future progress may depend more on data efficiency and model quality than sheer \\nsize. Though massive models grab headlines, computing power and energy constraints put a \\nlimit on unrestrained model growth. It’s also unclear if performance will keep up further with \\nthe growth in parameters. The future could see the co-existence of massive, general models with \\nsmaller and more accessible specialized niche models that provide faster and cheaper training, \\nmaintenance, and inference.\\nIt has already been shown that smaller specialized models can prove highly performant. As men-\\ntioned in Chapter 6, Developing Software with Generative AI, we’ve recently seen models such as \\nphi-1 (Textbooks Are All You Need, 2023, Gunasekar and colleagues), with about 1 billion parameters, \\nthat – despite its smaller scale – achieve high accuracy on evaluation benchmarks. The authors \\nsuggest that improving data quality can dramatically change the shape of scaling laws.\\nFurther, there is a body of work on simplified model architectures, which have substantially fewer \\nparameters and only modestly drop accuracy (for example, One Wide Feedforward is All You Need, \\nPessoa Pires and others, 2023). Additionally, techniques such as fine-tuning, distillation, and \\nprompting techniques can enable smaller models to leverage the capabilities of large foundations \\nwithout replicating their costs. To compensate for model limitations, tools like search engines \\nand calculators have been incorporated into agents, and multi-step reasoning strategies, plugins, \\nand extensions may be increasingly used to expand capabilities.\\nThe KM scaling law, proposed by Kaplan and colleagues, derived through empirical \\nanalysis and fitting of model performance with varied data sizes, model sizes, and \\ntraining compute, presents power-law relationships, indicating a strong codepen -\\ndence between model performance and factors such as model size, dataset size, and \\ntraining compute.\\nThe Chinchilla scaling law, developed by the Google DeepMind team, involved ex-\\nperiments with a wider range of model sizes and data sizes and suggests an optimal \\nallocation of compute budget to model size and data size, which can be determined \\nby optimizing a specific loss function under a constraint.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 328}, page_content='The Future of Generative Models306\\nThe rapidly decreasing costs of AI model training represent a significant shift in the landscape, \\nenabling broader participation in cutting-edge AI research and development. As noted, several \\nfactors are contributing to this trend, including optimization of training regimes, improvements \\nin data quality, and the introduction of novel model architectures. Here is a brief summary of \\ntechniques and approaches for making generative AI more accessible and effective:\\n• Simplified model architectures: Streamlining model design for easier management, better \\ninterpretability, and lower computational cost.\\n• Synthetic data generation: Creating artificial training data to augment datasets while \\npreserving privacy.\\n• Model distillation: Transferring knowledge from a large model into a smaller, more ef-\\nficient one for easy deployment.\\n• Optimized inference engines: Software frameworks that increase the speed and efficiency \\nof executing AI models on a given hardware.\\n• Dedicated AI hardware accelerators: Specialized hardware like GPUs and TPUs that \\ndramatically accelerate AI computations.\\n• Open-source and synthetic data: High-quality public datasets enable collaboration and \\nsynthetic data enhances privacy and can help reduce bias.\\n• Quantization: Converting models to lower precision by reducing bit sizes of weights and \\nactivations, decreasing model size and compute costs.\\n• Incorporating knowledge bases: Grounding model outputs in factual databases reduces \\nhallucinations and improves accuracy.\\n• Retrieval augmented generation: Enhancing text generation by retrieving relevant in -\\nformation from sources.\\n• Federated learning: Training models on decentralized data to improve privacy while \\nbenefiting from diverse sources.\\nAmong the technical advancements helping drive down these costs, quantization techniques have \\nemerged as an essential contributor. Open-source datasets and techniques such as synthetic data \\ngeneration further democratize access to AI training by providing high-quality and data-efficient \\nmodel development and removing some reliance on vast, proprietary datasets. Open-source ini-\\ntiatives contribute to the trend by providing cost-effective, collaborative platforms for innovation.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 329}, page_content='Chapter 10 307\\nThese innovations collectively lower barriers that have so far impeded real-world generative AI \\nadoption across various segments:\\n• Financial barriers are reduced by compressing large model performance into far smaller \\nform factors through quantization and distillation.\\n• Privacy risks are mitigated via federated and synthetic techniques circumventing exposure.\\n• The accuracy limitations hampering small models are relieved through grounding gen -\\neration with external information.\\n• Specialized hardware exponentially accelerates throughput while optimized software \\nmaximizes the existing infrastructure.\\n• Democratizing access by tackling constraints like cost, security, and reliability unlocks \\nbenefits for vastly expanded audiences, steering generative creativity from a narrow con-\\ncentration toward empowering diverse human talents.\\nThe landscape is shifting from a focus on sheer model size and brute-force compute to clever, nu-\\nanced approaches that maximize computational efficiency and model efficacy. With quantization \\nand related techniques lowering barriers, we’re poised for a more diverse and dynamic era of AI \\ndevelopment where resource wealth is not the only determinant of leadership in AI innovation.\\nThis could mean a democratization of the market, as we’ll see now.\\nBig Tech vs. small enterprises\\nAs for the spread of technology, two primary scenarios exist. In the centralized scenario, generative \\nAI and LLMs are primarily developed and controlled by large tech firms that invest heavily in the \\nnecessary computational hardware, data storage, and specialized AI/ML talent. Entities like these \\nbenefit from economies of scale and resources that allow them to bear the high costs of training \\nand maintaining these sophisticated systems. They produce general models that are often made \\naccessible to others through cloud services or APIs, but these one-size-fits-all solutions may not \\nperfectly align with the requirements of every user or organization.\\nConversely, in the self-service scenario, companies or individuals take on the task of training their \\nown AI models. This approach allows for models that are customized to the specific needs and pro-\\nprietary data of the user, providing more targeted and relevant functionality. However, this route \\ntraditionally requires significant AI expertise, substantial computational resources, and rigorous \\ndata privacy safeguards, which can be prohibitively expensive and complex for smaller entities.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 330}, page_content='The Future of Generative Models308\\nThe central question is how these scenarios will coexist and evolve. Presently, the centralized \\napproach dominates due to the barriers in cost and expertise required for the self-service model. \\nYet with the democratization of AI – driven by declining computational costs, more widespread \\nAI training and tools, and innovations that simplify model training – the self-service scenario \\nmay become increasingly viable for smaller organizations, local governments, and community \\ngroups. These groups could potentially harness tailored AI solutions for highly specific tasks, \\ngaining advantages in agility and privacy preservation.\\nAs these two business models continue to develop, a hybrid landscape may emerge where both \\napproaches fulfill distinct roles based on use cases, resources, expertise, and privacy considerations. \\nLarge firms might continue to excel in providing industry-specific models, while smaller entities \\ncould increasingly train or fine-tune their own models to meet niche demands. The evolution of \\nthis landscape will largely depend on the pace of advancements that make AI more accessible, \\nmore cost-effective, and simpler to use without compromising robustness or privacy.\\nIf robust tools emerge to simplify and automate AI development, custom generative models may \\neven be viable for local governments, community groups, and individuals to address hyper-local \\nchallenges. While centralized Big Tech firms benefit currently from economies of scale, distributed \\ninnovation from smaller entities could unlock generative AI’s full potential across all sectors of \\nsociety.\\nWhile large tech firms currently dominate generative AI research and development, smaller entities \\nmay ultimately stand to gain the most from these technologies. As costs decline for computing, \\ndata storage, and AI talent, custom pre-training of specialized models could become feasible for \\nsmall and mid-sized companies.\\nIn a timeframe of 3–5 years, constraints around computing and talent availability could ease \\nconsiderably, eroding the centralized moat created by massive investments. Specifically, if cloud \\ncomputing costs decline as projected, and AI skills become more widespread through education \\nand automated tools, self-training customized LLMs may become feasible for many companies.\\nRather than relying on generic models from Big Tech, tailored generative AI fine-tuned on niche \\ndatasets could better serve unique needs. Start-ups and non-profits often excel at rapidly iterat-\\ning to build cutting-edge solutions for specialized domains. Democratized access through cost \\nreductions could enable such focused players to train performant models exceeding the capa -\\nbilities of generalized systems.\\nIn the next section, we’ll discuss the  potential of Artificial General Intelligence (AGI ) and the \\nthreat of extinction by the malicious actions of a superintelligent artificial entity.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 331}, page_content='Chapter 10 309\\nArtificial General Intelligence\\nNot all abilities in LLMs scale predictably with model size. Capabilities such as in-context learn-\\ning may remain exclusive to particularly large models due to factors beyond raw computational \\ngrowth. There’s speculation that sustained scaling – training vast models on even larger data-\\nsets – might lead to broader skill sets and, some suggest, toward the development of AGI with \\nreasoning abilities on par or beyond humans.\\nNevertheless, current neuroscientific perspectives and the limitations of existing AI structures \\nprovide compelling arguments against an imminent leap to AGI (inspired by the discussion in \\nthe article The feasibility of artificial consciousness through the lens of neuroscience  by Jaan Aru and \\nothers; 2023):\\n• Lack of embodied, embedded information: The current generation of LLMs lacks multi-\\nmodal and embodied experiences, being trained predominantly on textual data. In contrast, \\nhuman common sense and understanding of the physical world are developed through \\nrich, diverse interactions involving multiple senses.\\n• Different architecture from biological brains: The relatively simple stacked transform-\\ner architecture used in models like GPT-4 lacks the complex recurrent and hierarchical \\nstructures of the thalamocortical system thought to enable consciousness and general \\nreasoning in humans.\\n• Narrow capabilities: Existing models remain specialized for particular domains like text \\nand fall short in flexibility, causal reasoning, planning, social skills, and general prob -\\nlem-solving intelligence. This could change either with increasing tool use or with fun-\\ndamental changes to the models.\\n• Minimal social abilities or intent: Current AI systems have no innate motivations, social \\nintelligence, or intent beyond their training objectives. Fears of malicious goals or desire \\nfor domination seem unfounded.\\n• Limited real-world knowledge: Despite ingesting huge datasets, the factual knowledge \\nand common sense of large models remain very restricted compared to humans. This \\nimpedes applicability in the physical world.\\n• Data-driven limitations: Reliance on pattern recognition from training data rather than \\nstructured knowledge makes reliable generalization to novel situations difficult.\\nAs we address pressing AI challenges, the discourse around AI’s threat and its potential for societal \\ndisruption should not overshadow immediate issues like fairness and privacy. '),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 332}, page_content='The Future of Generative Models310\\nGiven current model limitations and the lack of agency, the notion of today’s AI rapidly evolving \\ninto a dangerous superintelligence appears highly unlikely. In formulating regulations, we must \\nbe vigilant against regulatory capture, where dominant industry players invoke far-fetched sce-\\nnarios of AI-driven destruction to distract from pressing concerns and to shape rules to fit their \\ninterests, potentially marginalizing the concerns of smaller entities and the public. Nonetheless, \\nongoing attention to safety research and ethical concerns is essential, especially as AI advances.\\nLet’s discuss the broader economy, and – the elephant in the room – jobs!\\nEconomic consequences\\nIntegrating generative AI promises immense productivity gains through automating tasks across \\nsectors – albeit risking workforce disruptions given the pace of change. Assuming computing \\nscales sustainably, projections estimate 30–50% of current work activities will be automatible \\nby 2030, adding $6–8 trillion annually to global GDP. Sectors like customer service, marketing, \\nsoftware engineering, and R&D may see over 75% of use case value. However, past innovations \\nultimately spawned new occupations, suggesting long-term realignment.\\nDeveloped regions are likely to witness faster uptake, displacing administrative, creative, and \\nanalytical roles initially. Yet automation extends beyond employment loss – at present, under \\n20% of US worker tasks seem automatable directly through LLMs. But LLM-enhanced software \\ncould transform 50% of tasks, affirming the force multiplication from complementary innovations.\\nThus automation’s labor impact remains complex – while augmenting productivity, transitional \\npains persist. Still, the virtuous cycle between AI progress and emerging specializations signals \\nhopes for an uplift over redundancy. And braiding priorities of sustainability, equity, and human \\ndignity throughout this transformation promises optimizing empowerment over exploitation.\\nIn a professional context, generative AI is poised to amplify human creativity and transform \\ntraditional workflows across a range of industries. For content creators, such as marketers and \\njournalists, AI can rapidly generate initial drafts, fostering a baseline that human creativity can \\nbuild upon for more customized outputs. Software developers benefit from AI’s ability to produce \\ncode snippets, helping to expedite the development process. For scholars and scientists, the ability \\nof AI to distill complex research into comprehensive summaries can catalyze scholarly progress \\nand innovation.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 333}, page_content='Chapter 10 311\\nHere are some key predictions about how jobs may be impacted by advances in language models \\nand generative AI:\\n• Routine legal work like draft preparation will be increasingly automated, changing job \\nroles for junior lawyers and paralegals.\\n• Software engineering will see a rise in AI coding assistants handling mundane tasks, en-\\nabling developers to focus on complex problem-solving.\\n• Data scientists will spend more time refining AI systems rather than building predictive \\nmodels from scratch.\\n• Demand for specialized roles like prompt engineering will continue to rise.\\n• Teachers will utilize AI for course preparation and personalized student support.\\n• Journalists, paralegals, and graphic designers will employ generative AI to enhance content \\ncreation, raising concerns about job impacts.\\n• Demand will grow for experts in AI ethics, regulations, and security to oversee responsible \\ndevelopment.\\n• Musicians and artists will collaborate with AI, boosting creative expression and acces -\\nsibility.\\n• Striking an optimal balance between AI capabilities and human judgment will be vital \\nacross sectors.\\n• The common thread is that while routine tasks face increasing automation, human ex -\\npertise to steer AI directions and ensure responsible outcomes will remain indispensable.\\nWhile certain jobs may be displaced by AI in the near term, especially routine cognitive tasks, it \\nmay automate certain activities rather than eliminate entire occupations. Technical experts like \\ndata scientists and programmers will remain key to developing AI tools and realizing their full \\nbusiness potential. By automating rote tasks, models may free up human time for higher-value \\nwork, boosting economic output.\\nConcerns have emerged about saturation as generative AI tools are relatively easy to build using \\nfoundation models. Customization of models and tools will allow value creation, but it’s unclear \\nwho will capture the most upsides and how powerful these applications can be. While current \\nmarket hype is high, investors are tempering decisions given lower valuations and skepticism \\nfollowing the 2021 AI boom/bust cycle. The long-term market impact and the winning generative \\nAI business models have yet to unfold.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 334}, page_content='The Future of Generative Models312\\nAs AI models become more sophisticated and economical to operate, we can anticipate a sub -\\nstantial proliferation of generative AI and LLM applications into novel domains. Beyond just the \\nplummeting hardware expenses that have historically followed Moore’s Law, there are additional \\neconomies of scale affecting AI systems.\\nIn Chapter 1, What Is Generative AI?, we discussed the pertinent trend in the AI industry that en -\\ncompasses gains in efficiency stemming from the iterative refinement of code, the development \\nof sophisticated tools, and the enhancement of techniques. The improved efficiency because of \\nnew techniques and approaches, combined with the declining hardware costs, fosters a virtu -\\nous cycle: as costs diminish, AI adoption widens, in turn spurring further cost reductions and \\nefficiency improvements. What emerges is a feedback loop where each iteration of efficiency \\ncatalyzes increased usage, which in itself leads to even greater efficiency – a dynamic poised to \\ndramatically advance the frontier of AI capabilities.\\nThe 2021 AI boom/bust cycle refers to a rapid acceleration in investment and growth \\nin the AI start-up space followed by a market cooldown and stabilization in 2022 as \\nprojections failed to materialize and valuations declined.\\nHere’s a quick summary:\\n• Boom phase (2020-2021): There was huge interest and skyrocketing invest-\\nment in AI start-ups offering innovative capabilities like computer vision, \\nnatural language processing, robotics, and machine learning platforms. To-\\ntal funding for AI start-ups hit record levels in 2021, with over $73 billion \\ninvested globally according to Pitchbook. Hundreds of AI start-ups were \\nfounded and funded during this period.\\n• Bust phase (2022): In 2022, the market underwent a correction, with val -\\nuations of AI start-ups falling significantly from their 2021 highs. Several \\nhigh-profile AI start-ups like Anthropic and Cohere faced valuation mark -\\ndowns. Many investors became more cautious and selective with funding \\nAI start-ups. Market corrections in the broader tech sector also contributed \\nto the bust.\\n• Key factors: Excessive hype, unrealistic growth projections, historically high \\nvaluations in 2021, and broader economic conditions all contributed to the \\nboom-bust cycle. The cycle followed a classic pattern seen previously in \\nsectors like dot-com and blockchain.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 335}, page_content='Chapter 10 313\\nLet’s look at various sectors where generative models will have profound near-term impacts, \\nstarting with creative endeavors.\\nCreative industries and advertising\\nThe gaming and entertainment industries are leveraging generative AI to craft uniquely immersive \\nuser experiences. Major efficiency gains from automating creative tasks could increase leisure time \\nspent online. Generative AI can enable machines to generate new and original content, such as art, \\nmusic, and literature, by learning from patterns and examples. This has implications for creative \\nindustries, as it can enhance the creative process and potentially create new revenue streams. It \\nalso unlocks new scales of personalized, dynamic content creation for media, film, and advertising.\\nFor media, film, and advertising, AI unlocks new scales of personalized, dynamic content creation. \\nIn journalism, automated article generation using massive datasets can free up reporters to focus \\non more complex investigative stories. AI-Generated Content (AIGC) is playing a growing role in \\ntransforming media production and delivery by enhancing efficiency and diversity. In journalism, \\ntext generation tools automate writing tasks traditionally done by human reporters, significant-\\nly boosting productivity while maintaining timeliness. Media outlets like the Associated Press \\ngenerate thousands of stories per year using AIGC. Robot reporters like the Los Angeles Times \\nQuakebot can swiftly produce articles on breaking news.\\nOther applications include Bloomberg News’ Bulletin service where chatbots create personalized \\none-sentence news summaries. AIGC also enables AI news anchors that co-present broadcasts \\nwith real anchors by mimicking human appearance and speech from text input. Chinese news \\nagency Xinhua’s virtual presenter Xin Xiaowei is an example, presenting broadcasts from different \\nangles for an immersive effect.\\nAIGC is transforming movie creation from screenwriting to post-production. AI screenwriting \\ntools analyze data to generate optimized scripts. Visual effects teams blend AI-enhanced digital \\nenvironments and de-aging with live footage for immersive visuals. Deep fake technology rec-\\nreates or revives characters convincingly.\\nAI also powers automated subtitle generation, even predicting dialogue in silent films by training \\nmodels on extensive audio samples. This expands accessibility via subtitles and recreates voice-\\novers synchronized to scenes. In post-production, AI color grading and editing tools like Colourlab \\nAI and Descript simplify processes like color correction using algorithms.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 336}, page_content='The Future of Generative Models314\\nIn advertising, AIGC unlocks new potential for efficient, customized advertising creativity and \\npersonalization. AI-generated content allows advertisers to create personalized, engaging ads \\ntailored to individual consumers at scale. Platforms like Creative Advertising System (CAS) and \\nSmart Generation System Personalized Advertising Copy (SGS-PAC) leverage data to automat-\\nically generate ads with messaging targeted to specific user needs and interests.\\nAI also assists in advertising creativity and design – tools like Vinci produce customized attrac-\\ntive posters from product images and slogans, while companies like Brandmark.io generate logo \\nvariations based on user preferences. GAN technologies automate product listing generation \\nwith keywords for effective peer-to-peer marketing. Synthetic ad production is also on the rise, \\nenabling highly personalized, scalable campaigns that save time.\\nIn music, tools like Google’s Magenta, IBM’s Watson Beat, and Sony CSL’s Flow Machine can \\ngenerate original melodies and compositions. AIVA similarly creates unique compositions from \\nparameters tuned by users. LANDR’s AI mastering uses machine learning to process and improve \\ndigital audio quality for musicians.\\nIn visual arts, MidJourney uses neural networks to generate inspirational images that can kick -\\nstart painting projects. Artists have used its outputs to create prize-winning works. DeepDream’s \\nalgorithm imposes patterns on images, creating psychedelic art. GANs can generate abstract \\npaintings converging on a desired style. AI painting conservation analyzes artwork to digitally \\nrepair damage and restore pieces.\\nAnimation tools like Adobe’s Character Animator and Anthropic’s Claude can help with the gen-\\neration of customized characters, scenes, and motion sequences, opening animation potential \\nfor non-professionals. ControlNet adds constraints to steer diffusion models, increasing output \\nvariability.\\nFor all these applications, advanced AI expands creative possibilities through both generative \\ncontent and data-driven insights. In all cases, quality control and properly attributing the contri-\\nbutions of human artists, developers, and training data remains an ongoing challenge as adoption \\nspreads.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 337}, page_content='Chapter 10 315\\nEducation\\nOne potential near-future scenario is that the rise of personalized AI tutors and mentors could \\ndemocratize access to education for high-demand skills aligned with an AI-driven economy. In \\nthe education sector, generative AI is already transforming how we teach and learn. Tools like \\nChatGPT can be used to automatically generate personalized lessons and customized content for \\nindividual students. This reduces instructor workloads substantially by automating repetitive \\nteaching tasks. AI tutors provide real-time feedback on student writing assignments, freeing up \\nteachers to focus on more complex skills. Virtual simulations powered by generative AI can also \\ncreate engaging, tailored learning experiences adapted to different learners’ needs and interests.\\nHowever, risks around perpetuating biases and spreading misinformation need to be studied \\nfurther as these technologies evolve. The accelerating pace of knowledge and the obsolescence \\nof scientific findings mean that training children’s curiosity-driven learning should focus on \\ndeveloping the cognitive mechanisms involved in initiating and sustaining curiosity, such as \\nawareness of knowledge gaps and the use of appropriate strategies to resolve them.\\nWhile AI tutors tailored to each student could enhance outcomes and engagement, poorer schools \\nmay be left behind, worsening inequality. Governments should promote equal access to prevent \\ngenerative AI from becoming a privilege of the affluent. Democratizing opportunity for all stu-\\ndents remains vital.\\nIf implemented thoughtfully, personalized AI-powered education could make crucial skills ac-\\nquisition accessible to anyone motivated to learn. Interactive AI assistants that adapt courses to \\nstudents’ strengths, needs, and interests could make learning efficient, engaging, and equitable. \\nHowever, challenges around access, biases, and socialization need addressing.\\nLaw\\nGenerative models like LLMs can automate routine legal tasks such as contract review, documen-\\ntation generation, and brief preparation. They also enable faster, comprehensive legal research \\nand analysis. Additional applications include explaining complex legal concepts in plain language \\nand predicting litigation outcomes using case data. However, responsible and ethical use remains \\ncritical given considerations around transparency, fairness, and accountability. Overall, properly \\nimplemented AI tools promise to boost legal productivity and access to justice while requiring \\nongoing scrutiny regarding reliability and ethics.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 338}, page_content='The Future of Generative Models316\\nManufacturing\\nIn the automotive sector, generative models are employed to generate 3D environments for simu-\\nlations and aid in the development of cars. Additionally, generative AI is utilized for road-testing \\nautonomous vehicles using synthetic data. These models can also process object information \\nto comprehend the surrounding environment, understand human intent through dialogues, \\ngenerate natural language responses to human input, and create manipulation plans to assist \\nhumans in various tasks.\\nMedicine\\nA model that can accurately predict physical properties from gene sequences would represent a \\nmajor breakthrough in medicine and could have profound impacts on society. It could further  \\naccelerate drug discovery and precision medicine, enable earlier disease prediction and prevention, \\nprovide a deeper understanding of complex diseases, and improve gene therapies. However, it also \\nraises major ethical concerns around genetic engineering and could exacerbate social inequalities.\\nNew techniques with neural networks are already employed to lower long-read DNA sequencing \\nerror rates (Baid and colleagues; DeepConsensus improves the accuracy of sequences with a gap-aware \\nsequence transformer, September 2022), and, according to a report by ARK Investment Manage -\\nment (2023), in the short term, technology like this can make it already possible to deliver the \\nfirst high-quality, whole long-read genome for less than $1,000. This means that large-scale \\ngene-to-expression models might not be far away either.\\nMilitary\\nMilitaries worldwide are investing in research to develop Lethal Autonomous Weapons Systems \\n(LAWS). Robots and drones can identify targets and deploy lethal force without any human su-\\npervision. Machines can process information and react faster than humans, removing emotion \\nfrom lethal decisions. However, this raises significant moral questions. Allowing machines to \\ndetermine whether lives should be taken crosses a troubling threshold. Even with sophisticated \\nAI, complex factors in war like proportionality and distinction between civilians and combatants \\nrequire human judgment.\\nIf deployed, completely autonomous lethal weapons would represent an alarming step toward \\nrelinquishing control over life-and-death decisions. They could violate international humanitarian \\nlaw or be used by despotic regimes to terrorize populations. Once unleashed fully independently, \\nthe actions of autonomous killer robots would be impossible to predict or restrain.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 339}, page_content='Chapter 10 317\\nThe advent of highly capable generative AI will likely transform many aspects of society in the \\ncoming years beyond the economics and the disruption of certain jobs. Let’s think a bit more \\nbroadly about the societal impact!\\nSocietal implications\\nAs generative models continue to develop and add value to businesses and creative projects, \\ngenerative AI will shape the future of technology and human interaction across domains. While \\ntheir widespread adoption brings forth numerous benefits and opportunities for businesses and \\nindividuals, it is crucial to address the ethical and societal concerns that arise from increasing \\nreliance on AI models in various fields.\\nGenerative AI offers immense potential benefits across personal, societal, and industrial realms if \\ndeployed thoughtfully. At a personal level, these models can enhance creativity and productivity, \\nand increase accessibility to services like healthcare, education, and finance. By democratizing \\naccess to knowledge resources, they can help students learn or aid professionals in making deci-\\nsions by synthesizing expertise. As virtual assistants, they provide instant, customized information \\nto facilitate routine tasks.\\nFrom a consumer standpoint, generative AI has the potential to deliver unprecedented person-\\nalization. Recommendation systems can fine-tune their outputs to individual preferences. Mar-\\nketing efforts can be adapted to specific customer segments and local tastes while maintaining \\nconsistency and scale.\\nThe rise of generative AI represents a significant milestone within a broader societal trend of how \\ncreative content is being generated and consumed. The internet has already nurtured a culture \\nof remixing, where derivative works and co-creation are the norms. Generative AI fits naturally \\nwithin this paradigm by creating new content through the recombination of existing digital \\nmaterials, promoting the ethos of shared, iterative creation.\\nHowever, the capacity of generative AI to synthesize and remix copyrighted materials at scale \\npresents intricate legal and ethical challenges. The training of these models on extensive corpora \\nthat encompass literature, articles, images, and other copyrighted works creates a tangled web \\nfor attribution and compensation. Existing tools struggle to identify content generated by AI, \\nwhich complicates efforts to apply traditional copyright and authorship principles. This dilemma \\nunderscores the urgent need for legal frameworks that can keep pace with technological advances \\nand navigate the complex interplay between rights-holders and AI-generated content.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 340}, page_content='The Future of Generative Models318\\nOne of the major problems that I can see is misinformation, either in the interest of political \\ninterest groups, foreign actors, or large corporations. Let’s discuss this threat!\\nMisinformation and cybersecurity\\nAI presents a dual-edged sword against disinformation. While it enables scalable detection, auto-\\nmation makes it easier to spread sophisticated, personalized propaganda. AI could help or harm \\nsecurity depending on whether it is used responsibly. It increases vulnerabilities to misinformation \\nalong with cyberattacks using generative hacking and social engineering.\\nThere are significant threats associated with AI techniques like micro-targeting and deepfakes. \\nPowerful AI can profile users psychologically to deliver personalized disinformation that facili -\\ntates concealed manipulation, escaping broad examination. Big Data and AI could be leveraged to \\nexploit psychological vulnerabilities and infiltrate online forums to attack and spread conspiracy \\ntheories.\\nDisinformation has transformed into a multifaceted phenomenon, involving biased information, \\nmanipulation, propaganda, and intent to influence political behavior. For example, during the \\nCOVID-19 pandemic, the spread of misinformation and infodemics has been a major challenge. \\nAI can influence public opinion and sway elections.\\nIt can also generate fake audio/video content to damage reputations and sow confusion. State \\nand non-state actors are weaponizing these capabilities for propaganda to damage reputations \\nand sow confusion. AI can be used by political parties, governments, criminal groups, and even \\nthe legal system to launch lawsuits and/or extract money.\\nThis likely will have far-reaching consequences in various domains. A significant portion of in-\\nternet users may be obtaining the information they need without accessing external websites. \\nThere is a danger of large corporations being the gatekeepers of information and controlling \\npublic opinion, effectively being able to restrict certain actions or viewpoints.\\nCareful governance and digital literacy are essential to build resilience. Though no single fix exists, \\ncollective efforts promoting responsible AI development can help democratic societies address \\nemerging threats.\\nLet’s talk more about regulations!'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 341}, page_content='Chapter 10 319\\nRegulations and implementation challenges\\nRealizing the potential of generative AI in a responsible manner involves addressing a number of \\npractical legal, ethical, and regulatory issues:\\n• Legal: Copyright laws remain ambiguous regarding AI-generated content. Who owns the \\noutput – the model creator, training data contributors, or end users? Replicating copy -\\nrighted data in training also raises fair use debates that need clarification.\\n• Data protection: Collecting, processing, and storing the massive datasets required to train \\nadvanced models creates data privacy and security risks. Governance models ensuring \\nconsent, anonymity, and safe access are vital.\\n• Oversight and regulations: Calls are mounting for oversight to ensure non-discrimina -\\ntion, accuracy, and accountability from advanced AI systems. However, flexible policies \\nbalancing innovation and risk are needed rather than burdensome bureaucracy.\\n• Ethics: Frameworks guiding development toward beneficial outcomes are indispensable. \\nIntegrating ethics through design practices focused on transparency, explicability, and \\nhuman oversight helps build trust.\\nOverall, proactive collaboration between policymakers, researchers, and civil society is essential \\nto settle unresolved issues around rights, ethics, and governance. With pragmatic guardrails in \\nplace, generative models can fulfill their promise while mitigating harm.\\nThere is a growing demand for algorithmic transparency. This means that tech companies and \\ndevelopers should reveal the source code and inner workings of their systems. However, there is \\nresistance from these companies and developers, who argue that disclosing proprietary informa-\\ntion would harm their competitive advantage. Open-source models will continue to thrive, and \\nlocal legislation in the EU and other countries will push for transparent use of AI.\\nThe consequence of AI bias includes potential harm to individuals or groups due to biased de -\\ncisions made by AI systems. Incorporating ethics training into computer science curricula can \\nhelp reduce biases in AI code. By teaching developers how to build applications that are ethical \\nby design, the probability of biases being embedded into the code can be minimized. To stay on \\nthe right path, organizations need to prioritize transparency, accountability, and guardrails to \\nprevent bias in their AI systems. AI bias prevention is a long-term priority for many organizations; \\nhowever, without legislation driving it, it can take time to be introduced. Local legislation in EU \\ncountries, for example, such as the European Commission’s proposal for harmonized rules on AI \\nregulation, will drive more ethical use of language and imagery.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 342}, page_content='The Future of Generative Models320\\nA current German law on fake news, which imposes a 24-hour timeframe for platforms to remove \\nfake news and hate speech, is impractical for both large and small platforms. Additionally, the \\nlimited resources of smaller platforms make it unrealistic for them to police all content. Further, \\nonline platforms should not have the sole authority to determine what is considered truth, as this \\ncould lead to excessive censorship. More nuanced policies are needed that balance free speech, \\naccountability, and feasibility for a diversity of technology platforms to comply. Relying solely on \\nprivate companies to regulate online content raises concerns about a lack of oversight and due \\nprocess. Broader collaboration between government, civil society, academics, and industry can \\ndevelop more effective frameworks to counter misinformation while protecting rights.\\nTo maximize benefits, companies need to ensure human oversight, diversity, and transparency \\nin development. Policymakers may need to implement guardrails preventing misuse while pro-\\nviding workers with support to transition as activities shift. With responsible implementation, \\ngenerative AI could propel growth, creativity, and accessibility in a more prosperous society. \\nAddressing potential risks early on and ensuring a just distribution of benefits designed to serve \\npublic welfare will cultivate a sense of trust among stakeholders, such as:\\n• The dynamics of progress: Fine-tuning the pace of transformation is critical to avoid any \\nundesired repercussions. Moreover, excessively slow developments could stifle innova-\\ntion, suggesting that determining an ideal pace through encompassing public discourse \\nis crucial.\\n• The human-AI symbiosis: Rather than striving for outright automation, more advanta-\\ngeous systems would integrate and complement the creative prowess of humans with the \\nproductive efficiency of AI. Such a hybrid model will ensure optimal oversight.\\n• Promoting access and inclusion: Equitable access to resources, relevant education, and \\nmyriad opportunities concerning AI is key to negating the amplification of disparities. \\nRepresentativeness and diversity should be prioritized.\\n• Preventive measures and risk management: Constant evaluation of freshly emerging \\ncapabilities via interdisciplinary insights is necessary to evade future dangers. Excessive \\napprehensions, however, should not impede potential progress.\\n• Upholding democratic norms: Collaborative discussions, communal efforts, and reaching \\na compromise will inevitably prove more constructive in defining the future course of AI, \\nas compared to unilateral decrees imposed by a solitary entity. Public interest must take \\nprecedence.\\nLet’s conclude this chapter!'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 343}, page_content='Chapter 10 321\\nThe road ahead\\nThe forthcoming era of generative AI models offers a plethora of intriguing opportunities and \\nunparalleled progression, yet it is interspersed with numerous uncertainties. As discussed in this \\nbook, many breakthroughs have been accomplished in recent months, but successive challenges \\ncontinue to linger, mainly pertaining to precision, reasoning ability, controllability, and entrenched \\nbias within these models. While grandiose claims of superintelligent AI on the horizon may seem \\nhyperbolic, consistent trends predict sophisticated capabilities sprouting within a few decades.\\nOn a technical level, generative models like ChatGPT often function as black boxes, with limited \\ntransparency into their decision-making processes. A lack of model interpretability makes it \\ndifficult to fully understand model behavior or to control outputs. There are also concerns about \\npotential biases that could emerge from imperfect training data. On a practical level, generative \\nmodels require extensive computational resources for training and deployment; however, we \\ndiscussed developments and trends that change that.\\nOn the positive side, AI can democratize skills, allowing amateurs to produce professional quality \\noutput in design, writing, and other areas. Businesses can benefit from faster, cheaper, on-demand \\nwork. However, there are major concerns about job losses, especially for specialized middle-class \\nroles like graphic designers, lawyers, and doctors. Their work is being automated while low-skilled \\nworkers learn to leverage AI as a superpower.\\nHowever, the proliferation of generative content raises valid concerns about misinformation, \\nplagiarism in academia, and impersonation in online spaces. As these models become more adept \\nat mimicking human expression, people may have difficulty discerning what is human-gener -\\nated versus AI-generated, enabling new forms of deception. Deepfakes produced in real-time \\nwill proliferate scams and erode trust. Most ominously, AI could be weaponized by militaries, \\nterrorists, criminals, and governments for propaganda and influence. There are also fears about \\ngenerative models exacerbating social media addiction due to their ability to produce endless \\ncustomized content.\\nThe sheer pace of advancement creates unease surrounding human obsolescence and job dis -\\nplacement, which could further divide economic classes. Unlike physical automation of the past, \\ngenerative AI threatens cognitive job categories previously considered safe from automation. \\nManaging this workforce transition ethically and equitably will require foresight and planning. \\nThere are also philosophical debates around whether AI should be creating art, literature, or music \\nthat has historically reflected the human condition.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 344}, page_content='The Future of Generative Models322\\nFor corporations, effective governance frameworks have yet to be established around acceptable \\nuse cases. Generative models amplify risks of misuse, ranging from creating misinformation such \\nas deepfakes to generating unsafe medical advice. Legal questions around content licensing and \\nintellectual property arise. While generative models can enhance business productivity, quality \\ncontrol and bias mitigation incur costs.\\nLooking decades ahead, perhaps the deepest challenges are ethical. As AI is entrusted with more \\nconsequential decisions, alignment with human values becomes critical. While accuracy, rea -\\nsoning ability, controllability, and mitigating bias remain technical priorities, other priorities \\nshould include fortifying model robustness, promoting transparency, and ensuring alignment \\nwith human values.\\nWhile future capabilities remain uncertain, proactive governance and democratization of access \\nare essential to direct these technologies toward equitable, benevolent outcomes. Collaboration \\nbetween researchers, policymakers, and civil society around issues of transparency, accountability, \\nand ethics can help align emerging innovations with shared human values. The goal should be \\nto empower human potential, not mere technological advancement.\\nJoin our community on Discord\\nJoin our community’s Discord space for discussions with the authors and other readers:\\nhttps://packt.link/lang'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 345}, page_content='packt.com\\nSubscribe to our online digital library for full access to over 7,000 books and videos, as well as \\nindustry leading tools to help you plan your personal development and advance your career. For \\nmore information, please visit our website.\\nWhy subscribe?\\n• Spend less time learning and more time coding with practical eBooks and Videos from \\nover 4,000 industry professionals\\n• Improve your learning with Skill Plans built especially for you\\n• Get a free eBook or video every month\\n• Fully searchable for easy access to vital information\\n• Copy and paste, print, and bookmark content\\nAt www.packt.com, you can also read a collection of free technical articles, sign up for a range of \\nfree newsletters, and receive exclusive discounts and offers on Packt books and eBooks.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 346}, page_content=''),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 347}, page_content='Other Books  \\nYou May Enjoy\\nIf you enjoyed this book, you may be interested in these other books by Packt:\\nTransformers for Natural Language Processing and Computer Vision\\nDenis Rothman\\nISBN: 9781805128724\\n• Master the art of fine-tuning models and engineering effective prompts\\n• Tackle examples of LLM risks by delving into strategies to mitigate them\\n• Learn about the potential functional AGI capabilities of foundation models\\n• Visualize transformer model activity for deeper insights using BertViz, LIME, and SHAP\\n• Create and implement cross-platform chained models, such as HuggingGPT\\n• Skyrocket your productivity with an automated generative ideation process\\n• Go in-depth into vision transformers with CLIP, DALL-E 2, DALL-E 3, and GPT-4V'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 348}, page_content='Other Books You May Enjoy326\\nBuilding LLM Apps\\nValentina Alto\\nISBN: 9781835462317\\n• Core components of LLMs’ architecture, including encoder-decoders blocks, embedding \\nand so on\\n• Get well-versed with unique features of LLMs like GPT-3.5/4, Llama 2, and Falcon LLM\\n• Use AI orchestrators like LangChain, and Streamlit as frontend\\n• Get familiar with LLMs components such as memory, prompts and tools\\n• Learn non-parametric knowledge, embeddings and vector databases\\n• Understand the implications of LFMs for AI research, and industry applications\\n• Customize your LLMs with fine tuning\\n• Learn the ethical implications of LLM-powered applications'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 349}, page_content='Other Books You May Enjoy 327\\nGenerative AI Engineering\\nKonrad Banachewicz\\nISBN: 9781805120513\\n• Get to grips with the fundamentals of generative AI and its applications\\n• Familiarize yourself with different types of generative models and when to use them\\n• Train and Finetune generative models using PyTorch\\n• Evaluate the performance of your models and fine-tune them for optimal results\\n• Find best practices for deploying and scaling generative AI models in production envi -\\nronments'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 350}, page_content='Other Books You May Enjoy328\\nPackt is searching for authors like you\\nIf you’re interested in becoming an author for Packt, please visit authors.packtpub.com and \\napply today. We have worked with thousands of developers and tech professionals, just like you, \\nto help them share their insight with the global tech community. You can make a general appli-\\ncation, apply for a specific hot topic that we are recruiting an author for, or submit your own idea.\\nShare your thoughts\\nNow you’ve finished Generative AI with LangChain, we’d love to hear your thoughts! If you pur -\\nchased the book from Amazon, please click here to go straight to the Amazon review \\npage for this book and share your feedback or leave a review on the site that you purchased it from.\\nYour review is important to us and the tech community and will help us make sure we’re deliv -\\nering excellent quality content.'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 351}, page_content='Index\\nSymbols\\n2021 AI boom/bust cycle  312\\nA\\nAgentOps  261\\nagents  52, 53\\nbenefits  52\\nAI, for software development\\ncode LLMs  175-179\\nAI-Generated Content (AIGC)  313\\nalignment  226\\nAnnoy (Approximate Nearest Neighbors Oh \\nYeah) algorithm  141\\nAnthropic  85\\nAPI model integrations\\nAnthropic  85\\nAzure  84\\nexploring  69-72\\nfake LLM  72, 73\\nGoogle Cloud Platform  77-79\\nHugging Face  75, 76\\nJina AI  80-82\\nOpenAI  73, 75\\nReplicate  82-84\\napplication, for customer service\\nbuilding  89-95\\nApplication Programming Interface (APIs)  69\\nApproximate Nearest Neighbor \\n (ANN)  59, 141\\nArgilla  289, 290\\nArtificial General Intelligence (AGI)  308-310\\nArtificial Intelligence (AI)  1, 5, 144\\nusing, for software development  174, 175\\narXiv  117\\nautomated data science  207, 209\\nAutoML  211-213\\ndata collection  209\\nLLMs and generative AI benefits  206\\npreprocessing and feature extraction  210\\nvisualization and EDA  210\\nAutomated Machine Learning  \\n(AutoML)  211-213\\nAutomatic Speech Recognition (ASR)  33\\nAzure  84\\nB\\nbase model  15\\nBig Bang of DL  10\\nblack-box scenario  212'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 352}, page_content='Index330\\nBoom Phase  312\\nBust Phase   312\\nByte-Pair Encoding (BPE)  25\\nC\\nChain of Density (CoD)  105, 106\\nChain-of-Thought (CoT)  128\\nChain-of-Thought (CoT) \\n prompting  169, 244, 248, 249\\nchains  50, 51\\nchatbot  132\\nELIZA  132\\nPARRY  132\\nresponses, moderating  167-169\\nuse cases  133\\nchatbot implementation  153\\ndocument loader  154, 155\\nmemory  160\\nvector storage  155-160\\nChatGPT  132\\nChinchilla scaling law  305\\nChroma  147, 148\\nClaude and Claude 2  18\\nClearML  290\\ncode LLMs  175-179\\ncode, with LLMs\\nLlama 2  186\\nsmall local model  187-189\\nStarChat  184-186\\nStarCoder  179-184\\nwriting  179\\nComet.ml  289\\ncommercial models  241, 242\\nCommon European Framework of Reference \\nfor Languages (CEFR)  17\\nConda\\ncons  66\\npros  66\\nreference link  68\\nusing  68\\nconditioning  226\\nconditioning LLMs  226, 227\\nmethods  227, 228\\nconditioning LLMs, methods\\ninference-time conditioning  230-232\\nlow-rank adaptation  229, 230\\nreinforcement learning, with human \\nfeedback  228\\ncontextual compression  156\\ncontinuous integration and continuous \\ndelivery (CI/CD)  276\\nConversationSummaryMemory\\nusing  164\\nconvolutional neural network (CNN)  31, 136\\nCreative Advertising System (CAS)  314\\nD\\nDatadog APM integration  290\\ndata exploration\\nwith LLMs  217-222\\nDataRobot MLOps  290\\ndata science\\ngenerative models, impact  204-206\\ngenerative models impact, principal  \\nareas  204\\nquestions, answering by agents  213-216\\nuse cases, for generative AI  204\\nDeepEval  290\\nDeep Learning (DL)  5\\nDenoising Diffusion Implicit Model  \\n(DDIM)  29'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 353}, page_content='Index 331\\ndependencies\\nsetting up  65-67\\nDocArray  156\\nDocker\\ncons  66\\npros  66\\nreference link  68\\nusing  68\\ndocument loaders, LangChain  149, 150\\ndocuments\\ninformation, extracting from  112-115\\nDuckDuckGo  117\\nE\\neconomic consequences  310-312\\ncreative industries and advertising  313, 314\\neducation  315\\nlaw  315\\nmanufacturing  316\\nmedicine  316\\nmilitary  316\\nEfficient Transfer Learning (PEL T)  232\\nEmbedding class  73\\nembeddings  70, 135-138\\nbag-of-words approach  136\\nword2vec  136\\nExploratory Data Analysis (EDA)  209\\nextract, transform, and load (ETL)  209\\nF\\nFacebook AI Similarity Search (Faiss)  59, 142\\nfact-checking\\nhallucination, mitigating through  100-103\\nfact-checking stages\\nclaim detection  100\\nevidence retrieval  100\\nverdict prediction  100\\nfake LLM  72, 73\\nFastAPI   276-279\\nfew-shot chain-of-thought prompting   249\\nfew-shot learning prompting  246-248\\nFinancial PhraseBank  91\\nFinetuner  80\\nfine-tuning  225, 232, 233\\nadvantages  232\\ncommercial models  241, 242\\nopen-source models  236-241\\nsetting up  233-236\\nFizzBuzz  79\\nFlowise library  49\\nforward diffusion process  28\\nFoundational Model Orchestration  \\n(FOMO)  261\\nfoundation model  15\\nG\\ngcloud command-line interface (CLI)\\ninstallation link  77\\nGenerative Adversarial Networks (GANs)  28\\ngenerative AI models  2-8\\nArtificial General Intelligence  309, 310\\nBig Tech, versus small enterprises  307, 308\\nchallenges  302, 303\\ncurrent state  300, 301\\ndeveloping, terms  19\\nforthcoming era  321, 322\\nhandling, on various domains  6'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 354}, page_content='Index332\\nimpact, on data science  204-206\\nneed for  8-11\\ntechniques and approaches, for making \\naccessible  306\\ntrends, in model development  304-307\\nusage, in other domains  33, 34\\nGenerative Pre-trained Transformer (GPT) \\nmodels  3, 13-16\\nconditioning  26\\npre-training  23, 24\\nscaling  25, 26\\ntokenization  24, 25\\nusage, considerations  20-23\\nGoogle Cloud Natural Language (NL)  77\\nGoogle Cloud Platform (GCP)  77-80\\nGoogle Colab  233\\nGPT4All  88, 89\\ngrade-school math questions (GSM8k)  235\\nGraph Convolutional Networks (GCNs)  141\\nGraphics Processing Units (GPUs)  8, 233\\nGraph Neural Networks (GNNs)  141\\nGrouped-Query Attention (GQA)  22\\nH\\nhallucination\\nmitigating, through fact-checking  100-103\\nhierarchical navigable small world  \\n(HNSW)  141, 144\\nhnswlib  142\\nHugging Face  75, 76\\nHugging Face Transformers  86, 87\\nHumanEval dataset  176\\nHyperText Markup Language (HTML)  59\\nI\\nIBM Watson OpenScale  290\\ninference-time conditioning  230-232\\ntechniques  231\\ninformation, summarizing  103\\nbasic prompting  103, 104\\nChain of Density (CoD)  105, 106\\nmap reduce approach  107-109\\nprompt templates, using  104\\ntoken usage, monitoring  109-111\\ninfrastructure as a Service (IaaS)  84\\nInfrastructure as Code (IaC)  276\\nIntegrated Development Environments \\n(IDEs)  174\\nJ\\nJina AI  80-82\\nreference link  80\\nK\\nk-dimensional trees (k-d trees)  140\\nKM scaling law  305\\nL\\nLadder Side-Tuning (LST)  232\\nLangChain  46-50, 147\\nagents  52, 53\\nbenefits  47\\nchains  50, 51\\ncomparing, with frameworks  60, 61\\ndata loaders  148, 149\\ndocument loaders  149, 150\\nkey components, exploring  50'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 355}, page_content='Index 333\\nmemory  54, 55\\nretrievers  148-151\\ntools  55, 56\\nworking  57-59\\nLangChain API\\nreference link  59\\nLangChain Expression Language (LCEL)  105\\nLangChainHub library  49\\nLangFlow library  49\\nLangfuse  290\\nLangKit  290\\nLangServe  274\\nLangSmith  291-293\\nLanguage Models (LMs)  5\\nLarge Language Models  \\n(LLMs)  1, 5, 11, 12, 37, 43-45\\nareas  12\\ndeploying  273\\ndeployment readiness, ensuring  258, 259\\nevaluating  261-264\\nexamples  45\\nexternal services, integrating  44, 45\\nFoundational Model Orchestration  \\n(FOMO)  261\\nlimitations  38-42\\nlimitations, mitigating  42, 43\\nLLMOps  260\\nMLOps  260\\nModelOps  261\\nneed for  45\\nobserving  284-286\\nparameters  9\\ntasks, related to programming   174\\nusage  27\\nused, for data exploration  217-222\\nlarge language models (LLMs), technical \\nbackground\\nGPT  13-16\\nGPT models  20\\nmajor players  18-20\\nnotable foundational GPT models  16-18\\nlatent diffusion model  31\\nLethal Autonomous Weapons Systems \\n(LAWS)  316\\nLlama 2  186\\nLLaMa and LLaMa 2 series  17\\nllama.cpp  87, 88\\nLlamaHub library  48\\nLLM apps, deployment  273-275\\naspects, for production  273\\nFastAPI web server, using  276-279\\nRay, using  280\\n requirements for running  275\\nservices and frameworks  274\\nLLM apps, evaluation\\ncomparing, against criteria  265-267\\nrunning, against datasets  268-272\\nstring and semantic comparisons  267\\ntwo outputs, comparing  264, 265\\nLLM apps, monitoring\\nconsiderations  285\\nevaluation areas  286\\nLangSmith  291-293\\nobservability tools  289-291\\nPromptWatch  294, 295\\nresponses, tracing  287-289\\nLLMonitor  290\\nLLMOps  260\\nLMOps  260'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 356}, page_content='Index334\\nlocality sensitive hashing (LSH)  141\\nlocal models\\nexploring  85\\nlow-rank adaptation  229, 230\\nLow-Rank Adaptation (LoRA)  228, 229\\nM\\nMachine Learning (ML)  5\\nmap reduce approach  107-109\\nmaps tokens  24\\nMasked Language Modeling (MLM)  23\\nMassive Multitask Language Understanding \\n(MMLU)  2\\nMaximum Marginal Relevance (MMR)   156\\nmd5 checksum tool  87\\nMean Squared Error (MSE)  31\\nmemory  54, 55\\noptions  54\\nmemory, chatbot implementation  160\\nconversation buffers  161-163\\nConversationSummaryMemory  164\\nknowledge graphs, storing  164\\nlong-term persistence  166\\nmemory mechanisms, combining  165, 166\\nMixture of Experts (MoE) model  15\\nMLOps  260\\nModelOps  261\\nmonitoring process  284\\nMulti-Head Attention (MHA)  21\\nMulti-Query Attention (MQA)  22\\nN\\nNatural Language Processing (NLP)  2\\nNegative Log-Likelihood (NLL)  23\\nNeural Machine Translation (NMT)  20\\nnmslib  142\\nO\\nobservation-dependent reasoning  123, 124\\nOpenAI  4, 73-75\\nopen-source models  236-241\\nOptical Character Recognition (OCR)  7\\nP\\nPaLM 2  16\\nParameter-Efficient Fine-Tuning (PEFT)  229\\nPerplexity (PPL)  23\\nPip\\ncons  66\\npros  66\\nreference link  67\\nusing  68\\nplan-and-execute agent  123, 124\\nplatform as a service (PaaS)  84\\nPoetry\\ncons  66\\npros  66\\nreference link  68\\nusing  68\\nPortkey  289\\nproduct quantization (PQ)  140\\nprompt chaining  50\\nprompt engineering  225, 242-244\\ncomponents  242\\ntechniques  244-246\\nprompt engineering, techniques\\nchain-of-thought  248, 249\\nfew-shot learning  246-248'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 357}, page_content='Index 335\\nself-consistency prompting  249-251\\nTree-of-Thought (ToT) prompting  251-255\\nzero-shot prompting  246\\nPromptWatch  294, 295\\nProsusAI/finbert  91\\nProximal Policy Optimization (PPO)  228\\nQ\\nquantization  230\\nR\\nRay  280-284\\nRead-Eval-Print Loop (REPL)  73\\nreasoning strategies\\nexploring  121-128\\nreinforcement learning\\nwith human feedback  228, 229\\nReinforcement Learning with Human \\nFeedback (RLHF)  26, 228\\nReplicate  82-84\\nreference link  83\\nRepresentational State Transfer Application \\nProgramming Interface  \\n(REST API)  276\\nRetrieval-Augmented Generation  \\n(RAG)  44, 131, 134\\nRetrieval-Augmented Language Models \\n(RALMs)  134\\nretrievers, LangChain  150\\nArxiv retriever  151\\nBM25 retriever  150\\ncustom retrievers  153\\ndense retriever  151\\nkNN retriever  151, 152\\nPubMed retriever  152\\nTF-IDF retriever  151\\nWikipedia retriever  151\\nreverse diffusion process  29\\nS\\nself-consistency prompting  249-251\\nsemantic search  143\\nsimilarity search  143\\nSimple Workflow Service (SWF)  209\\nSite Reliability Engineering (SRE)  286\\nsmall local model  187-189\\nSmart Generation System Personalized   314\\nsocietal implications  317\\nmisinformation and cybersecurity  318\\nregulations and implementation  \\nchallenges  319, 320\\nSoftware as a Service (SaaS)  84\\nsoftware development\\nAI, using for  174, 175\\nautomating  189-201\\nSplunk  290\\nSPTAG  143\\nStable Diffusion  30\\nStarChat  184-186\\nStarCoder  179-184\\nstochastic parrots  38\\nStreamlit app  159\\nadvantages  120\\nT\\nTechnology Innovation Institute (TII)  19\\nTensor Processing Units (TPUs)  85, 233'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 358}, page_content='Index336\\ntext-to-image models  27-32\\napplications  27\\ntokenization  24\\ntoken usage\\nmonitoring  109-111\\ntools  55, 56\\nexamples  55, 56\\ninformation, retrieving with  116, 117\\nquestions, answering with  116\\nvisual interface, building with  118-121\\ntracking  287\\ntransformer-based models  137\\ntransformers  13\\narchitectural features  21\\nTree-of-Thought (ToT) prompting  251-255\\nTuring test  132\\nU\\nU-Net  31\\nuser interface (UI)  126\\nutility chains  50\\nV\\nVariational Autoencoders (VAEs)  10, 30\\nvector databases  143\\nanomaly detection  143\\ncharacteristics  144\\nexamples  145-147\\nNatural Language Processing (NLP)  143\\npersonalization  143\\nvector indexing  140\\nvector libraries  141-143\\nAnnoy  142\\nFaiss  142\\nhnswlib  142\\nnmslib  142\\nSPTAG  143\\nvector search  135, 139\\nvector storage  135, 139\\nventure capitalists (VCs)  144\\nVisual Foundation Models (VFMs)  15\\nW\\nWeights and Biases (W&B)  234\\ntracing  290\\nWikipedia  117\\nWolfram Alpha  117\\nreference link  118\\nZ\\nZero-Shot agent  124\\nzero-shot chain-of-thought  249\\nzero-shot prompting  231, 246'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 359}, page_content='Download a free PDF copy of this book\\nThanks for purchasing this book!\\nDo you like to read on the go but are unable to carry your print books everywhere? Is your eBook \\npurchase not compatible with the device of your choice?\\nDon’t worry, now with every Packt book you get a DRM-free PDF version of that book at no cost.\\nRead anywhere, any place, on any device. Search, copy, and paste code from your favorite technical \\nbooks directly into your application.\\xa0\\nThe perks don’t stop there, you can get exclusive access to discounts, newsletters, and great free \\ncontent in your inbox daily\\nFollow these simple steps to get the benefits:\\n1. Scan the QR code or visit the link below\\nhttps://packt.link/free-ebook/9781835083468\\n2. Submit your proof of purchase\\n3. That’s it! We’ll send your free PDF and other benefits to your email directly'),\n",
       " Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 360}, page_content='')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "361"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents:  499\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "print(\"Total number of documents: \", len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'Generative AI with LangChain.pdf', 'page': 1}, page_content='Generative AI with LangChain\\nBuild large language model (LLM) apps with Python, \\nChatGPT, and other LLMs\\nBen Auffarth\\nBIRMINGHAM—MUMBAI')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini API Key.\n",
    "\n",
    "##### Get the gemini-api-key from : https://ai.google.dev/gemini-api/docs/api-key\n",
    "\n",
    "##### Embedding models: https://python.langchain.com/v0.1/docs/integrations/text_embedding/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.05168594419956207,\n",
       " -0.030764883384108543,\n",
       " -0.03062233328819275,\n",
       " -0.02802734263241291,\n",
       " 0.01813093200325966]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "google_api_key = os.environ[\"GOOGLE_API_KEY\"]\n",
    "\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "vector = embeddings.embed_query(\"hello, world!\")\n",
    "vector[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=docs, embedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":10})\n",
    "\n",
    "retrieved_docs = retriever.invoke(\"What is new in Generative AI with LangChain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 16, 'source': 'Generative AI with LangChain.pdf'}, page_content='Prefacexvi\\nWhether you are a beginner or an experienced developer, this book will be a valuable resource \\nfor anyone who wants to get the most out of LLMs and to stay ahead of the curve about LLMs \\nand LangChain.\\nWhat this book covers\\nChapter 1, What Is Generative AI? , explains how generative AI has revolutionized the processing of \\ntext, images, and video, with deep learning at its core. This chapter introduces generative models \\nsuch as LLMs, detailing their technical underpinnings and transformative potential across various \\nsectors. This chapter covers the theory behind these models, highlighting neural networks and \\ntraining approaches, and the creation of human-like content. The chapter outlines the evolution \\nof AI, Transformer architecture, text-to-image models like Stable Diffusion, and touches on sound \\nand video applications.\\nChapter 2, LangChain for LLM Apps, uncovers the need to expand beyond the stochastic parrots \\nof LLMs–models that mimic language without true understanding–by harnessing LangChain’s \\nframework. Addressing limitations like outdated knowledge, action limitations, and hallucination \\nrisks, the chapter highlights how LangChain integrates external data and interventions for more \\ncoherent AI applications. The chapter critically engages with the concept of stochastic parrots, \\nrevealing the deficiencies in models that produce fluent but meaningless language, and explicates \\nhow prompting, chain-of-thought reasoning, and retrieval grounding augment LLMs to address \\nissues of contextuality, bias, and intransparency.\\nChapter 3, Getting Started with LangChain, provides foundational knowledge for you to set up \\nyour environment to run all examples in the book. It begins with installation guidance for Dock-\\ner, Conda, Pip, and Poetry. The chapter then details integrating models from various providers \\nlike OpenAI’s ChatGPT and Hugging Face, including obtaining necessary API keys. It also deals \\nwith running open-source models locally. The chapter culminates in constructing an LLM app \\nto assist customer service agents, exemplifying how LangChain can streamline operations and \\nenhance the accuracy of responses.\\nChapter 4, Building Capable Assistants, tackles turning LLMs into reliable assistants by weaving \\nin fact-checking to reduce misinformation, employing sophisticated prompting strategies for \\nsummarization, and integrating external tools for enhanced knowledge. It explores the Chain of \\nDensity for information extraction and discusses LangChain decorators and expression language \\nfor customizing behavior. The chapter introduces map-reduce in LangChain for handling long \\ndocuments and discusses token monitoring to manage API usage costs. '),\n",
       " Document(metadata={'page': 59, 'source': 'Generative AI with LangChain.pdf'}, page_content='2\\nLangChain for LLM Apps\\nLarge Language Models (LLMs) like GPT-4 have demonstrated immense capabilities in generating \\nhuman-like text. However, simply accessing LLMs via APIs has limitations. Instead, combining \\nthem with other data sources and tools can enable more powerful applications. In this chapter, \\nwe will introduce LangChain as a way to overcome LLM limitations and build innovative lan -\\nguage-based applications. We aim to demonstrate the potential of combining recent AI advance-\\nments with a robust framework like LangChain.\\nWe will start by outlining some challenges faced when using LLMs on their own, like the lack of \\nexternal knowledge, incorrect reasoning, and the inability to take action. LangChain provides \\nsolutions to these issues through different integrations and off-the-shelf components for specific \\ntasks. We will walk through examples of how developers can use LangChain’s capabilities to cre-\\nate customized natural language processing solutions, outlining the components and concepts \\ninvolved.\\nThe goal is to illustrate how LangChain enables building dynamic, data-aware applications that \\ngo beyond what is possible by simply accessing LLMs via API calls. Lastly, we will talk about \\nimportant concepts related to LangChain, such as chains, action plan generation, and memory, \\nwhich are important concepts to understand how LangChain works.\\nThe main sections of this chapter are:\\n• Going beyond stochastic parrots\\n• What is LangChain?\\n• Exploring key components of LangChain\\n• How does LangChain work?\\n• Comparing LangChain with other frameworks'),\n",
       " Document(metadata={'page': 350, 'source': 'Generative AI with LangChain.pdf'}, page_content='Other Books You May Enjoy328\\nPackt is searching for authors like you\\nIf you’re interested in becoming an author for Packt, please visit authors.packtpub.com and \\napply today. We have worked with thousands of developers and tech professionals, just like you, \\nto help them share their insight with the global tech community. You can make a general appli-\\ncation, apply for a specific hot topic that we are recruiting an author for, or submit your own idea.\\nShare your thoughts\\nNow you’ve finished Generative AI with LangChain, we’d love to hear your thoughts! If you pur -\\nchased the book from Amazon, please click here to go straight to the Amazon review \\npage for this book and share your feedback or leave a review on the site that you purchased it from.\\nYour review is important to us and the tech community and will help us make sure we’re deliv -\\nering excellent quality content.'),\n",
       " Document(metadata={'page': 2, 'source': 'Generative AI with LangChain.pdf'}, page_content='Generative AI with LangChain\\nCopyright © 2023 Packt Publishing\\nAll rights reserved. No part of this book may be reproduced, stored in a retrieval system, or transmitted in \\nany form or by any means, without the prior written permission of the publisher, except in the case of brief \\nquotations embedded in critical articles or reviews.\\nEvery effort has been made in the preparation of this book to ensure the accuracy of the information \\npresented. However, the information contained in this book is sold without warranty, either express or \\nimplied. Neither the author, nor Packt Publishing or its dealers and distributors, will be held liable for any \\ndamages caused or alleged to have been caused directly or indirectly by this book.\\nPackt Publishing has endeavored to provide trademark information about all of the companies and products \\nmentioned in this book by the appropriate use of capitals. However, Packt Publishing cannot guarantee \\nthe accuracy of this information.\\nSenior Publishing Product Manager: Tushar Gupta\\nAcquisition Editor – Peer Reviews: Tejas Mhasvekar\\nProject Editor: Namrata Katare\\nContent Development Editors: Tanya D’cruz and Elliot Dallow\\nCopy Editor: Safis Editing\\nTechnical Editor: Kushal Sharma\\nProofreader: Safis Editing\\nIndexer: Manju Arasan\\nPresentation Designer: Ajay Patule\\nDeveloper Relations Marketing Executive: Monika Sangwan\\nFirst published: December 2023\\nProduction reference: 1141223\\nPublished by Packt Publishing Ltd.\\nGrosvenor House\\n11 St Paul’s Square\\nBirmingham\\nB3 1RB, UK.\\nISBN 978-1-83508-346-8\\nwww.packt.com'),\n",
       " Document(metadata={'page': 91, 'source': 'Generative AI with LangChain.pdf'}, page_content='Chapter 3 69\\nWith the help of LangChain, we can interact with all of these – for example, through Application \\nProgramming Interface (APIs), or we can call models that we have downloaded on our computer. \\nLet’s start with models accessed through APIs with cloud providers.\\nExploring API model integrations\\nBefore properly starting with generative AI, we need to set up access to models such as LLMs or \\ntext-to-image models so we can integrate them into our applications. As discussed in Chapter 1, \\nWhat Is Generative AI?, there are various LLMs by tech giants, like GPT-4 by OpenAI, BERT and \\nPaLM-2 by Google, LLaMA by Meta, and many more.\\nFor LLMs, OpenAI, Hugging Face, Cohere, Anthropic, Azure, Google Cloud Platform’s Vertex AI \\n(PaLM-2), and Jina AI are among the many providers supported in LangChain; however, this \\nlist is growing all the time. You can check out the full list of supported integrations for LLMs at \\nhttps://integrations.langchain.com/llms.\\nHere’s a screenshot of this page as of the time of writing (October 2023), which includes both \\ncloud providers and interfaces for local models:\\nFigure 3.1: LLM integrations in LangChain'),\n",
       " Document(metadata={'page': 118, 'source': 'Generative AI with LangChain.pdf'}, page_content='Getting Started with LangChain96\\nI hope it was exciting to see how quickly we can throw a few models and tools together in Lang-\\nChain to get something that looks actually useful. With thoughtful implementation, such AI \\nautomation can complement human agents – handling frequent questions to allow focusing \\non complex problems. Overall, this demonstrates generative AI’s potential to enhance customer \\nservice workflows.\\nWe could easily expose this in a graphical interface for customer service agents to see and interact \\nwith. This is something we will do in the next chapter.\\nLet’s wrap up!\\nSummary\\nIn this chapter, we walked through four distinct ways of installing LangChain and other libraries \\nneeded in this book as an environment. Then, we introduced several providers of models for text \\nand images. For each of them, we explained where to get the API token, and demonstrated how \\nto call a model.\\nFinally, we developed an LLM app for text categorization (intent classification) and sentiment \\nanalysis in a use case for customer service. This showcases LangChain’s ease in orchestrating \\nmultiple models to create useful applications. By chaining together various functionalities in \\nLangChain, we can help reduce response times in customer service and make sure answers are \\naccurate and to the point.\\nIn Chapter 4, Building Capable Assistants and Chapter 5, Building a Chatbot Like ChatGPT, we’ll dive \\nmore into use cases such as question answering in chatbots through augmentation with tools \\nand retrieval.\\nQuestions\\nPlease look to see whether you can provide answers to these questions. I’d recommend you go \\nback to the corresponding sections of this chapter if you are unsure about any of them:\\n1. How do you install LangChain?\\n2. List at least 4 cloud providers of LLMs apart from OpenAI!\\n3. What are Jina AI and Hugging Face?\\n4. How do you generate images with LangChain?\\n5. How do you run a model locally on your own machine rather than through a service?\\n6. How do you perform text classification in LangChain?\\n7. How can we help customer service agents in their work through generative AI?'),\n",
       " Document(metadata={'page': 84, 'source': 'Generative AI with LangChain.pdf'}, page_content='LangChain for LLM Apps62\\nAs we saw in this chapter, chains allow sequencing calls to LLMs, databases, APIs, and more to \\naccomplish multi-step workflows. Agents leverage chains to take actions based on observations \\nfor managing dynamic applications. Memory persists information across executions to maintain \\nstate. Together, these concepts enable developers to overcome the limitations of individual LLMs \\nby integrating external data, actions, and context. In other words, LangChain reduces complex \\norchestration into customizable building blocks.\\nIn the next chapters, we’ll build on these LangChain fundamentals to create capable, real-world \\napplications. We’ll implement conversational agents combining LLMs with knowledge bases and \\nadvanced reasoning algorithms. By leveraging LangChain’s capabilities, developers can unlock \\nthe full potential of LLMs to power the next generation of AI software. In the next chapter, we’ll \\nimplement our first apps with Langchain!\\nQuestions\\nPlease see if you can come up with answers to these questions. I’d recommend you go back to the \\ncorresponding sections of this chapter if you are unsure about any of them:\\n1. What are the limitations of LLMs?\\n2. What are stochastic parrots?\\n3. What are LLM applications?\\n4. What is LangChain and why should you use it?\\n5. What are LangChain’s key features?\\n6. What is a chain in LangChain?\\n7. What is an agent?\\n8. What is memory and why do we need it?\\n9. What kind of tools are available in LangChain?\\n10. How does LangChain work?'),\n",
       " Document(metadata={'page': 1, 'source': 'Generative AI with LangChain.pdf'}, page_content='Generative AI with LangChain\\nBuild large language model (LLM) apps with Python, \\nChatGPT, and other LLMs\\nBen Auffarth\\nBIRMINGHAM—MUMBAI'),\n",
       " Document(metadata={'page': 288, 'source': 'Generative AI with LangChain.pdf'}, page_content='Generative AI in Production266\\nLangChain supports both custom criteria and predefined principles for evaluation. Custom criteria \\ncan be defined using a dictionary of criterion_name: criterion_description pairs. These \\ncriteria can be used to assess outputs based on specific requirements or rubrics.\\nHere’s a simple example:\\ncustom_criteria = {\\n    \"simplicity\": \"Is the language straightforward and unpretentious?\",\\n    \"clarity\": \"Are the sentences clear and easy to understand?\",\\n    \"precision\": \"Is the writing precise, with no unnecessary words or \\ndetails?\",\\n    \"truthfulness\": \"Does the writing feel honest and sincere?\",\\n    \"subtext\": \"Does the writing suggest deeper meanings or themes?\",\\n}\\nevaluator = load_evaluator(\"pairwise_string\", criteria=custom_criteria)\\nevaluator.evaluate_string_pairs(\\n    prediction=\"Every cheerful household shares a similar rhythm of joy; \\nbut sorrow, in each household, plays a unique, haunting melody.\",\\n    prediction_b=\"Where one finds a symphony of joy, every domicile of \\nhappiness resounds in harmonious,\"\\n    \" identical notes; yet, every abode of despair conducts a dissonant \\norchestra, each\"\\n    \" playing an elegy of grief that is peculiar and profound to its own \\nexistence.\",\\n    input=\"Write some prose about families.\",\\n)\\nWe can get a very nuanced comparison of the two outputs, as this result shows:\\n{\\'reasoning\\': \\'Response A is simple, clear, and precise. It uses \\nstraightforward language to convey a deep and sincere message about \\nfamilies. The metaphor of music is used effectively to suggest deeper \\nmeanings about the shared joys and unique sorrows of families.\\\\n\\\\nResponse \\nB, on the other hand, is less simple and clear. The language is more \\ncomplex and pretentious, with phrases like \"domicile of happiness\" and \\n\"abode of despair\" instead of the simpler \"household\" used in Response A. \\nThe message is similar to that of Response A, but it is less effectively \\nconveyed due to the unnecessary complexity of the language.\\\\n\\\\nTherefore, \\nbased on the criteria of simplicity, clarity, precision, truthfulness, '),\n",
       " Document(metadata={'page': 292, 'source': 'Generative AI with LangChain.pdf'}, page_content='Generative AI in Production270\\n)\\nfor q in questions:\\n    client.create_example(inputs={\"input\": q}, dataset_id=ds.id)\\nWe can then define an LLM agent or chain on the dataset like this:\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.chains import LLMChain\\nllm = ChatOpenAI(model=\"gpt-4\", temperature=0.0)\\ndef construct_chain():\\n    return LLMChain.from_string(\\n        llm,\\n        template=\"Help out as best you can.\\\\nQuestion: {input}\\\\nResponse: \\n\",\\n    )\\nTo run an evaluation on a dataset, we can either specify an LLM or – for parallelism – use a \\nconstructor function to initialize the model or LLM app for each input. Now, to evaluate the per-\\nformance against our dataset, we need to define an evaluator as we saw in the previous section:\\nfrom langchain.smith import RunEvalConfig\\nevaluation_config = RunEvalConfig(\\n    evaluators=[\\n        RunEvalConfig.Criteria({\"helpfulness\": \"Is the response \\nhelpful?\"}),\\n        RunEvalConfig.Criteria({\"insightful\": \"Is the response carefully \\nthought out?\"})\\n    ]\\n)\\nAs seen, the criteria are defined by a dictionary that includes a criterion as a key and a question \\nto check for as the value.\\nWe’ll pass a dataset together with the evaluation configuration with evaluators to run_on_\\ndataset() to generate metrics and feedback:\\nfrom langchain.smith import run_on_dataset\\nresults = run_on_dataset(\\n  client=client,\\n  dataset_name=shared_dataset_name,\\n  dataset=dataset,')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0.3, max_tokens=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrived context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain offers a framework to build more powerful applications by combining LLMs with other data sources and tools.  It addresses LLM limitations like outdated knowledge and lack of action capabilities.  The book also covers model integrations, building assistants, and chatbot creation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What is new in Generative AI with LangChain?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
